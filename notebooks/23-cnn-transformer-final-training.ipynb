{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": "# CNN-Transformer Final Training and Evaluation\n\nThis notebook trains CNN-Transformer with the best hyperparameters found during tuning and evaluates on the test set.\n\n**Task**: Multiclass fault classification (18 classes)\n\n**Data Split**:\n- Train: Model fitting\n- Validation: Early stopping monitoring\n- Test: Final evaluation (never seen during training)\n\n**Architecture**: CNN feature extraction + Transformer encoder with attention\n\n**Outputs**:\n- Trained model: `outputs/models/cnn_transformer_final.pt`\n- Metrics: `outputs/metrics/cnn_transformer_metrics.json`\n- Confusion matrix: `outputs/figures/cnn_transformer_confusion_matrix.png`"
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport sys\nimport time\nimport json\nimport math\nfrom pathlib import Path\n\nstart_time = time.time()\nprint(\"=\"*60)\nprint(\"CNN-Transformer Final Training and Evaluation\")\nprint(\"=\"*60)\nprint(f\"Started at: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n\n# Quick mode configuration\nQUICK_MODE = os.getenv('QUICK_MODE', 'False').lower() in ('true', '1', 'yes')\n\nif QUICK_MODE:\n    TRAIN_FRACTION = 0.01\n    MAX_EPOCHS = 1\n    PATIENCE = 1\n    print(\"ðŸš€ QUICK MODE (1% data, 1 epoch)\")\nelse:\n    TRAIN_FRACTION = 1.0\n    MAX_EPOCHS = 100\n    PATIENCE = 10\n    print(\"ðŸ”¬ FULL MODE (100% data, up to 100 epochs)\")\n\nDATA_DIR = Path('../data')\nOUTPUT_DIR = Path('../outputs')\nHYPERPARAM_DIR = OUTPUT_DIR / 'hyperparams'\nMODEL_DIR = OUTPUT_DIR / 'models'\nMETRICS_DIR = OUTPUT_DIR / 'metrics'\nFIGURES_DIR = OUTPUT_DIR / 'figures'\n\nMODEL_DIR.mkdir(parents=True, exist_ok=True)\nMETRICS_DIR.mkdir(parents=True, exist_ok=True)\nFIGURES_DIR.mkdir(parents=True, exist_ok=True)\n\nRANDOM_SEED = 42\nMODE_SUFFIX = '_quick' if QUICK_MODE else ''\n\n# Load best hyperparameters\nif (HYPERPARAM_DIR / 'cnn_transformer_best.json').exists():\n    hp_file = HYPERPARAM_DIR / 'cnn_transformer_best.json'\n    print(\"Using FULL mode hyperparameters\")\nelse:\n    hp_file = HYPERPARAM_DIR / 'cnn_transformer_best_quick.json'\n    print(\"Using QUICK mode hyperparameters\")\n\nwith open(hp_file) as f:\n    hp_data = json.load(f)\n    best_params = hp_data['best_params']\n\nprint(f\"\\nHyperparameters:\")\nfor k, v in best_params.items():\n    print(f\"  {k}: {v}\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "id": "imports-header",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[Step 1/6] Loading libraries...\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    classification_report, confusion_matrix, balanced_accuracy_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"âœ“ Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-header",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n[Step 2/6] Loading datasets...\")\ndata_load_start = time.time()\n\ntrain = pd.read_csv(DATA_DIR / 'multiclass_train.csv')\nval = pd.read_csv(DATA_DIR / 'multiclass_val.csv')\ntest = pd.read_csv(DATA_DIR / 'multiclass_test.csv')\n\nprint(f\"âœ“ Train: {train.shape}\")\nprint(f\"âœ“ Val: {val.shape}\")\nprint(f\"âœ“ Test: {test.shape}\")\nprint(f\"âœ“ Data loading time: {time.time() - data_load_start:.2f}s\")\n\nfeatures = [col for col in train.columns if 'xmeas' in col or 'xmv' in col]\nnum_features = len(features)\nprint(f\"âœ“ Number of features: {num_features}\")\n\n# Subsample if in quick mode (subsample runs, not random rows, to preserve sequences)\nif TRAIN_FRACTION < 1.0:\n    train_runs = train[['faultNumber', 'simulationRun']].drop_duplicates()\n    val_runs = val[['faultNumber', 'simulationRun']].drop_duplicates()\n    \n    n_train_runs = max(1, int(len(train_runs) * TRAIN_FRACTION))\n    n_val_runs = max(1, int(len(val_runs) * TRAIN_FRACTION))\n    \n    sampled_train_runs = train_runs.sample(n=n_train_runs, random_state=RANDOM_SEED)\n    sampled_val_runs = val_runs.sample(n=n_val_runs, random_state=RANDOM_SEED)\n    \n    train = train.merge(sampled_train_runs, on=['faultNumber', 'simulationRun'])\n    val = val.merge(sampled_val_runs, on=['faultNumber', 'simulationRun'])\n    \n    print(f\"âœ“ Subsampled train to {TRAIN_FRACTION*100:.1f}%: {train.shape}\")\n    print(f\"âœ“ Subsampled val to {TRAIN_FRACTION*100:.1f}%: {val.shape}\")\n\n# Fit scaler on training data only\nscaler = StandardScaler()\nscaler.fit(train[features])\n\n# Fit label encoder on training data only\nlabel_encoder = LabelEncoder()\nlabel_encoder.fit(train['faultNumber'])\nnum_classes = len(label_encoder.classes_)\nclass_names = [str(int(c)) for c in label_encoder.classes_]\nprint(f\"âœ“ Number of classes: {num_classes}\")"
  },
  {
   "cell_type": "markdown",
   "id": "model-header",
   "metadata": {},
   "source": [
    "## Model and Dataset Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[Step 3/6] Defining model and dataset...\")\n",
    "\n",
    "class SimulationRunDataset(Dataset):\n",
    "    \"\"\"Dataset that creates windows WITHIN simulation runs only.\"\"\"\n",
    "    def __init__(self, df, features, scaler, label_encoder, sequence_length=10):\n",
    "        self.seq_len = sequence_length\n",
    "        self.windows = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for (fault, run), group in df.groupby(['faultNumber', 'simulationRun']):\n",
    "            group = group.sort_values('sample')\n",
    "            X = scaler.transform(group[features].values)\n",
    "            y = label_encoder.transform(group['faultNumber'].values)\n",
    "            \n",
    "            for i in range(len(X) - sequence_length + 1):\n",
    "                self.windows.append(X[i:i+sequence_length])\n",
    "                self.labels.append(y[i+sequence_length-1])\n",
    "        \n",
    "        self.windows = np.array(self.windows, dtype=np.float32)\n",
    "        self.labels = np.array(self.labels, dtype=np.int64)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.windows)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.windows[idx]), torch.tensor(self.labels[idx])\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=100):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class CNNTransformer(nn.Module):\n",
    "    \"\"\"CNN-Transformer: CNN feature extraction + Transformer attention.\"\"\"\n",
    "    def __init__(self, input_size, conv_filters, kernel_size, d_model, \n",
    "                 nhead, num_encoder_layers, dim_feedforward, dropout, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(input_size, conv_filters, kernel_size, padding='same')\n",
    "        self.bn1 = nn.BatchNorm1d(conv_filters)\n",
    "        self.conv2 = nn.Conv1d(conv_filters, d_model, kernel_size, padding='same')\n",
    "        self.bn2 = nn.BatchNorm1d(d_model)\n",
    "        \n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_encoder_layers)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))\n",
    "        x = torch.relu(self.bn2(self.conv2(x)))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "print(\"âœ“ Model and dataset classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-datasets",
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n[Step 4/6] Creating datasets...\")\ndataset_start = time.time()\n\nsequence_length = best_params['sequence_length']\nbatch_size = best_params['batch_size']\n\n# Create separate datasets for train, val, and test\ntrain_dataset = SimulationRunDataset(train, features, scaler, label_encoder, sequence_length)\nval_dataset = SimulationRunDataset(val, features, scaler, label_encoder, sequence_length)\ntest_dataset = SimulationRunDataset(test, features, scaler, label_encoder, sequence_length)\n\nprint(f\"âœ“ Train dataset: {len(train_dataset)} windows\")\nprint(f\"âœ“ Val dataset: {len(val_dataset)} windows\")\nprint(f\"âœ“ Test dataset: {len(test_dataset)} windows\")\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n\nprint(f\"âœ“ Dataset creation time: {time.time() - dataset_start:.2f}s\")"
  },
  {
   "cell_type": "markdown",
   "id": "train-header",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-model",
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n[Step 5/6] Training final model with early stopping on validation set...\")\ntrain_start = time.time()\n\nmodel = CNNTransformer(\n    input_size=num_features,\n    conv_filters=best_params['conv_filters'],\n    kernel_size=best_params['kernel_size'],\n    d_model=best_params['d_model'],\n    nhead=best_params['nhead'],\n    num_encoder_layers=best_params['num_encoder_layers'],\n    dim_feedforward=best_params['dim_feedforward'],\n    dropout=best_params['dropout'],\n    num_classes=num_classes\n).to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=best_params['learning_rate'])\n\n# Training with early stopping on validation loss\nbest_val_loss = float('inf')\npatience_counter = 0\nbest_model_state = None\nhistory = {'train_loss': [], 'val_loss': [], 'epoch': []}\n\nprint(f\"Training for up to {MAX_EPOCHS} epochs with patience {PATIENCE}...\")\n\nfor epoch in range(MAX_EPOCHS):\n    # Training phase\n    model.train()\n    train_loss = 0.0\n    \n    for X_batch, y_batch in train_loader:\n        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(X_batch)\n        loss = criterion(outputs, y_batch)\n        loss.backward()\n        optimizer.step()\n        \n        train_loss += loss.item()\n    \n    avg_train_loss = train_loss / len(train_loader)\n    \n    # Validation phase\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for X_batch, y_batch in val_loader:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            outputs = model(X_batch)\n            loss = criterion(outputs, y_batch)\n            val_loss += loss.item()\n    \n    avg_val_loss = val_loss / len(val_loader)\n    \n    history['train_loss'].append(avg_train_loss)\n    history['val_loss'].append(avg_val_loss)\n    history['epoch'].append(epoch + 1)\n    \n    if (epoch + 1) % 5 == 0 or epoch == 0:\n        print(f\"Epoch {epoch+1}/{MAX_EPOCHS}: Train Loss = {avg_train_loss:.6f}, Val Loss = {avg_val_loss:.6f}\")\n    \n    # Early stopping on validation loss\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        patience_counter = 0\n        best_model_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n    else:\n        patience_counter += 1\n        if patience_counter >= PATIENCE:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n\n# Restore best model\nif best_model_state is not None:\n    model.load_state_dict({k: v.to(device) for k, v in best_model_state.items()})\n\ntrain_time = time.time() - train_start\nbest_epoch = history['epoch'][history['val_loss'].index(min(history['val_loss']))]\nprint(f\"\\nâœ“ Training complete in {train_time:.2f}s ({epoch+1} epochs)\")\nprint(f\"âœ“ Best epoch: {best_epoch} (val_loss = {best_val_loss:.6f})\")"
  },
  {
   "cell_type": "markdown",
   "id": "eval-header",
   "metadata": {},
   "source": [
    "## Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate",
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n[Step 6/6] Evaluating on test set...\")\n\nmodel.eval()\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():\n    for X_batch, y_batch in test_loader:\n        X_batch = X_batch.to(device)\n        outputs = model(X_batch)\n        preds = outputs.argmax(dim=1).cpu().numpy()\n        all_preds.extend(preds)\n        all_labels.extend(y_batch.numpy())\n\ny_test = np.array(all_labels)\ny_pred = np.array(all_preds)\n\naccuracy = accuracy_score(y_test, y_pred)\nbalanced_acc = balanced_accuracy_score(y_test, y_pred)\nf1_weighted = f1_score(y_test, y_pred, average='weighted')\nf1_macro = f1_score(y_test, y_pred, average='macro')\nprecision_weighted = precision_score(y_test, y_pred, average='weighted')\nrecall_weighted = recall_score(y_test, y_pred, average='weighted')\n\nprint(f\"\\n{'='*60}\")\nprint(f\"TEST SET RESULTS {'(QUICK MODE)' if QUICK_MODE else ''}\")\nprint(f\"{'='*60}\")\nprint(f\"Accuracy:          {accuracy:.4f} ({accuracy*100:.2f}%)\")\nprint(f\"Balanced Accuracy: {balanced_acc:.4f} ({balanced_acc*100:.2f}%)\")\nprint(f\"F1 (weighted):     {f1_weighted:.4f}\")\nprint(f\"F1 (macro):        {f1_macro:.4f}\")\nprint(f\"Precision (weighted): {precision_weighted:.4f}\")\nprint(f\"Recall (weighted):    {recall_weighted:.4f}\")\nprint(f\"{'='*60}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classification-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPer-Class Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=class_names, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-header",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-history",
   "metadata": {},
   "outputs": [],
   "source": "fig, ax = plt.subplots(figsize=(10, 5))\nax.plot(history['epoch'], history['train_loss'], 'b-', linewidth=2, label='Train Loss')\nax.plot(history['epoch'], history['val_loss'], 'r-', linewidth=2, label='Val Loss')\nax.axvline(x=best_epoch, color='g', linestyle='--', alpha=0.7, label=f'Best Epoch ({best_epoch})')\nax.set_xlabel('Epoch')\nax.set_ylabel('Loss')\nax.set_title(f'CNN-Transformer Training History{\" - QUICK\" if QUICK_MODE else \"\"}')\nax.legend()\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig(FIGURES_DIR / f'cnn_transformer_training_history{MODE_SUFFIX}.png', dpi=150, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confusion-matrix",
   "metadata": {},
   "outputs": [],
   "source": "cm = confusion_matrix(y_test, y_pred)\ncm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\nfig, axes = plt.subplots(1, 2, figsize=(20, 8))\n\nsns.heatmap(cm, annot=False, fmt='d', cmap='Blues', \n            xticklabels=class_names, yticklabels=class_names, ax=axes[0])\naxes[0].set_xlabel('Predicted')\naxes[0].set_ylabel('Actual')\naxes[0].set_title(f'CNN-Transformer Confusion Matrix (Counts){\" - QUICK\" if QUICK_MODE else \"\"}')\n\nsns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n            xticklabels=class_names, yticklabels=class_names, ax=axes[1])\naxes[1].set_xlabel('Predicted')\naxes[1].set_ylabel('Actual')\naxes[1].set_title(f'CNN-Transformer Confusion Matrix (Normalized){\" - QUICK\" if QUICK_MODE else \"\"}')\n\nplt.tight_layout()\nplt.savefig(FIGURES_DIR / f'cnn_transformer_confusion_matrix{MODE_SUFFIX}.png', dpi=150, bbox_inches='tight')\nplt.show()\nprint(f\"âœ“ Saved confusion matrix to {FIGURES_DIR / f'cnn_transformer_confusion_matrix{MODE_SUFFIX}.png'}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "per-class-metrics",
   "metadata": {},
   "outputs": [],
   "source": "f1_per_class = f1_score(y_test, y_pred, average=None)\n\nfig, ax = plt.subplots(figsize=(12, 6))\nbars = ax.bar(class_names, f1_per_class, color='steelblue', edgecolor='black')\nax.axhline(y=f1_weighted, color='red', linestyle='--', label=f'Weighted Avg: {f1_weighted:.4f}')\nax.set_xlabel('Fault Class')\nax.set_ylabel('F1 Score')\nax.set_title(f'CNN-Transformer Per-Class F1 Scores{\" - QUICK\" if QUICK_MODE else \"\"}')\nax.set_ylim(0, 1.05)\nax.legend()\nax.grid(axis='y', alpha=0.3)\n\nfor bar, f1 in zip(bars, f1_per_class):\n    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n            f'{f1:.3f}', ha='center', va='bottom', fontsize=8, rotation=90)\n\nplt.tight_layout()\nplt.savefig(FIGURES_DIR / f'cnn_transformer_per_class_f1{MODE_SUFFIX}.png', dpi=150, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "save-header",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-results",
   "metadata": {},
   "outputs": [],
   "source": "end_time = time.time()\ntotal_runtime = end_time - start_time\n\nmetrics = {\n    'model': 'CNN-Transformer',\n    'task': 'multiclass',\n    'quick_mode': QUICK_MODE,\n    'train_fraction': TRAIN_FRACTION,\n    'train_samples': len(train_dataset),\n    'val_samples': len(val_dataset),\n    'test_samples': len(test_dataset),\n    'best_epoch': best_epoch,\n    'best_val_loss': float(best_val_loss),\n    'accuracy': float(accuracy),\n    'balanced_accuracy': float(balanced_acc),\n    'f1_weighted': float(f1_weighted),\n    'f1_macro': float(f1_macro),\n    'precision_weighted': float(precision_weighted),\n    'recall_weighted': float(recall_weighted),\n    'per_class_f1': {class_names[i]: float(f1_per_class[i]) for i in range(num_classes)},\n    'hyperparameters': best_params,\n    'epochs_trained': len(history['epoch']),\n    'training_time_seconds': float(train_time),\n    'total_runtime_seconds': float(total_runtime),\n    'random_seed': RANDOM_SEED\n}\n\nwith open(METRICS_DIR / f'cnn_transformer_metrics{MODE_SUFFIX}.json', 'w') as f:\n    json.dump(metrics, f, indent=2)\nprint(f\"âœ“ Saved metrics to {METRICS_DIR / f'cnn_transformer_metrics{MODE_SUFFIX}.json'}\")\n\ntorch.save({\n    'model_state_dict': model.state_dict(),\n    'model_config': {\n        'input_size': num_features,\n        'conv_filters': best_params['conv_filters'],\n        'kernel_size': best_params['kernel_size'],\n        'd_model': best_params['d_model'],\n        'nhead': best_params['nhead'],\n        'num_encoder_layers': best_params['num_encoder_layers'],\n        'dim_feedforward': best_params['dim_feedforward'],\n        'dropout': best_params['dropout'],\n        'num_classes': num_classes,\n        'sequence_length': sequence_length\n    },\n    'scaler_mean': scaler.mean_.tolist(),\n    'scaler_scale': scaler.scale_.tolist(),\n    'label_encoder_classes': label_encoder.classes_.tolist(),\n    'features': features\n}, MODEL_DIR / f'cnn_transformer_final{MODE_SUFFIX}.pt')\nprint(f\"âœ“ Saved model to {MODEL_DIR / f'cnn_transformer_final{MODE_SUFFIX}.pt'}\")\n\ncm_df = pd.DataFrame(cm, index=class_names, columns=class_names)\ncm_df.to_csv(METRICS_DIR / f'cnn_transformer_confusion_matrix{MODE_SUFFIX}.csv')\nprint(f\"âœ“ Saved confusion matrix to {METRICS_DIR / f'cnn_transformer_confusion_matrix{MODE_SUFFIX}.csv'}\")\n\nprint(f\"\\n{'='*60}\")\nprint(f\"âœ“ CNN-Transformer Final Training Complete! {'(QUICK MODE)' if QUICK_MODE else ''}\")\nprint(f\"{'='*60}\")\nprint(f\"Total runtime: {int(total_runtime // 60)}m {int(total_runtime % 60)}s\")\nprint(f\"Best epoch: {best_epoch}\")\nprint(f\"Test Accuracy: {accuracy:.4f}\")\nprint(f\"Test F1 (weighted): {f1_weighted:.4f}\")\nprint(f\"{'='*60}\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}