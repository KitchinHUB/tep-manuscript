{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# TransKal with HMM Classification Filter\n\n",
    "This notebook evaluates the effect of applying an HMM classification filter to TransKal predictions.\n\n",
    "**Method**: The HMM filter exploits temporal consistency using a sticky transition model.\n\n",
    "**Stickiness values tested**: 0.7, 0.85, 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## Configuration & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\nimport json\nimport time\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\nDATA_DIR = Path('../data')\nMODEL_DIR = Path('../outputs/models')\nMETRICS_DIR = Path('../outputs/metrics')\nFIGURES_DIR = Path('../outputs/figures')\n\nMETRICS_DIR.mkdir(parents=True, exist_ok=True)\nFIGURES_DIR.mkdir(parents=True, exist_ok=True)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nSTICKINESS_VALUES = [0.7, 0.85, 0.95]\n\nprint('='*60)\n",
    "print('TransKal with HMM Classification Filter')\nprint(f'Device: {device}')\nprint('='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filter-header",
   "metadata": {},
   "source": [
    "## HMM Classification Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filter-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationFilter:\n    def __init__(self, n_classes, confusion_matrix=None, stickiness=0.9):\n        self.n_classes = n_classes\n        self.stickiness = stickiness\n        \n        if confusion_matrix is not None:\n            self.emission = confusion_matrix / confusion_matrix.sum(axis=1, keepdims=True)\n        else:\n            accuracy = 0.8\n            self.emission = np.full((n_classes, n_classes), (1 - accuracy) / (n_classes - 1))\n            np.fill_diagonal(self.emission, accuracy)\n        \n        self.transition = np.full((n_classes, n_classes), (1 - stickiness) / (n_classes - 1))\n        np.fill_diagonal(self.transition, stickiness)\n        \n        self.belief = np.ones(n_classes) / n_classes\n    \n    def reset(self):\n        self.belief = np.ones(self.n_classes) / self.n_classes\n    \n    def update(self, observation):\n        predicted_belief = self.transition.T @ self.belief\n        likelihood = self.emission[:, observation]\n        posterior = predicted_belief * likelihood\n        posterior_sum = posterior.sum()\n        if posterior_sum > 0:\n            posterior /= posterior_sum\n        else:\n            posterior = np.ones(self.n_classes) / self.n_classes\n        self.belief = posterior\n        return np.argmax(posterior), posterior\n    \n    def filter_sequence(self, observations, reset_on_run=None):\n        self.reset()\n        estimates = []\n        prev_run = None\n        \n        for i, obs in enumerate(observations):\n            if reset_on_run is not None:\n                current_run = reset_on_run[i]\n                if prev_run is not None and current_run != prev_run:\n                    self.reset()\n                prev_run = current_run\n            \n            est, _ = self.update(obs)\n            estimates.append(est)\n        \n        return np.array(estimates)\n\nprint('ClassificationFilter defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-header",
   "metadata": {},
   "source": [
    "## Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-def",
   "metadata": {},
   "outputs": [],
   "source": "import math\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe.unsqueeze(0))\n        \n    def forward(self, x):\n        x = x + self.pe[:, :x.size(1)]\n        return self.dropout(x)\n\nclass TransformerClassifier(nn.Module):\n    \"\"\"Transformer classifier for time series (matches 24-transkal-final-training.ipynb).\"\"\"\n    def __init__(self, input_dim, num_classes, d_model=32, nhead=2, \n                 num_layers=1, dropout=0.3):\n        super().__init__()\n        self.d_model = d_model\n        \n        self.embedding = nn.Sequential(\n            nn.Linear(input_dim, d_model),\n            nn.LayerNorm(d_model),\n            nn.Dropout(dropout)\n        )\n        \n        self.pos_encoder = PositionalEncoding(d_model, dropout)\n        \n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model, nhead=nhead, \n            dim_feedforward=d_model * 2,\n            dropout=dropout, batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        \n        self.classifier = nn.Sequential(\n            nn.Linear(d_model, 64),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(64, num_classes)\n        )\n        \n    def forward(self, x):\n        x = self.embedding(x) * np.sqrt(self.d_model)\n        x = self.pos_encoder(x)\n        x = self.transformer(x)\n        x = x.mean(dim=1)\n        return self.classifier(x)\n\nprint('Model class defined')"
  },
  {
   "cell_type": "markdown",
   "id": "load-header",
   "metadata": {},
   "source": [
    "## Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-model",
   "metadata": {},
   "outputs": [],
   "source": "print('\\n[Step 1/5] Loading trained TransKal model...')\n\nmodel_path = MODEL_DIR / 'transkal_final.pt'\nmodel_data = torch.load(model_path, map_location=device, weights_only=False)\n\nconfig = model_data['model_config']\nfeatures = model_data['features']\nlabel_classes = model_data['label_encoder_classes']\nscaler_mean = model_data['scaler_mean']\nscaler_scale = model_data['scaler_scale']\n\nnum_classes = len(label_classes)\nclass_names = [str(int(c)) for c in label_classes]\n\nmodel = TransformerClassifier(\n    input_dim=config['input_dim'],\n    num_classes=num_classes,\n    d_model=config['d_model'],\n    nhead=config['nhead'],\n    num_layers=config['num_layers'],\n    dropout=config['dropout']\n)\nmodel.load_state_dict(model_data['model_state_dict'])\nmodel.to(device)\nmodel.eval()\n\nprint(f'Model loaded: {len(features)} features, {num_classes} classes')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n[Step 2/5] Loading test dataset...')\nstart_time = time.time()\n\ntest_data = pd.read_csv(DATA_DIR / 'multiclass_test.csv')\n\nprint(f'Test data loaded in {time.time() - start_time:.2f}s')\nprint(f'  Shape: {test_data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset-header",
   "metadata": {},
   "source": [
    "## Create Dataset and Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimulationRunDataset(Dataset):\n    def __init__(self, df, features, scaler_mean, scaler_scale, label_classes, sequence_length=10):\n        self.sequence_length = sequence_length\n        self.class_to_idx = {c: i for i, c in enumerate(label_classes)}\n        \n        self.sequences = []\n        self.labels = []\n        self.run_ids = []\n        \n        for (fault, run), group in df.groupby(['faultNumber', 'simulationRun']):\n            group = group.sort_values('sample')\n            X = group[features].values\n            X = (X - scaler_mean) / scaler_scale\n            \n            run_id = f'{fault}_{run}'\n            for i in range(len(X) - sequence_length + 1):\n                self.sequences.append(X[i:i+sequence_length])\n                self.labels.append(self.class_to_idx[fault])\n                self.run_ids.append(run_id)\n        \n        self.sequences = np.array(self.sequences, dtype=np.float32)\n        self.labels = np.array(self.labels, dtype=np.int64)\n    \n    def __len__(self):\n        return len(self.sequences)\n    \n    def __getitem__(self, idx):\n        return torch.tensor(self.sequences[idx]), torch.tensor(self.labels[idx])\n\nprint('Dataset class defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-predictions",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n[Step 3/5] Generating raw predictions...')\npred_start = time.time()\n\nseq_len = config['sequence_length']\ntest_dataset = SimulationRunDataset(test_data, features, scaler_mean, scaler_scale, label_classes, seq_len)\ntest_loader = DataLoader(test_dataset, batch_size=512, shuffle=False, num_workers=0)\n\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():\n    for X_batch, y_batch in test_loader:\n        X_batch = X_batch.to(device)\n        outputs = model(X_batch)\n        preds = outputs.argmax(dim=1).cpu().numpy()\n        all_preds.extend(preds)\n        all_labels.extend(y_batch.numpy())\n\ny_pred_raw = np.array(all_preds)\ny_test = np.array(all_labels)\nrun_ids = np.array(test_dataset.run_ids)\n\nprint(f'Predictions generated in {time.time() - pred_start:.2f}s')\nprint(f'  Raw accuracy: {accuracy_score(y_test, y_pred_raw):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apply-header",
   "metadata": {},
   "source": [
    "## Apply HMM Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apply-filter",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n[Step 4/5] Applying HMM filter...')\n\ncm_raw = confusion_matrix(y_test, y_pred_raw)\n\nfiltered_predictions = {}\n\nfor stickiness in STICKINESS_VALUES:\n    filter_start = time.time()\n    \n    hmm_filter = ClassificationFilter(\n        n_classes=num_classes,\n        confusion_matrix=cm_raw.astype(float),\n        stickiness=stickiness\n    )\n    \n    y_pred_filtered = hmm_filter.filter_sequence(y_pred_raw, reset_on_run=run_ids)\n    filtered_predictions[stickiness] = y_pred_filtered\n    \n    acc = accuracy_score(y_test, y_pred_filtered)\n    print(f'  Stickiness {stickiness}: Accuracy = {acc:.4f} (time: {time.time() - filter_start:.2f}s)')\n\nprint('\\nHMM filtering complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metrics-header",
   "metadata": {},
   "source": [
    "## Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compute-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n[Step 5/5] Computing metrics...')\n\ndef compute_metrics(y_true, y_pred):\n    return {\n        'accuracy': accuracy_score(y_true, y_pred),\n        'balanced_accuracy': balanced_accuracy_score(y_true, y_pred),\n        'f1_weighted': f1_score(y_true, y_pred, average='weighted'),\n        'f1_macro': f1_score(y_true, y_pred, average='macro'),\n        'per_class_f1': dict(zip(class_names, f1_score(y_true, y_pred, average=None)))\n    }\n\nraw_metrics = compute_metrics(y_test, y_pred_raw)\nfiltered_metrics = {s: compute_metrics(y_test, filtered_predictions[s]) for s in STICKINESS_VALUES}\n\nprint('\\n' + '='*80)\nprint('METRICS COMPARISON: RAW vs HMM-FILTERED')\nprint('='*80)\nprint(f\"\\n{'Method':<25} {'Accuracy':>10} {'Bal.Acc':>10} {'F1(W)':>10}\")\nprint('-'*55)\nprint(f\"{'Raw'::<25} {raw_metrics['accuracy']:>10.4f} {raw_metrics['balanced_accuracy']:>10.4f} {raw_metrics['f1_weighted']:>10.4f}\")\n\nfor stickiness in STICKINESS_VALUES:\n    m = filtered_metrics[stickiness]\n    print(f\"{'HMM (y=' + str(stickiness) + ')':<25} {m['accuracy']:>10.4f} {m['balanced_accuracy']:>10.4f} {m['f1_weighted']:>10.4f}\")\n\nprint('='*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-header",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confusion-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_stickiness = max(STICKINESS_VALUES, key=lambda s: filtered_metrics[s]['accuracy'])\n\nfig, axes = plt.subplots(1, 2, figsize=(16, 7))\n\ncm_raw_norm = cm_raw.astype('float') / cm_raw.sum(axis=1)[:, np.newaxis]\nsns.heatmap(cm_raw_norm, annot=True, fmt='.2f', cmap='Blues',\n            xticklabels=class_names, yticklabels=class_names, ax=axes[0], vmin=0, vmax=1)\naxes[0].set_xlabel('Predicted')\naxes[0].set_ylabel('Actual')\naxes[0].set_title(f'Raw {name} (Acc: {raw_metrics[\"accuracy\"]:.4f})')\n\ncm_hmm = confusion_matrix(y_test, filtered_predictions[best_stickiness])\ncm_hmm_norm = cm_hmm.astype('float') / cm_hmm.sum(axis=1)[:, np.newaxis]\nsns.heatmap(cm_hmm_norm, annot=True, fmt='.2f', cmap='Blues',\n            xticklabels=class_names, yticklabels=class_names, ax=axes[1], vmin=0, vmax=1)\naxes[1].set_xlabel('Predicted')\naxes[1].set_ylabel('Actual')\naxes[1].set_title(f'HMM Filtered (y={best_stickiness}, Acc: {filtered_metrics[best_stickiness][\"accuracy\"]:.4f})')\n\n",
    "plt.suptitle('TransKal Confusion Matrices: Raw vs HMM-Filtered', fontsize=14, y=1.02)\nplt.tight_layout()\n",
    "output_file = FIGURES_DIR / 'transkal_hmm_confusion_matrices.png'\nplt.savefig(output_file, dpi=150, bbox_inches='tight')\nplt.show()\nprint(f'Saved to {output_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-header",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_metrics = filtered_metrics[best_stickiness]\n\nresults = {\n",
    "    'model': 'TransKal',\n",
    "    'method': 'HMM Classification Filter',\n    'test_samples': len(y_test),\n    'n_classes': num_classes,\n    'raw_metrics': raw_metrics,\n    'filtered_metrics': {str(k): v for k, v in filtered_metrics.items()},\n    'best_stickiness': best_stickiness,\n    'best_improvement': {\n        'accuracy_delta': best_metrics['accuracy'] - raw_metrics['accuracy'],\n        'f1_weighted_delta': best_metrics['f1_weighted'] - raw_metrics['f1_weighted'],\n        'error_reduction_pct': ((1 - raw_metrics['accuracy']) - (1 - best_metrics['accuracy'])) / (1 - raw_metrics['accuracy']) * 100 if raw_metrics['accuracy'] < 1 else 0\n    },\n    'stickiness_values_tested': STICKINESS_VALUES\n}\n\n",
    "output_file = METRICS_DIR / 'transkal_hmm_filter_results.json'\nwith open(output_file, 'w') as f:\n    json.dump(results, f, indent=2)\nprint(f'Saved to {output_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*60)\n",
    "print('TransKal HMM Classification Filter Complete!')\n",
    "print('='*60)\nprint(f'\\nRaw Accuracy: {raw_metrics[\"accuracy\"]:.4f}')\nprint(f'Best HMM (y={best_stickiness}): {best_metrics[\"accuracy\"]:.4f}')\nprint(f'Improvement: {results[\"best_improvement\"][\"accuracy_delta\"]:+.4f}')\nprint(f'Error Reduction: {results[\"best_improvement\"][\"error_reduction_pct\"]:.1f}%')\nprint('='*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}