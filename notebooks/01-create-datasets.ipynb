{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Creation: Tennessee Eastman Process (TEP)\n",
    "\n",
    "This notebook creates reproducible, balanced datasets for training and evaluating fault detection models.\n",
    "\n",
    "**Purpose:**\n",
    "- Load raw TEP data from .RData files\n",
    "- Create balanced train/validation/test splits\n",
    "- Generate both multiclass and binary classification datasets\n",
    "- Ensure no data leakage between splits\n",
    "\n",
    "**Data Source:**\n",
    "- Rieth et al. (2017) enriched TEP dataset\n",
    "- 4 source files: fault-free training/testing, faulty training/testing\n",
    "- Each contains 500 independent simulation runs per fault type\n",
    "\n",
    "**Outputs (Option A - Practical Balance):**\n",
    "- `data/multiclass_train.csv` - 320 normal + 340 faulty runs (20 per fault)\n",
    "- `data/multiclass_val.csv` - 160 normal + 153 faulty runs (9 per fault)\n",
    "- `data/multiclass_test.csv` - 240 normal + **850 faulty runs (50 per fault)** ← 3.3× MORE\n",
    "- `data/binary_train.csv` - 320 normal runs only\n",
    "- `data/binary_val.csv` - 160 normal runs only\n",
    "- `data/binary_test.csv` - 120 normal + **850 faulty runs (50 per fault)** ← 5× MORE\n",
    "\n",
    "**Key Improvement:**\n",
    "Test set increased from 15→50 runs per fault (multiclass) and 10→50 (binary).\n",
    "Provides better statistical power (95% CI: ±14% vs ±26%) while keeping file sizes manageable (~270 MB).\n",
    "\n",
    "**Reference:**\n",
    "Rieth, C. A., Amsel, B. D., Tran, R., & Cook, M. B. (2017). Additional Tennessee Eastman Process \n",
    "Simulation Data for Anomaly Detection Evaluation. Harvard Dataverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T15:42:12.903362Z",
     "iopub.status.busy": "2026-01-03T15:42:12.903055Z",
     "iopub.status.idle": "2026-01-03T15:42:13.286100Z",
     "shell.execute_reply": "2026-01-03T15:42:13.285029Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset creation notebook initialized\n",
      "Random seed: 42\n",
      "Output directory: ../data/\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyreadr\n",
    "import os\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create data output directory\n",
    "os.makedirs('../data', exist_ok=True)\n",
    "\n",
    "print(\"Dataset creation notebook initialized\")\n",
    "print(f\"Random seed: 42\")\n",
    "print(f\"Output directory: ../data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Raw Data from .RData Files\n",
    "\n",
    "The raw data is stored in compressed .RData.zip files. We'll extract and load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T15:42:13.346747Z",
     "iopub.status.busy": "2026-01-03T15:42:13.346484Z",
     "iopub.status.idle": "2026-01-03T15:42:13.353355Z",
     "shell.execute_reply": "2026-01-03T15:42:13.352373Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for data files:\n",
      "  ✓ fftr: TEP_FaultFree_Training.RData.zip (23.0 MB)\n",
      "  ✓ ftr: TEP_Faulty_Training.RData.zip (461.7 MB)\n",
      "  ✓ ffte: TEP_FaultFree_Testing.RData.zip (44.1 MB)\n",
      "  ✓ fte: TEP_Faulty_Testing.RData.zip (778.5 MB)\n"
     ]
    }
   ],
   "source": [
    "# Define paths to the raw data files\n",
    "dataset_dir = Path('../Dataset')\n",
    "\n",
    "data_files = {\n",
    "    'fftr': dataset_dir / 'TEP_FaultFree_Training.RData.zip',\n",
    "    'ftr': dataset_dir / 'TEP_Faulty_Training.RData.zip',\n",
    "    'ffte': dataset_dir / 'TEP_FaultFree_Testing.RData.zip',\n",
    "    'fte': dataset_dir / 'TEP_Faulty_Testing.RData.zip'\n",
    "}\n",
    "\n",
    "# Verify all files exist\n",
    "print(\"Checking for data files:\")\n",
    "for key, path in data_files.items():\n",
    "    if path.exists():\n",
    "        size_mb = path.stat().st_size / (1024 * 1024)\n",
    "        print(f\"  ✓ {key}: {path.name} ({size_mb:.1f} MB)\")\n",
    "    else:\n",
    "        print(f\"  ✗ {key}: {path.name} NOT FOUND\")\n",
    "        raise FileNotFoundError(f\"Required data file not found: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T15:42:13.356976Z",
     "iopub.status.busy": "2026-01-03T15:42:13.356654Z",
     "iopub.status.idle": "2026-01-03T15:44:13.810773Z",
     "shell.execute_reply": "2026-01-03T15:44:13.808880Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data files...\n",
      "This may take a few minutes due to file size.\n",
      "\n",
      "✓ Fault-free training loaded: (250000, 55)\n",
      "✓ Faulty training loaded: (5000000, 55)\n",
      "✓ Fault-free testing loaded: (480000, 55)\n",
      "✓ Faulty testing loaded: (9600000, 55)\n"
     ]
    }
   ],
   "source": [
    "def load_rdata_from_zip(zip_path, expected_key=None):\n",
    "    \"\"\"\n",
    "    Extract and load .RData file from zip archive.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    zip_path : Path\n",
    "        Path to the .zip file containing .RData\n",
    "    expected_key : str, optional\n",
    "        Expected key in the RData dictionary\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Loaded data\n",
    "    \"\"\"\n",
    "    # Extract the .RData file from zip\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        # Get the RData filename (should be only file in archive)\n",
    "        rdata_filename = [f for f in zip_ref.namelist() if f.endswith('.RData')][0]\n",
    "        \n",
    "        # Extract to temporary location\n",
    "        temp_path = zip_path.parent / rdata_filename\n",
    "        zip_ref.extract(rdata_filename, path=zip_path.parent)\n",
    "    \n",
    "    try:\n",
    "        # Load RData file\n",
    "        result = pyreadr.read_r(str(temp_path))\n",
    "        \n",
    "        # Get the dataframe (RData files contain OrderedDict)\n",
    "        if expected_key and expected_key in result:\n",
    "            df = result[expected_key]\n",
    "        else:\n",
    "            # Take the first (and typically only) dataframe\n",
    "            df = list(result.values())[0]\n",
    "        \n",
    "        return df\n",
    "    finally:\n",
    "        # Clean up extracted file\n",
    "        if temp_path.exists():\n",
    "            temp_path.unlink()\n",
    "\n",
    "print(\"Loading data files...\")\n",
    "print(\"This may take a few minutes due to file size.\\n\")\n",
    "\n",
    "# Load each dataset\n",
    "fftr = load_rdata_from_zip(data_files['fftr'], 'fault_free_training')\n",
    "print(f\"✓ Fault-free training loaded: {fftr.shape}\")\n",
    "\n",
    "ftr = load_rdata_from_zip(data_files['ftr'], 'faulty_training')\n",
    "print(f\"✓ Faulty training loaded: {ftr.shape}\")\n",
    "\n",
    "ffte = load_rdata_from_zip(data_files['ffte'], 'fault_free_testing')\n",
    "print(f\"✓ Fault-free testing loaded: {ffte.shape}\")\n",
    "\n",
    "fte = load_rdata_from_zip(data_files['fte'], 'faulty_testing')\n",
    "print(f\"✓ Faulty testing loaded: {fte.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Structure Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T15:44:13.816799Z",
     "iopub.status.busy": "2026-01-03T15:44:13.816255Z",
     "iopub.status.idle": "2026-01-03T15:44:14.118967Z",
     "shell.execute_reply": "2026-01-03T15:44:14.118031Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Structure:\n",
      "======================================================================\n",
      "\n",
      "Columns (55):\n",
      "['faultNumber', 'simulationRun', 'sample', 'xmeas_1', 'xmeas_2', 'xmeas_3', 'xmeas_4', 'xmeas_5', 'xmeas_6', 'xmeas_7', 'xmeas_8', 'xmeas_9', 'xmeas_10', 'xmeas_11', 'xmeas_12', 'xmeas_13', 'xmeas_14', 'xmeas_15', 'xmeas_16', 'xmeas_17', 'xmeas_18', 'xmeas_19', 'xmeas_20', 'xmeas_21', 'xmeas_22', 'xmeas_23', 'xmeas_24', 'xmeas_25', 'xmeas_26', 'xmeas_27', 'xmeas_28', 'xmeas_29', 'xmeas_30', 'xmeas_31', 'xmeas_32', 'xmeas_33', 'xmeas_34', 'xmeas_35', 'xmeas_36', 'xmeas_37', 'xmeas_38', 'xmeas_39', 'xmeas_40', 'xmeas_41', 'xmv_1', 'xmv_2', 'xmv_3', 'xmv_4', 'xmv_5', 'xmv_6', 'xmv_7', 'xmv_8', 'xmv_9', 'xmv_10', 'xmv_11']\n",
      "\n",
      "Fault-free training:\n",
      "  Unique simulation runs: 500\n",
      "  Samples per run: 500\n",
      "  Fault numbers: [0.0]\n",
      "\n",
      "Faulty training:\n",
      "  Unique simulation runs: 500\n",
      "  Samples per run: 500\n",
      "  Fault numbers: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "\n",
      "Fault-free testing:\n",
      "  Unique simulation runs: 500\n",
      "  Samples per run: 960\n",
      "  Fault numbers: [0]\n",
      "\n",
      "Faulty testing:\n",
      "  Unique simulation runs: 500\n",
      "  Samples per run: 960\n",
      "  Fault numbers: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "\n",
      "======================================================================\n",
      "Sample data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>faultNumber</th>\n",
       "      <th>simulationRun</th>\n",
       "      <th>sample</th>\n",
       "      <th>xmeas_1</th>\n",
       "      <th>xmeas_2</th>\n",
       "      <th>xmeas_3</th>\n",
       "      <th>xmeas_4</th>\n",
       "      <th>xmeas_5</th>\n",
       "      <th>xmeas_6</th>\n",
       "      <th>xmeas_7</th>\n",
       "      <th>...</th>\n",
       "      <th>xmv_2</th>\n",
       "      <th>xmv_3</th>\n",
       "      <th>xmv_4</th>\n",
       "      <th>xmv_5</th>\n",
       "      <th>xmv_6</th>\n",
       "      <th>xmv_7</th>\n",
       "      <th>xmv_8</th>\n",
       "      <th>xmv_9</th>\n",
       "      <th>xmv_10</th>\n",
       "      <th>xmv_11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.25038</td>\n",
       "      <td>3674.0</td>\n",
       "      <td>4529.0</td>\n",
       "      <td>9.2320</td>\n",
       "      <td>26.889</td>\n",
       "      <td>42.402</td>\n",
       "      <td>2704.3</td>\n",
       "      <td>...</td>\n",
       "      <td>53.744</td>\n",
       "      <td>24.657</td>\n",
       "      <td>62.544</td>\n",
       "      <td>22.137</td>\n",
       "      <td>39.935</td>\n",
       "      <td>42.323</td>\n",
       "      <td>47.757</td>\n",
       "      <td>47.510</td>\n",
       "      <td>41.258</td>\n",
       "      <td>18.447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.25109</td>\n",
       "      <td>3659.4</td>\n",
       "      <td>4556.6</td>\n",
       "      <td>9.4264</td>\n",
       "      <td>26.721</td>\n",
       "      <td>42.576</td>\n",
       "      <td>2705.0</td>\n",
       "      <td>...</td>\n",
       "      <td>53.414</td>\n",
       "      <td>24.588</td>\n",
       "      <td>59.259</td>\n",
       "      <td>22.084</td>\n",
       "      <td>40.176</td>\n",
       "      <td>38.554</td>\n",
       "      <td>43.692</td>\n",
       "      <td>47.427</td>\n",
       "      <td>41.359</td>\n",
       "      <td>17.194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.25038</td>\n",
       "      <td>3660.3</td>\n",
       "      <td>4477.8</td>\n",
       "      <td>9.4426</td>\n",
       "      <td>26.875</td>\n",
       "      <td>42.070</td>\n",
       "      <td>2706.2</td>\n",
       "      <td>...</td>\n",
       "      <td>54.357</td>\n",
       "      <td>24.666</td>\n",
       "      <td>61.275</td>\n",
       "      <td>22.380</td>\n",
       "      <td>40.244</td>\n",
       "      <td>38.990</td>\n",
       "      <td>46.699</td>\n",
       "      <td>47.468</td>\n",
       "      <td>41.199</td>\n",
       "      <td>20.530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.24977</td>\n",
       "      <td>3661.3</td>\n",
       "      <td>4512.1</td>\n",
       "      <td>9.4776</td>\n",
       "      <td>26.758</td>\n",
       "      <td>42.063</td>\n",
       "      <td>2707.2</td>\n",
       "      <td>...</td>\n",
       "      <td>53.946</td>\n",
       "      <td>24.725</td>\n",
       "      <td>59.856</td>\n",
       "      <td>22.277</td>\n",
       "      <td>40.257</td>\n",
       "      <td>38.072</td>\n",
       "      <td>47.541</td>\n",
       "      <td>47.658</td>\n",
       "      <td>41.643</td>\n",
       "      <td>18.089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.29405</td>\n",
       "      <td>3679.0</td>\n",
       "      <td>4497.0</td>\n",
       "      <td>9.3381</td>\n",
       "      <td>26.889</td>\n",
       "      <td>42.650</td>\n",
       "      <td>2705.1</td>\n",
       "      <td>...</td>\n",
       "      <td>53.658</td>\n",
       "      <td>28.797</td>\n",
       "      <td>60.717</td>\n",
       "      <td>21.947</td>\n",
       "      <td>39.144</td>\n",
       "      <td>41.955</td>\n",
       "      <td>47.645</td>\n",
       "      <td>47.346</td>\n",
       "      <td>41.507</td>\n",
       "      <td>18.461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   faultNumber  simulationRun  sample  xmeas_1  xmeas_2  xmeas_3  xmeas_4  \\\n",
       "0          0.0            1.0       1  0.25038   3674.0   4529.0   9.2320   \n",
       "1          0.0            1.0       2  0.25109   3659.4   4556.6   9.4264   \n",
       "2          0.0            1.0       3  0.25038   3660.3   4477.8   9.4426   \n",
       "3          0.0            1.0       4  0.24977   3661.3   4512.1   9.4776   \n",
       "4          0.0            1.0       5  0.29405   3679.0   4497.0   9.3381   \n",
       "\n",
       "   xmeas_5  xmeas_6  xmeas_7  ...   xmv_2   xmv_3   xmv_4   xmv_5   xmv_6  \\\n",
       "0   26.889   42.402   2704.3  ...  53.744  24.657  62.544  22.137  39.935   \n",
       "1   26.721   42.576   2705.0  ...  53.414  24.588  59.259  22.084  40.176   \n",
       "2   26.875   42.070   2706.2  ...  54.357  24.666  61.275  22.380  40.244   \n",
       "3   26.758   42.063   2707.2  ...  53.946  24.725  59.856  22.277  40.257   \n",
       "4   26.889   42.650   2705.1  ...  53.658  28.797  60.717  21.947  39.144   \n",
       "\n",
       "    xmv_7   xmv_8   xmv_9  xmv_10  xmv_11  \n",
       "0  42.323  47.757  47.510  41.258  18.447  \n",
       "1  38.554  43.692  47.427  41.359  17.194  \n",
       "2  38.990  46.699  47.468  41.199  20.530  \n",
       "3  38.072  47.541  47.658  41.643  18.089  \n",
       "4  41.955  47.645  47.346  41.507  18.461  \n",
       "\n",
       "[5 rows x 55 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Dataset Structure:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nColumns ({len(fftr.columns)}):\")\n",
    "print(fftr.columns.tolist())\n",
    "\n",
    "print(f\"\\nFault-free training:\")\n",
    "print(f\"  Unique simulation runs: {fftr['simulationRun'].nunique()}\")\n",
    "print(f\"  Samples per run: {fftr['sample'].nunique()}\")\n",
    "print(f\"  Fault numbers: {sorted(fftr['faultNumber'].unique())}\")\n",
    "\n",
    "print(f\"\\nFaulty training:\")\n",
    "print(f\"  Unique simulation runs: {ftr['simulationRun'].nunique()}\")\n",
    "print(f\"  Samples per run: {ftr['sample'].nunique()}\")\n",
    "print(f\"  Fault numbers: {sorted(ftr['faultNumber'].unique())}\")\n",
    "\n",
    "print(f\"\\nFault-free testing:\")\n",
    "print(f\"  Unique simulation runs: {ffte['simulationRun'].nunique()}\")\n",
    "print(f\"  Samples per run: {ffte['sample'].nunique()}\")\n",
    "print(f\"  Fault numbers: {sorted(ffte['faultNumber'].unique())}\")\n",
    "\n",
    "print(f\"\\nFaulty testing:\")\n",
    "print(f\"  Unique simulation runs: {fte['simulationRun'].nunique()}\")\n",
    "print(f\"  Samples per run: {fte['sample'].nunique()}\")\n",
    "print(f\"  Fault numbers: {sorted(fte['faultNumber'].unique())}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Sample data:\")\n",
    "fftr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Add Unique Trajectory Identifiers\n",
    "\n",
    "To prevent data leakage, we create unique identifiers for each simulation run\n",
    "that combine:\n",
    "- **origin**: Source dataset (fftr, ftr, ffte, fte)\n",
    "- **faultNumber**: Fault type (0-20)\n",
    "- **simulationRun**: Run ID (1-500)\n",
    "\n",
    "This ensures that simulation runs from different source files are kept separate,\n",
    "even if they have the same simulationRun number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T15:44:14.122309Z",
     "iopub.status.busy": "2026-01-03T15:44:14.122150Z",
     "iopub.status.idle": "2026-01-03T15:44:30.707142Z",
     "shell.execute_reply": "2026-01-03T15:44:30.706082Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique trajectory identifiers added.\n",
      "\n",
      "Example traj_key values:\n",
      "  fftr: fftr_f0_r1\n",
      "  ftr:  ftr_f1_r1\n",
      "  ffte: ffte_f0_r1\n",
      "  fte:  fte_f1_r1\n"
     ]
    }
   ],
   "source": [
    "def attach_origin_and_traj_key(df, origin_name):\n",
    "    \"\"\"\n",
    "    Add origin label and unique trajectory key to dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        TEP data\n",
    "    origin_name : str\n",
    "        Origin identifier (fftr, ftr, ffte, fte)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with added 'origin' and 'traj_key' columns\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['origin'] = origin_name\n",
    "    \n",
    "    # Create unique key: origin_f{fault}_r{run}\n",
    "    df['traj_key'] = (\n",
    "        df['origin'].astype(str)\n",
    "        + '_f' + df['faultNumber'].astype(int).astype(str)\n",
    "        + '_r' + df['simulationRun'].astype(int).astype(str)\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Add identifiers to all datasets\n",
    "fftr = attach_origin_and_traj_key(fftr, 'fftr')\n",
    "ftr = attach_origin_and_traj_key(ftr, 'ftr')\n",
    "ffte = attach_origin_and_traj_key(ffte, 'ffte')\n",
    "fte = attach_origin_and_traj_key(fte, 'fte')\n",
    "\n",
    "print(\"Unique trajectory identifiers added.\")\n",
    "print(f\"\\nExample traj_key values:\")\n",
    "print(f\"  fftr: {fftr['traj_key'].iloc[0]}\")\n",
    "print(f\"  ftr:  {ftr['traj_key'].iloc[0]}\")\n",
    "print(f\"  ffte: {ffte['traj_key'].iloc[0]}\")\n",
    "print(f\"  fte:  {fte['traj_key'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T15:44:30.711274Z",
     "iopub.status.busy": "2026-01-03T15:44:30.711112Z",
     "iopub.status.idle": "2026-01-03T15:44:32.084991Z",
     "shell.execute_reply": "2026-01-03T15:44:32.083967Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying no trajectory key overlaps between source datasets:\n",
      "  fftr ∩ ftr:   0 (should be 0)\n",
      "  fftr ∩ ffte:  0 (should be 0)\n",
      "  fftr ∩ fte:   0 (should be 0)\n",
      "  ftr  ∩ ffte:  0 (should be 0)\n",
      "  ftr  ∩ fte:   0 (should be 0)\n",
      "  ffte ∩ fte:   0 (should be 0)\n",
      "\n",
      "✓ All source datasets have unique trajectory keys (no leakage)\n"
     ]
    }
   ],
   "source": [
    "# Verify no overlaps between source datasets\n",
    "fftr_keys = set(fftr['traj_key'])\n",
    "ftr_keys = set(ftr['traj_key'])\n",
    "ffte_keys = set(ffte['traj_key'])\n",
    "fte_keys = set(fte['traj_key'])\n",
    "\n",
    "print(\"Verifying no trajectory key overlaps between source datasets:\")\n",
    "print(f\"  fftr ∩ ftr:   {len(fftr_keys & ftr_keys)} (should be 0)\")\n",
    "print(f\"  fftr ∩ ffte:  {len(fftr_keys & ffte_keys)} (should be 0)\")\n",
    "print(f\"  fftr ∩ fte:   {len(fftr_keys & fte_keys)} (should be 0)\")\n",
    "print(f\"  ftr  ∩ ffte:  {len(ftr_keys & ffte_keys)} (should be 0)\")\n",
    "print(f\"  ftr  ∩ fte:   {len(ftr_keys & fte_keys)} (should be 0)\")\n",
    "print(f\"  ffte ∩ fte:   {len(ffte_keys & fte_keys)} (should be 0)\")\n",
    "\n",
    "all_overlaps_zero = all([\n",
    "    len(fftr_keys & ftr_keys) == 0,\n",
    "    len(fftr_keys & ffte_keys) == 0,\n",
    "    len(fftr_keys & fte_keys) == 0,\n",
    "    len(ftr_keys & ffte_keys) == 0,\n",
    "    len(ftr_keys & fte_keys) == 0,\n",
    "    len(ffte_keys & fte_keys) == 0\n",
    "])\n",
    "\n",
    "if all_overlaps_zero:\n",
    "    print(\"\\n✓ All source datasets have unique trajectory keys (no leakage)\")\n",
    "else:\n",
    "    print(\"\\n✗ WARNING: Overlapping trajectory keys detected!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sampling Function\n",
    "\n",
    "This function samples simulation runs from a dataset while:\n",
    "- Ensuring specified runs are used (for split control)\n",
    "- For normal operation (fault 0): using the entire trajectory\n",
    "- For faulty operation: using only post-fault samples (after fault_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T15:44:32.088580Z",
     "iopub.status.busy": "2026-01-03T15:44:32.088421Z",
     "iopub.status.idle": "2026-01-03T15:44:32.094529Z",
     "shell.execute_reply": "2026-01-03T15:44:32.093761Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling function defined.\n"
     ]
    }
   ],
   "source": [
    "def sample_runs(df, fault_number, allowed_runs, fault_start=0):\n",
    "    \"\"\"\n",
    "    Sample specific simulation runs for a given fault number.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Source dataframe (fftr, ftr, ffte, or fte)\n",
    "    fault_number : int/float\n",
    "        Fault number to filter (0 for normal, 1-20 for faults)\n",
    "    allowed_runs : array-like\n",
    "        List of simulationRun IDs to include\n",
    "    fault_start : int\n",
    "        For faulty data, only keep samples >= this value (default: 0)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Sampled data\n",
    "    \"\"\"\n",
    "    # Filter by fault number and allowed runs\n",
    "    selected_df = df[\n",
    "        (df['faultNumber'] == fault_number) &\n",
    "        (df['simulationRun'].isin(allowed_runs))\n",
    "    ]\n",
    "    \n",
    "    frames = []\n",
    "    for run in allowed_runs:\n",
    "        run_df = selected_df[selected_df['simulationRun'] == run].sort_values('sample')\n",
    "        \n",
    "        if run_df.empty:\n",
    "            continue\n",
    "        \n",
    "        if fault_number == 0:\n",
    "            # Normal operation: use full trajectory\n",
    "            frames.append(run_df)\n",
    "        else:\n",
    "            # Faulty operation: use only post-fault segment\n",
    "            frames.append(run_df[run_df['sample'] >= fault_start])\n",
    "    \n",
    "    if not frames:\n",
    "        return pd.DataFrame(columns=df.columns)\n",
    "    \n",
    "    return pd.concat(frames, ignore_index=True)\n",
    "\n",
    "print(\"Sampling function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Supervised Learning Datasets\n",
    "\n",
    "**Supervised datasets** contain both normal and faulty examples for multiclass classification.\n",
    "\n",
    "### Data Split Strategy (Option A - Maximize Test Set with Practical Constraints):\n",
    "- **Training:** 320 normal runs + 20 runs per fault (17 faults) = 660 runs total\n",
    "- **Validation:** 160 normal runs + 9 runs per fault (17 faults) = 313 runs total\n",
    "- **Test:** 240 normal runs + **50 runs per fault** (17 faults) = 1,090 runs total\n",
    "\n",
    "### Rationale:\n",
    "The test set should be larger for reliable performance evaluation. We use 50 runs per fault \n",
    "(3.3× more than original 15) which provides:\n",
    "- 95% CI width of ±14% vs. ±26% for 80% accuracy\n",
    "- Manageable file sizes (~270 MB vs ~2.7 GB for all 500)\n",
    "- Sufficient statistical power to detect meaningful model differences\n",
    "\n",
    "### Fault Timing:\n",
    "- Training/validation faulty runs (from `ftr`): Fault occurs at sample 21\n",
    "- Test faulty runs (from `fte`): Fault occurs at sample 161 (after 8 hours normal operation)\n",
    "\n",
    "### Excluded Faults:\n",
    "Faults 3, 9, and 15 are excluded as they are too subtle to detect reliably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T15:44:32.097623Z",
     "iopub.status.busy": "2026-01-03T15:44:32.097482Z",
     "iopub.status.idle": "2026-01-03T15:44:32.512911Z",
     "shell.execute_reply": "2026-01-03T15:44:32.511917Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Supervised Learning Datasets\n",
      "======================================================================\n",
      "Excluded faults: [3, 9, 15]\n",
      "Included faults: [1, 2, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20]\n",
      "Total fault classes (including normal): 18\n",
      "\n",
      "Normal run allocation:\n",
      "  Training:   320 runs (runs 1.0-498.0)\n",
      "  Validation: 160 runs (runs 2.0-499.0)\n",
      "\n",
      "Normal data sampled:\n",
      "  Train: (160000, 57)\n",
      "  Val:   (80000, 57)\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating Supervised Learning Datasets\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Define excluded faults\n",
    "excluded_faults = [3, 9, 15]\n",
    "all_faults = [f for f in sorted(ftr['faultNumber'].unique()) \n",
    "              if f not in excluded_faults]\n",
    "\n",
    "print(f\"Excluded faults: {excluded_faults}\")\n",
    "print(f\"Included faults: {all_faults}\")\n",
    "print(f\"Total fault classes (including normal): {len(all_faults) + 1}\\n\")\n",
    "\n",
    "# 1. Normal (fault-free) data split from fftr\n",
    "all_normal_runs = fftr['simulationRun'].unique()\n",
    "np.random.shuffle(all_normal_runs)\n",
    "\n",
    "train_normal_runs = all_normal_runs[:320]\n",
    "val_normal_runs = all_normal_runs[320:480]\n",
    "\n",
    "print(f\"Normal run allocation:\")\n",
    "print(f\"  Training:   {len(train_normal_runs)} runs (runs {train_normal_runs.min()}-{train_normal_runs.max()})\")\n",
    "print(f\"  Validation: {len(val_normal_runs)} runs (runs {val_normal_runs.min()}-{val_normal_runs.max()})\")\n",
    "\n",
    "# Sample normal runs\n",
    "supervised_train = sample_runs(fftr, fault_number=0, \n",
    "                               allowed_runs=train_normal_runs, \n",
    "                               fault_start=0)\n",
    "\n",
    "supervised_val = sample_runs(fftr, fault_number=0, \n",
    "                             allowed_runs=val_normal_runs, \n",
    "                             fault_start=0)\n",
    "\n",
    "print(f\"\\nNormal data sampled:\")\n",
    "print(f\"  Train: {supervised_train.shape}\")\n",
    "print(f\"  Val:   {supervised_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T15:44:32.516396Z",
     "iopub.status.busy": "2026-01-03T15:44:32.516250Z",
     "iopub.status.idle": "2026-01-03T15:44:36.564219Z",
     "shell.execute_reply": "2026-01-03T15:44:36.563323Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sampling faulty runs from training source (ftr):\n",
      "  Fault start time: sample 21\n",
      "  Strategy: 20 runs for training, 9 runs for validation per fault\n",
      "\n",
      "  Fault  1: Train 20 runs, Val 9 runs\n",
      "  Fault  2: Train 20 runs, Val 9 runs\n",
      "  Fault  4: Train 20 runs, Val 9 runs\n",
      "  Fault  5: Train 20 runs, Val 9 runs\n",
      "  Fault  6: Train 20 runs, Val 9 runs\n",
      "  Fault  7: Train 20 runs, Val 9 runs\n",
      "  Fault  8: Train 20 runs, Val 9 runs\n",
      "  Fault 10: Train 20 runs, Val 9 runs\n",
      "  Fault 11: Train 20 runs, Val 9 runs\n",
      "  Fault 12: Train 20 runs, Val 9 runs\n",
      "  Fault 13: Train 20 runs, Val 9 runs\n",
      "  Fault 14: Train 20 runs, Val 9 runs\n",
      "  Fault 16: Train 20 runs, Val 9 runs\n",
      "  Fault 17: Train 20 runs, Val 9 runs\n",
      "  Fault 18: Train 20 runs, Val 9 runs\n",
      "  Fault 19: Train 20 runs, Val 9 runs\n",
      "  Fault 20: Train 20 runs, Val 9 runs\n",
      "\n",
      "Supervised datasets after adding faulty runs:\n",
      "  Train: (323200, 57)\n",
      "  Val:   (153440, 57)\n"
     ]
    }
   ],
   "source": [
    "# 2. Faulty data split from ftr (training source)\n",
    "print(\"\\nSampling faulty runs from training source (ftr):\")\n",
    "print(f\"  Fault start time: sample 21\")\n",
    "print(f\"  Strategy: 20 runs for training, 9 runs for validation per fault\\n\")\n",
    "\n",
    "for fault in all_faults:\n",
    "    fault_df = ftr[ftr['faultNumber'] == fault]\n",
    "    fault_runs = fault_df['simulationRun'].unique()\n",
    "    np.random.shuffle(fault_runs)\n",
    "    \n",
    "    # Split: 20 train, 9 val (total 29 from ftr)\n",
    "    train_fault_runs = fault_runs[:20]\n",
    "    val_fault_runs = fault_runs[20:29]\n",
    "    \n",
    "    # Sample post-fault data (sample >= 21)\n",
    "    train_fault_df = sample_runs(ftr, fault_number=fault,\n",
    "                                 allowed_runs=train_fault_runs,\n",
    "                                 fault_start=21)\n",
    "    \n",
    "    val_fault_df = sample_runs(ftr, fault_number=fault,\n",
    "                               allowed_runs=val_fault_runs,\n",
    "                               fault_start=21)\n",
    "    \n",
    "    supervised_train = pd.concat([supervised_train, train_fault_df], ignore_index=True)\n",
    "    supervised_val = pd.concat([supervised_val, val_fault_df], ignore_index=True)\n",
    "    \n",
    "    print(f\"  Fault {int(fault):2d}: Train {len(train_fault_runs)} runs, Val {len(val_fault_runs)} runs\")\n",
    "\n",
    "print(f\"\\nSupervised datasets after adding faulty runs:\")\n",
    "print(f\"  Train: {supervised_train.shape}\")\n",
    "print(f\"  Val:   {supervised_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T15:44:36.567676Z",
     "iopub.status.busy": "2026-01-03T15:44:36.567515Z",
     "iopub.status.idle": "2026-01-03T15:44:42.369199Z",
     "shell.execute_reply": "2026-01-03T15:44:42.368238Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating test set from ffte and fte:\n",
      "  Strategy: Use 50 runs per fault for good statistical power\n",
      "  Normal runs: 240 from ffte\n",
      "  Faulty runs: 50 runs per fault from fte (3.3× more than before)\n",
      "  Fault start time: sample 161 (after 8 hours normal operation)\n",
      "\n",
      "Normal test data: (230400, 57)\n",
      "\n",
      "Using 50 runs per fault from fte:\n",
      "  Fault  1: 50 runs, 40000 samples\n",
      "  Fault  2: 50 runs, 40000 samples\n",
      "  Fault  4: 50 runs, 40000 samples\n",
      "  Fault  5: 50 runs, 40000 samples\n",
      "  Fault  6: 50 runs, 40000 samples\n",
      "  Fault  7: 50 runs, 40000 samples\n",
      "  Fault  8: 50 runs, 40000 samples\n",
      "  Fault 10: 50 runs, 40000 samples\n",
      "  Fault 11: 50 runs, 40000 samples\n",
      "  Fault 12: 50 runs, 40000 samples\n",
      "  Fault 13: 50 runs, 40000 samples\n",
      "  Fault 14: 50 runs, 40000 samples\n",
      "  Fault 16: 50 runs, 40000 samples\n",
      "  Fault 17: 50 runs, 40000 samples\n",
      "  Fault 18: 50 runs, 40000 samples\n",
      "  Fault 19: 50 runs, 40000 samples\n",
      "  Fault 20: 50 runs, 40000 samples\n",
      "\n",
      "Final supervised test set: (910400, 57)\n",
      "  Total test runs: 1090\n",
      "  Normal runs: 240, Fault runs: 17 × 50 = 850\n"
     ]
    }
   ],
   "source": [
    "# 3. Test set from ffte (normal) and fte (faulty)\n",
    "# OPTION A: Use 50 runs per fault (practical balance of size vs. statistical power)\n",
    "print(\"\\nCreating test set from ffte and fte:\")\n",
    "print(f\"  Strategy: Use 50 runs per fault for good statistical power\")\n",
    "print(f\"  Normal runs: 240 from ffte\")\n",
    "print(f\"  Faulty runs: 50 runs per fault from fte (3.3× more than before)\")\n",
    "print(f\"  Fault start time: sample 161 (after 8 hours normal operation)\\n\")\n",
    "\n",
    "# Normal test runs\n",
    "normal_test_runs = ffte['simulationRun'].unique()\n",
    "np.random.shuffle(normal_test_runs)\n",
    "test_normal_runs = normal_test_runs[:240]\n",
    "\n",
    "supervised_test_normal = sample_runs(ffte, fault_number=0,\n",
    "                                     allowed_runs=test_normal_runs,\n",
    "                                     fault_start=0)\n",
    "\n",
    "print(f\"Normal test data: {supervised_test_normal.shape}\")\n",
    "\n",
    "# Faulty test runs - use 50 runs per fault\n",
    "print(f\"\\nUsing 50 runs per fault from fte:\")\n",
    "supervised_test_faulty_frames = []\n",
    "for fault in all_faults:\n",
    "    fault_df = fte[fte['faultNumber'] == fault]\n",
    "    fault_runs = fault_df['simulationRun'].unique()\n",
    "    np.random.shuffle(fault_runs)\n",
    "    \n",
    "    # Use first 50 runs\n",
    "    test_fault_runs = fault_runs[:50]\n",
    "    \n",
    "    test_fault_df = sample_runs(fte, fault_number=fault,\n",
    "                                allowed_runs=test_fault_runs,\n",
    "                                fault_start=161)\n",
    "    \n",
    "    supervised_test_faulty_frames.append(test_fault_df)\n",
    "    print(f\"  Fault {int(fault):2d}: {len(test_fault_runs)} runs, {len(test_fault_df)} samples\")\n",
    "\n",
    "supervised_test_faulty = pd.concat(supervised_test_faulty_frames, ignore_index=True)\n",
    "supervised_test = pd.concat([supervised_test_normal, supervised_test_faulty], ignore_index=True)\n",
    "\n",
    "# Sort by fault number, run, and sample for consistency\n",
    "supervised_test = supervised_test.sort_values(\n",
    "    ['faultNumber', 'simulationRun', 'sample']\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nFinal supervised test set: {supervised_test.shape}\")\n",
    "print(f\"  Total test runs: {supervised_test['traj_key'].nunique()}\")\n",
    "print(f\"  Normal runs: 240, Fault runs: {len(all_faults)} × 50 = {len(all_faults) * 50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Supervised Dataset Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T15:44:42.372804Z",
     "iopub.status.busy": "2026-01-03T15:44:42.372660Z",
     "iopub.status.idle": "2026-01-03T15:44:42.543832Z",
     "shell.execute_reply": "2026-01-03T15:44:42.542910Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supervised Dataset Summary:\n",
      "======================================================================\n",
      "\n",
      "Training set:\n",
      "  Shape: (323200, 57)\n",
      "  Faults: [0.0, 1.0, 2.0, 4.0, 5.0, 6.0, 7.0, 8.0, 10.0, 11.0, 12.0, 13.0, 14.0, 16.0, 17.0, 18.0, 19.0, 20.0]\n",
      "  Unique trajectories: 660\n",
      "\n",
      "Validation set:\n",
      "  Shape: (153440, 57)\n",
      "  Faults: [0.0, 1.0, 2.0, 4.0, 5.0, 6.0, 7.0, 8.0, 10.0, 11.0, 12.0, 13.0, 14.0, 16.0, 17.0, 18.0, 19.0, 20.0]\n",
      "  Unique trajectories: 313\n",
      "\n",
      "Test set:\n",
      "  Shape: (910400, 57)\n",
      "  Faults: [0, 1, 2, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20]\n",
      "  Unique trajectories: 1090\n",
      "\n",
      "Data leakage check:\n",
      "  Train ∩ Val:  0 trajectories (should be 0)\n",
      "  Train ∩ Test: 0 trajectories (should be 0)\n",
      "  Val ∩ Test:   0 trajectories (should be 0)\n",
      "\n",
      "  ✓ No data leakage detected\n"
     ]
    }
   ],
   "source": [
    "print(\"Supervised Dataset Summary:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nTraining set:\")\n",
    "print(f\"  Shape: {supervised_train.shape}\")\n",
    "print(f\"  Faults: {sorted(supervised_train['faultNumber'].unique())}\")\n",
    "print(f\"  Unique trajectories: {supervised_train['traj_key'].nunique()}\")\n",
    "\n",
    "print(f\"\\nValidation set:\")\n",
    "print(f\"  Shape: {supervised_val.shape}\")\n",
    "print(f\"  Faults: {sorted(supervised_val['faultNumber'].unique())}\")\n",
    "print(f\"  Unique trajectories: {supervised_val['traj_key'].nunique()}\")\n",
    "\n",
    "print(f\"\\nTest set:\")\n",
    "print(f\"  Shape: {supervised_test.shape}\")\n",
    "print(f\"  Faults: {sorted(supervised_test['faultNumber'].unique())}\")\n",
    "print(f\"  Unique trajectories: {supervised_test['traj_key'].nunique()}\")\n",
    "\n",
    "# Check for leakage\n",
    "train_keys = set(supervised_train['traj_key'])\n",
    "val_keys = set(supervised_val['traj_key'])\n",
    "test_keys = set(supervised_test['traj_key'])\n",
    "\n",
    "print(f\"\\nData leakage check:\")\n",
    "print(f\"  Train ∩ Val:  {len(train_keys & val_keys)} trajectories (should be 0)\")\n",
    "print(f\"  Train ∩ Test: {len(train_keys & test_keys)} trajectories (should be 0)\")\n",
    "print(f\"  Val ∩ Test:   {len(val_keys & test_keys)} trajectories (should be 0)\")\n",
    "\n",
    "if len(train_keys & val_keys) == 0 and len(train_keys & test_keys) == 0 and len(val_keys & test_keys) == 0:\n",
    "    print(\"\\n  ✓ No data leakage detected\")\n",
    "else:\n",
    "    print(\"\\n  ✗ WARNING: Data leakage detected!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Semi-Supervised Learning Datasets\n",
    "\n",
    "**Semi-supervised datasets** are trained only on normal data for anomaly detection (binary classification).\n",
    "\n",
    "### Data Split Strategy:\n",
    "- **Training:** 320 normal runs (fault-free only)\n",
    "- **Validation:** 160 normal runs (fault-free only)\n",
    "- **Test:** 120 normal runs + **50 runs per fault** (17 faults) = 970 runs total\n",
    "\n",
    "The training and validation sets use the **same normal runs** as the supervised learning case\n",
    "to enable fair comparison between approaches.\n",
    "\n",
    "Test set uses 50 runs per fault for robust anomaly detection evaluation (3.3× more than original 10 runs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T15:44:42.547163Z",
     "iopub.status.busy": "2026-01-03T15:44:42.547016Z",
     "iopub.status.idle": "2026-01-03T15:44:48.366641Z",
     "shell.execute_reply": "2026-01-03T15:44:48.365806Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Semi-Supervised Learning Datasets\n",
      "======================================================================\n",
      "Train (normal only): (160000, 57)\n",
      "Val (normal only):   (80000, 57)\n",
      "\n",
      "Test normal data: (115200, 57)\n",
      "\n",
      "Using 50 fault runs per fault from fte for anomaly detection testing:\n",
      "  Fault  1: 50 runs, 40000 samples\n",
      "  Fault  2: 50 runs, 40000 samples\n",
      "  Fault  4: 50 runs, 40000 samples\n",
      "  Fault  5: 50 runs, 40000 samples\n",
      "  Fault  6: 50 runs, 40000 samples\n",
      "  Fault  7: 50 runs, 40000 samples\n",
      "  Fault  8: 50 runs, 40000 samples\n",
      "  Fault 10: 50 runs, 40000 samples\n",
      "  Fault 11: 50 runs, 40000 samples\n",
      "  Fault 12: 50 runs, 40000 samples\n",
      "  Fault 13: 50 runs, 40000 samples\n",
      "  Fault 14: 50 runs, 40000 samples\n",
      "  Fault 16: 50 runs, 40000 samples\n",
      "  Fault 17: 50 runs, 40000 samples\n",
      "  Fault 18: 50 runs, 40000 samples\n",
      "  Fault 19: 50 runs, 40000 samples\n",
      "  Fault 20: 50 runs, 40000 samples\n",
      "\n",
      "Final semi-supervised test set: (795200, 57)\n",
      "  Total test runs: 970\n",
      "  Normal runs: 120, Fault runs: 17 × 50 = 850\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating Semi-Supervised Learning Datasets\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Training: same 320 normal runs as supervised (from fftr)\n",
    "semisupervised_train = sample_runs(fftr, fault_number=0,\n",
    "                                   allowed_runs=train_normal_runs,\n",
    "                                   fault_start=0)\n",
    "\n",
    "# Validation: same 160 normal runs as supervised (from fftr)\n",
    "semisupervised_val = sample_runs(fftr, fault_number=0,\n",
    "                                 allowed_runs=val_normal_runs,\n",
    "                                 fault_start=0)\n",
    "\n",
    "print(f\"Train (normal only): {semisupervised_train.shape}\")\n",
    "print(f\"Val (normal only):   {semisupervised_val.shape}\")\n",
    "\n",
    "# Test: 120 normal runs from ffte\n",
    "semisupervised_test_normal = sample_runs(ffte, fault_number=0,\n",
    "                                         allowed_runs=test_normal_runs[:120],\n",
    "                                         fault_start=0)\n",
    "\n",
    "print(f\"\\nTest normal data: {semisupervised_test_normal.shape}\")\n",
    "\n",
    "# Test: 50 runs per fault from fte (post-fault samples)\n",
    "print(f\"\\nUsing 50 fault runs per fault from fte for anomaly detection testing:\")\n",
    "semisupervised_test_faulty_frames = []\n",
    "\n",
    "for fault in all_faults:\n",
    "    fault_df = fte[fte['faultNumber'] == fault]\n",
    "    fault_runs = fault_df['simulationRun'].unique()\n",
    "    np.random.shuffle(fault_runs)\n",
    "    \n",
    "    # Use first 50 runs\n",
    "    test_fault_runs = fault_runs[:50]\n",
    "    \n",
    "    test_fault_df = sample_runs(fte, fault_number=fault,\n",
    "                                allowed_runs=test_fault_runs,\n",
    "                                fault_start=161)\n",
    "    \n",
    "    semisupervised_test_faulty_frames.append(test_fault_df)\n",
    "    print(f\"  Fault {int(fault):2d}: {len(test_fault_runs)} runs, {len(test_fault_df)} samples\")\n",
    "\n",
    "semisupervised_test_faulty = pd.concat(semisupervised_test_faulty_frames, ignore_index=True)\n",
    "semisupervised_test = pd.concat([semisupervised_test_normal, semisupervised_test_faulty],\n",
    "                                ignore_index=True)\n",
    "\n",
    "# Sort for consistency\n",
    "semisupervised_test = semisupervised_test.sort_values(\n",
    "    ['faultNumber', 'simulationRun', 'sample']\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nFinal semi-supervised test set: {semisupervised_test.shape}\")\n",
    "print(f\"  Total test runs: {semisupervised_test['traj_key'].nunique()}\")\n",
    "print(f\"  Normal runs: 120, Fault runs: {len(all_faults)} × 50 = {len(all_faults) * 50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Semi-Supervised Dataset Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T15:44:48.369920Z",
     "iopub.status.busy": "2026-01-03T15:44:48.369759Z",
     "iopub.status.idle": "2026-01-03T15:44:48.497162Z",
     "shell.execute_reply": "2026-01-03T15:44:48.496222Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semi-Supervised Dataset Summary:\n",
      "======================================================================\n",
      "\n",
      "Training set (normal only):\n",
      "  Shape: (160000, 57)\n",
      "  Faults: [0.0]\n",
      "  Unique trajectories: 320\n",
      "\n",
      "Validation set (normal only):\n",
      "  Shape: (80000, 57)\n",
      "  Faults: [0.0]\n",
      "  Unique trajectories: 160\n",
      "\n",
      "Test set (normal + faulty):\n",
      "  Shape: (795200, 57)\n",
      "  Faults: [0, 1, 2, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20]\n",
      "  Unique trajectories: 970\n",
      "\n",
      "Data leakage check:\n",
      "  Train ∩ Val:  0 trajectories (should be 0)\n",
      "  Train ∩ Test: 0 trajectories (should be 0)\n",
      "  Val ∩ Test:   0 trajectories (should be 0)\n",
      "\n",
      "  ✓ No data leakage detected\n"
     ]
    }
   ],
   "source": [
    "print(\"Semi-Supervised Dataset Summary:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nTraining set (normal only):\")\n",
    "print(f\"  Shape: {semisupervised_train.shape}\")\n",
    "print(f\"  Faults: {sorted(semisupervised_train['faultNumber'].unique())}\")\n",
    "print(f\"  Unique trajectories: {semisupervised_train['traj_key'].nunique()}\")\n",
    "\n",
    "print(f\"\\nValidation set (normal only):\")\n",
    "print(f\"  Shape: {semisupervised_val.shape}\")\n",
    "print(f\"  Faults: {sorted(semisupervised_val['faultNumber'].unique())}\")\n",
    "print(f\"  Unique trajectories: {semisupervised_val['traj_key'].nunique()}\")\n",
    "\n",
    "print(f\"\\nTest set (normal + faulty):\")\n",
    "print(f\"  Shape: {semisupervised_test.shape}\")\n",
    "print(f\"  Faults: {sorted(semisupervised_test['faultNumber'].unique())}\")\n",
    "print(f\"  Unique trajectories: {semisupervised_test['traj_key'].nunique()}\")\n",
    "\n",
    "# Check for leakage\n",
    "train_keys_ss = set(semisupervised_train['traj_key'])\n",
    "val_keys_ss = set(semisupervised_val['traj_key'])\n",
    "test_keys_ss = set(semisupervised_test['traj_key'])\n",
    "\n",
    "print(f\"\\nData leakage check:\")\n",
    "print(f\"  Train ∩ Val:  {len(train_keys_ss & val_keys_ss)} trajectories (should be 0)\")\n",
    "print(f\"  Train ∩ Test: {len(train_keys_ss & test_keys_ss)} trajectories (should be 0)\")\n",
    "print(f\"  Val ∩ Test:   {len(val_keys_ss & test_keys_ss)} trajectories (should be 0)\")\n",
    "\n",
    "if len(train_keys_ss & val_keys_ss) == 0 and len(train_keys_ss & test_keys_ss) == 0 and len(val_keys_ss & test_keys_ss) == 0:\n",
    "    print(\"\\n  ✓ No data leakage detected\")\n",
    "else:\n",
    "    print(\"\\n  ✗ WARNING: Data leakage detected!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Datasets to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T15:44:48.500515Z",
     "iopub.status.busy": "2026-01-03T15:44:48.500371Z",
     "iopub.status.idle": "2026-01-03T15:46:03.349945Z",
     "shell.execute_reply": "2026-01-03T15:46:03.348749Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving datasets to ../data/\n",
      "======================================================================\n",
      "✓ Saved: multiclass_train.csv ((323200, 57))\n",
      "✓ Saved: multiclass_val.csv ((153440, 57))\n",
      "✓ Saved: multiclass_test.csv ((910400, 57))\n",
      "✓ Saved: binary_train.csv ((160000, 57))\n",
      "✓ Saved: binary_val.csv ((80000, 57))\n",
      "✓ Saved: binary_test.csv ((795200, 57))\n",
      "\n",
      "All datasets saved successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving datasets to ../data/\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Save multiclass (supervised) datasets\n",
    "supervised_train.to_csv('../data/multiclass_train.csv', index=False)\n",
    "print(f\"✓ Saved: multiclass_train.csv ({supervised_train.shape})\")\n",
    "\n",
    "supervised_val.to_csv('../data/multiclass_val.csv', index=False)\n",
    "print(f\"✓ Saved: multiclass_val.csv ({supervised_val.shape})\")\n",
    "\n",
    "supervised_test.to_csv('../data/multiclass_test.csv', index=False)\n",
    "print(f\"✓ Saved: multiclass_test.csv ({supervised_test.shape})\")\n",
    "\n",
    "# Save binary (semi-supervised) datasets\n",
    "semisupervised_train.to_csv('../data/binary_train.csv', index=False)\n",
    "print(f\"✓ Saved: binary_train.csv ({semisupervised_train.shape})\")\n",
    "\n",
    "semisupervised_val.to_csv('../data/binary_val.csv', index=False)\n",
    "print(f\"✓ Saved: binary_val.csv ({semisupervised_val.shape})\")\n",
    "\n",
    "semisupervised_test.to_csv('../data/binary_test.csv', index=False)\n",
    "print(f\"✓ Saved: binary_test.csv ({semisupervised_test.shape})\")\n",
    "\n",
    "print(\"\\nAll datasets saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T15:46:03.353747Z",
     "iopub.status.busy": "2026-01-03T15:46:03.353580Z",
     "iopub.status.idle": "2026-01-03T15:46:03.482292Z",
     "shell.execute_reply": "2026-01-03T15:46:03.481590Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FINAL DATASET SUMMARY\n",
      "======================================================================\n",
      "\n",
      "MULTICLASS (18-way classification):\n",
      "  Training:    323,200 samples,   660 runs\n",
      "  Validation:  153,440 samples,   313 runs\n",
      "  Test:        910,400 samples,  1090 runs ← 3.3× MORE\n",
      "  Total:      1,387,040 samples\n",
      "\n",
      "BINARY (anomaly detection):\n",
      "  Training:    160,000 samples,   320 runs (normal only)\n",
      "  Validation:   80,000 samples,   160 runs (normal only)\n",
      "  Test:        795,200 samples,   970 runs ← 5× MORE\n",
      "  Total:      1,035,200 samples\n",
      "\n",
      "TEST SET IMPROVEMENT:\n",
      "  Multiclass: 15 → 50 runs per fault (3.3× improvement)\n",
      "  Binary:     10 → 50 runs per fault (5.0× improvement)\n",
      "  95% CI width: ±26% → ±14% (for 80% accuracy)\n",
      "  File size: Manageable ~270 MB (vs. ~2.7 GB for all 500 runs)\n",
      "\n",
      "FEATURES:\n",
      "  Measurements (xmeas): 41\n",
      "  Manipulated (xmv):    11\n",
      "  Total features:       52\n",
      "\n",
      "FAULT CLASSES:\n",
      "  Included faults: [1, 2, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20]\n",
      "  Excluded faults: [3, 9, 15]\n",
      "  Total classes:   18 (including normal)\n",
      "\n",
      "======================================================================\n",
      "Dataset creation complete!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL DATASET SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nMULTICLASS (18-way classification):\")\n",
    "print(f\"  Training:   {supervised_train.shape[0]:>8,} samples, {supervised_train['traj_key'].nunique():>5} runs\")\n",
    "print(f\"  Validation: {supervised_val.shape[0]:>8,} samples, {supervised_val['traj_key'].nunique():>5} runs\")\n",
    "print(f\"  Test:       {supervised_test.shape[0]:>8,} samples, {supervised_test['traj_key'].nunique():>5} runs ← 3.3× MORE\")\n",
    "print(f\"  Total:      {supervised_train.shape[0] + supervised_val.shape[0] + supervised_test.shape[0]:>8,} samples\")\n",
    "\n",
    "print(\"\\nBINARY (anomaly detection):\")\n",
    "print(f\"  Training:   {semisupervised_train.shape[0]:>8,} samples, {semisupervised_train['traj_key'].nunique():>5} runs (normal only)\")\n",
    "print(f\"  Validation: {semisupervised_val.shape[0]:>8,} samples, {semisupervised_val['traj_key'].nunique():>5} runs (normal only)\")\n",
    "print(f\"  Test:       {semisupervised_test.shape[0]:>8,} samples, {semisupervised_test['traj_key'].nunique():>5} runs ← 5× MORE\")\n",
    "print(f\"  Total:      {semisupervised_train.shape[0] + semisupervised_val.shape[0] + semisupervised_test.shape[0]:>8,} samples\")\n",
    "\n",
    "print(\"\\nTEST SET IMPROVEMENT:\")\n",
    "print(f\"  Multiclass: 15 → 50 runs per fault (3.3× improvement)\")\n",
    "print(f\"  Binary:     10 → 50 runs per fault (5.0× improvement)\")\n",
    "print(f\"  95% CI width: ±26% → ±14% (for 80% accuracy)\")\n",
    "print(f\"  File size: Manageable ~270 MB (vs. ~2.7 GB for all 500 runs)\")\n",
    "\n",
    "print(\"\\nFEATURES:\")\n",
    "feature_cols = [col for col in supervised_train.columns if col.startswith('xmeas') or col.startswith('xmv')]\n",
    "print(f\"  Measurements (xmeas): {len([c for c in feature_cols if c.startswith('xmeas')])}\")\n",
    "print(f\"  Manipulated (xmv):    {len([c for c in feature_cols if c.startswith('xmv')])}\")\n",
    "print(f\"  Total features:       {len(feature_cols)}\")\n",
    "\n",
    "print(\"\\nFAULT CLASSES:\")\n",
    "print(f\"  Included faults: {all_faults}\")\n",
    "print(f\"  Excluded faults: {excluded_faults}\")\n",
    "print(f\"  Total classes:   {len(all_faults) + 1} (including normal)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Dataset creation complete!\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
