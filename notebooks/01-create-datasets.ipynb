{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Creation: Tennessee Eastman Process (TEP)\n",
    "\n",
    "This notebook creates reproducible, **BALANCED** datasets for training and evaluating fault detection models.\n",
    "\n",
    "**Purpose:**\n",
    "- Load raw TEP data from .RData files\n",
    "- Create balanced train/validation/test splits with EQUAL samples per class\n",
    "- Generate both multiclass and binary classification datasets\n",
    "- Ensure no data leakage between splits\n",
    "\n",
    "**Data Source:**\n",
    "- Rieth et al. (2017) enriched TEP dataset\n",
    "- 4 source files: fault-free training/testing, faulty training/testing\n",
    "- Each contains 500 independent simulation runs per fault type\n",
    "\n",
    "**Outputs (MEDIUM SIZE - BALANCED CLASSES):**\n",
    "- `data/multiclass_train.csv` - 18 classes × 48,000 samples = 864,000 total (~330 MB)\n",
    "- `data/multiclass_val.csv` - 18 classes × 24,000 samples = 432,000 total (~165 MB)\n",
    "- `data/multiclass_test.csv` - 18 classes × 160,000 samples = 2,880,000 total (~1.1 GB)\n",
    "- `data/binary_train.csv` - 320 normal runs only\n",
    "- `data/binary_val.csv` - 160 normal runs only\n",
    "- `data/binary_test.csv` - 120 normal + 850 faulty runs (for anomaly detection)\n",
    "\n",
    "**Key Feature: PERFECT CLASS BALANCE + MEDIUM SIZE**\n",
    "All 18 classes in multiclass datasets have EXACTLY the same number of samples by:\n",
    "- Using 100 train / 50 val / 200 test runs per class (MEDIUM allocation)\n",
    "- Downsampling normal class from 500 → 480 samples/run (train/val)\n",
    "- Downsampling normal class from 960 → 800 samples/run (test)\n",
    "\n",
    "**Benefits of Medium Size:**\n",
    "- 5× more training data than minimal (excellent for deep learning)\n",
    "- 4× more test data than minimal (better statistical confidence)\n",
    "- Total: ~4.2M samples (~1.6 GB - still fast to train)\n",
    "- Uses only 0.3% of available RAM (plenty of headroom)\n",
    "\n",
    "**Reference:**\n",
    "Rieth, C. A., Amsel, B. D., Tran, R., & Cook, M. B. (2017). Additional Tennessee Eastman Process \n",
    "Simulation Data for Anomaly Detection Evaluation. Harvard Dataverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T18:15:06.670940Z",
     "iopub.status.busy": "2026-01-03T18:15:06.670584Z",
     "iopub.status.idle": "2026-01-03T18:15:07.056968Z",
     "shell.execute_reply": "2026-01-03T18:15:07.055919Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset creation notebook initialized\n",
      "Random seed: 42\n",
      "Output directory: ../data/\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyreadr\n",
    "import os\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create data output directory\n",
    "os.makedirs('../data', exist_ok=True)\n",
    "\n",
    "print(\"Dataset creation notebook initialized\")\n",
    "print(f\"Random seed: 42\")\n",
    "print(f\"Output directory: ../data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Raw Data from .RData Files\n",
    "\n",
    "The raw data is stored in compressed .RData.zip files. We'll extract and load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T18:15:07.117219Z",
     "iopub.status.busy": "2026-01-03T18:15:07.116747Z",
     "iopub.status.idle": "2026-01-03T18:15:07.124262Z",
     "shell.execute_reply": "2026-01-03T18:15:07.123394Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for data files:\n",
      "  ✓ fftr: TEP_FaultFree_Training.RData.zip (23.0 MB)\n",
      "  ✓ ftr: TEP_Faulty_Training.RData.zip (461.7 MB)\n",
      "  ✓ ffte: TEP_FaultFree_Testing.RData.zip (44.1 MB)\n",
      "  ✓ fte: TEP_Faulty_Testing.RData.zip (778.5 MB)\n"
     ]
    }
   ],
   "source": [
    "# Define paths to the raw data files\n",
    "dataset_dir = Path('../Dataset')\n",
    "\n",
    "data_files = {\n",
    "    'fftr': dataset_dir / 'TEP_FaultFree_Training.RData.zip',\n",
    "    'ftr': dataset_dir / 'TEP_Faulty_Training.RData.zip',\n",
    "    'ffte': dataset_dir / 'TEP_FaultFree_Testing.RData.zip',\n",
    "    'fte': dataset_dir / 'TEP_Faulty_Testing.RData.zip'\n",
    "}\n",
    "\n",
    "# Verify all files exist\n",
    "print(\"Checking for data files:\")\n",
    "for key, path in data_files.items():\n",
    "    if path.exists():\n",
    "        size_mb = path.stat().st_size / (1024 * 1024)\n",
    "        print(f\"  ✓ {key}: {path.name} ({size_mb:.1f} MB)\")\n",
    "    else:\n",
    "        print(f\"  ✗ {key}: {path.name} NOT FOUND\")\n",
    "        raise FileNotFoundError(f\"Required data file not found: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T18:15:07.127976Z",
     "iopub.status.busy": "2026-01-03T18:15:07.127657Z",
     "iopub.status.idle": "2026-01-03T18:17:01.174332Z",
     "shell.execute_reply": "2026-01-03T18:17:01.173205Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data files...\n",
      "This may take a few minutes due to file size.\n",
      "\n",
      "✓ Fault-free training loaded: (250000, 55)\n",
      "✓ Faulty training loaded: (5000000, 55)\n",
      "✓ Fault-free testing loaded: (480000, 55)\n",
      "✓ Faulty testing loaded: (9600000, 55)\n"
     ]
    }
   ],
   "source": [
    "def load_rdata_from_zip(zip_path, expected_key=None):\n",
    "    \"\"\"\n",
    "    Extract and load .RData file from zip archive.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    zip_path : Path\n",
    "        Path to the .zip file containing .RData\n",
    "    expected_key : str, optional\n",
    "        Expected key in the RData dictionary\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Loaded data\n",
    "    \"\"\"\n",
    "    # Extract the .RData file from zip\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        # Get the RData filename (should be only file in archive)\n",
    "        rdata_filename = [f for f in zip_ref.namelist() if f.endswith('.RData')][0]\n",
    "        \n",
    "        # Extract to temporary location\n",
    "        temp_path = zip_path.parent / rdata_filename\n",
    "        zip_ref.extract(rdata_filename, path=zip_path.parent)\n",
    "    \n",
    "    try:\n",
    "        # Load RData file\n",
    "        result = pyreadr.read_r(str(temp_path))\n",
    "        \n",
    "        # Get the dataframe (RData files contain OrderedDict)\n",
    "        if expected_key and expected_key in result:\n",
    "            df = result[expected_key]\n",
    "        else:\n",
    "            # Take the first (and typically only) dataframe\n",
    "            df = list(result.values())[0]\n",
    "        \n",
    "        return df\n",
    "    finally:\n",
    "        # Clean up extracted file\n",
    "        if temp_path.exists():\n",
    "            temp_path.unlink()\n",
    "\n",
    "print(\"Loading data files...\")\n",
    "print(\"This may take a few minutes due to file size.\\n\")\n",
    "\n",
    "# Load each dataset\n",
    "fftr = load_rdata_from_zip(data_files['fftr'], 'fault_free_training')\n",
    "print(f\"✓ Fault-free training loaded: {fftr.shape}\")\n",
    "\n",
    "ftr = load_rdata_from_zip(data_files['ftr'], 'faulty_training')\n",
    "print(f\"✓ Faulty training loaded: {ftr.shape}\")\n",
    "\n",
    "ffte = load_rdata_from_zip(data_files['ffte'], 'fault_free_testing')\n",
    "print(f\"✓ Fault-free testing loaded: {ffte.shape}\")\n",
    "\n",
    "fte = load_rdata_from_zip(data_files['fte'], 'faulty_testing')\n",
    "print(f\"✓ Faulty testing loaded: {fte.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Structure Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T18:17:01.178734Z",
     "iopub.status.busy": "2026-01-03T18:17:01.178541Z",
     "iopub.status.idle": "2026-01-03T18:17:01.425337Z",
     "shell.execute_reply": "2026-01-03T18:17:01.424655Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Structure:\n",
      "======================================================================\n",
      "\n",
      "Columns (55):\n",
      "['faultNumber', 'simulationRun', 'sample', 'xmeas_1', 'xmeas_2', 'xmeas_3', 'xmeas_4', 'xmeas_5', 'xmeas_6', 'xmeas_7', 'xmeas_8', 'xmeas_9', 'xmeas_10', 'xmeas_11', 'xmeas_12', 'xmeas_13', 'xmeas_14', 'xmeas_15', 'xmeas_16', 'xmeas_17', 'xmeas_18', 'xmeas_19', 'xmeas_20', 'xmeas_21', 'xmeas_22', 'xmeas_23', 'xmeas_24', 'xmeas_25', 'xmeas_26', 'xmeas_27', 'xmeas_28', 'xmeas_29', 'xmeas_30', 'xmeas_31', 'xmeas_32', 'xmeas_33', 'xmeas_34', 'xmeas_35', 'xmeas_36', 'xmeas_37', 'xmeas_38', 'xmeas_39', 'xmeas_40', 'xmeas_41', 'xmv_1', 'xmv_2', 'xmv_3', 'xmv_4', 'xmv_5', 'xmv_6', 'xmv_7', 'xmv_8', 'xmv_9', 'xmv_10', 'xmv_11']\n",
      "\n",
      "Fault-free training:\n",
      "  Unique simulation runs: 500\n",
      "  Samples per run: 500\n",
      "  Fault numbers: [0.0]\n",
      "\n",
      "Faulty training:\n",
      "  Unique simulation runs: 500\n",
      "  Samples per run: 500\n",
      "  Fault numbers: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "\n",
      "Fault-free testing:\n",
      "  Unique simulation runs: 500\n",
      "  Samples per run: 960\n",
      "  Fault numbers: [0]\n",
      "\n",
      "Faulty testing:\n",
      "  Unique simulation runs: 500\n",
      "  Samples per run: 960\n",
      "  Fault numbers: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "\n",
      "======================================================================\n",
      "Sample data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>faultNumber</th>\n",
       "      <th>simulationRun</th>\n",
       "      <th>sample</th>\n",
       "      <th>xmeas_1</th>\n",
       "      <th>xmeas_2</th>\n",
       "      <th>xmeas_3</th>\n",
       "      <th>xmeas_4</th>\n",
       "      <th>xmeas_5</th>\n",
       "      <th>xmeas_6</th>\n",
       "      <th>xmeas_7</th>\n",
       "      <th>...</th>\n",
       "      <th>xmv_2</th>\n",
       "      <th>xmv_3</th>\n",
       "      <th>xmv_4</th>\n",
       "      <th>xmv_5</th>\n",
       "      <th>xmv_6</th>\n",
       "      <th>xmv_7</th>\n",
       "      <th>xmv_8</th>\n",
       "      <th>xmv_9</th>\n",
       "      <th>xmv_10</th>\n",
       "      <th>xmv_11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.25038</td>\n",
       "      <td>3674.0</td>\n",
       "      <td>4529.0</td>\n",
       "      <td>9.2320</td>\n",
       "      <td>26.889</td>\n",
       "      <td>42.402</td>\n",
       "      <td>2704.3</td>\n",
       "      <td>...</td>\n",
       "      <td>53.744</td>\n",
       "      <td>24.657</td>\n",
       "      <td>62.544</td>\n",
       "      <td>22.137</td>\n",
       "      <td>39.935</td>\n",
       "      <td>42.323</td>\n",
       "      <td>47.757</td>\n",
       "      <td>47.510</td>\n",
       "      <td>41.258</td>\n",
       "      <td>18.447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.25109</td>\n",
       "      <td>3659.4</td>\n",
       "      <td>4556.6</td>\n",
       "      <td>9.4264</td>\n",
       "      <td>26.721</td>\n",
       "      <td>42.576</td>\n",
       "      <td>2705.0</td>\n",
       "      <td>...</td>\n",
       "      <td>53.414</td>\n",
       "      <td>24.588</td>\n",
       "      <td>59.259</td>\n",
       "      <td>22.084</td>\n",
       "      <td>40.176</td>\n",
       "      <td>38.554</td>\n",
       "      <td>43.692</td>\n",
       "      <td>47.427</td>\n",
       "      <td>41.359</td>\n",
       "      <td>17.194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.25038</td>\n",
       "      <td>3660.3</td>\n",
       "      <td>4477.8</td>\n",
       "      <td>9.4426</td>\n",
       "      <td>26.875</td>\n",
       "      <td>42.070</td>\n",
       "      <td>2706.2</td>\n",
       "      <td>...</td>\n",
       "      <td>54.357</td>\n",
       "      <td>24.666</td>\n",
       "      <td>61.275</td>\n",
       "      <td>22.380</td>\n",
       "      <td>40.244</td>\n",
       "      <td>38.990</td>\n",
       "      <td>46.699</td>\n",
       "      <td>47.468</td>\n",
       "      <td>41.199</td>\n",
       "      <td>20.530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.24977</td>\n",
       "      <td>3661.3</td>\n",
       "      <td>4512.1</td>\n",
       "      <td>9.4776</td>\n",
       "      <td>26.758</td>\n",
       "      <td>42.063</td>\n",
       "      <td>2707.2</td>\n",
       "      <td>...</td>\n",
       "      <td>53.946</td>\n",
       "      <td>24.725</td>\n",
       "      <td>59.856</td>\n",
       "      <td>22.277</td>\n",
       "      <td>40.257</td>\n",
       "      <td>38.072</td>\n",
       "      <td>47.541</td>\n",
       "      <td>47.658</td>\n",
       "      <td>41.643</td>\n",
       "      <td>18.089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.29405</td>\n",
       "      <td>3679.0</td>\n",
       "      <td>4497.0</td>\n",
       "      <td>9.3381</td>\n",
       "      <td>26.889</td>\n",
       "      <td>42.650</td>\n",
       "      <td>2705.1</td>\n",
       "      <td>...</td>\n",
       "      <td>53.658</td>\n",
       "      <td>28.797</td>\n",
       "      <td>60.717</td>\n",
       "      <td>21.947</td>\n",
       "      <td>39.144</td>\n",
       "      <td>41.955</td>\n",
       "      <td>47.645</td>\n",
       "      <td>47.346</td>\n",
       "      <td>41.507</td>\n",
       "      <td>18.461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   faultNumber  simulationRun  sample  xmeas_1  xmeas_2  xmeas_3  xmeas_4  \\\n",
       "0          0.0            1.0       1  0.25038   3674.0   4529.0   9.2320   \n",
       "1          0.0            1.0       2  0.25109   3659.4   4556.6   9.4264   \n",
       "2          0.0            1.0       3  0.25038   3660.3   4477.8   9.4426   \n",
       "3          0.0            1.0       4  0.24977   3661.3   4512.1   9.4776   \n",
       "4          0.0            1.0       5  0.29405   3679.0   4497.0   9.3381   \n",
       "\n",
       "   xmeas_5  xmeas_6  xmeas_7  ...   xmv_2   xmv_3   xmv_4   xmv_5   xmv_6  \\\n",
       "0   26.889   42.402   2704.3  ...  53.744  24.657  62.544  22.137  39.935   \n",
       "1   26.721   42.576   2705.0  ...  53.414  24.588  59.259  22.084  40.176   \n",
       "2   26.875   42.070   2706.2  ...  54.357  24.666  61.275  22.380  40.244   \n",
       "3   26.758   42.063   2707.2  ...  53.946  24.725  59.856  22.277  40.257   \n",
       "4   26.889   42.650   2705.1  ...  53.658  28.797  60.717  21.947  39.144   \n",
       "\n",
       "    xmv_7   xmv_8   xmv_9  xmv_10  xmv_11  \n",
       "0  42.323  47.757  47.510  41.258  18.447  \n",
       "1  38.554  43.692  47.427  41.359  17.194  \n",
       "2  38.990  46.699  47.468  41.199  20.530  \n",
       "3  38.072  47.541  47.658  41.643  18.089  \n",
       "4  41.955  47.645  47.346  41.507  18.461  \n",
       "\n",
       "[5 rows x 55 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Dataset Structure:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nColumns ({len(fftr.columns)}):\")\n",
    "print(fftr.columns.tolist())\n",
    "\n",
    "print(f\"\\nFault-free training:\")\n",
    "print(f\"  Unique simulation runs: {fftr['simulationRun'].nunique()}\")\n",
    "print(f\"  Samples per run: {fftr['sample'].nunique()}\")\n",
    "print(f\"  Fault numbers: {sorted(fftr['faultNumber'].unique())}\")\n",
    "\n",
    "print(f\"\\nFaulty training:\")\n",
    "print(f\"  Unique simulation runs: {ftr['simulationRun'].nunique()}\")\n",
    "print(f\"  Samples per run: {ftr['sample'].nunique()}\")\n",
    "print(f\"  Fault numbers: {sorted(ftr['faultNumber'].unique())}\")\n",
    "\n",
    "print(f\"\\nFault-free testing:\")\n",
    "print(f\"  Unique simulation runs: {ffte['simulationRun'].nunique()}\")\n",
    "print(f\"  Samples per run: {ffte['sample'].nunique()}\")\n",
    "print(f\"  Fault numbers: {sorted(ffte['faultNumber'].unique())}\")\n",
    "\n",
    "print(f\"\\nFaulty testing:\")\n",
    "print(f\"  Unique simulation runs: {fte['simulationRun'].nunique()}\")\n",
    "print(f\"  Samples per run: {fte['sample'].nunique()}\")\n",
    "print(f\"  Fault numbers: {sorted(fte['faultNumber'].unique())}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Sample data:\")\n",
    "fftr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Add Unique Trajectory Identifiers\n",
    "\n",
    "To prevent data leakage, we create unique identifiers for each simulation run\n",
    "that combine:\n",
    "- **origin**: Source dataset (fftr, ftr, ffte, fte)\n",
    "- **faultNumber**: Fault type (0-20)\n",
    "- **simulationRun**: Run ID (1-500)\n",
    "\n",
    "This ensures that simulation runs from different source files are kept separate,\n",
    "even if they have the same simulationRun number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T18:17:01.428747Z",
     "iopub.status.busy": "2026-01-03T18:17:01.428613Z",
     "iopub.status.idle": "2026-01-03T18:17:15.772410Z",
     "shell.execute_reply": "2026-01-03T18:17:15.771370Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique trajectory identifiers added.\n",
      "\n",
      "Example traj_key values:\n",
      "  fftr: fftr_f0_r1\n",
      "  ftr:  ftr_f1_r1\n",
      "  ffte: ffte_f0_r1\n",
      "  fte:  fte_f1_r1\n"
     ]
    }
   ],
   "source": [
    "def attach_origin_and_traj_key(df, origin_name):\n",
    "    \"\"\"\n",
    "    Add origin label and unique trajectory key to dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        TEP data\n",
    "    origin_name : str\n",
    "        Origin identifier (fftr, ftr, ffte, fte)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with added 'origin' and 'traj_key' columns\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['origin'] = origin_name\n",
    "    \n",
    "    # Create unique key: origin_f{fault}_r{run}\n",
    "    df['traj_key'] = (\n",
    "        df['origin'].astype(str)\n",
    "        + '_f' + df['faultNumber'].astype(int).astype(str)\n",
    "        + '_r' + df['simulationRun'].astype(int).astype(str)\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Add identifiers to all datasets\n",
    "fftr = attach_origin_and_traj_key(fftr, 'fftr')\n",
    "ftr = attach_origin_and_traj_key(ftr, 'ftr')\n",
    "ffte = attach_origin_and_traj_key(ffte, 'ffte')\n",
    "fte = attach_origin_and_traj_key(fte, 'fte')\n",
    "\n",
    "print(\"Unique trajectory identifiers added.\")\n",
    "print(f\"\\nExample traj_key values:\")\n",
    "print(f\"  fftr: {fftr['traj_key'].iloc[0]}\")\n",
    "print(f\"  ftr:  {ftr['traj_key'].iloc[0]}\")\n",
    "print(f\"  ffte: {ffte['traj_key'].iloc[0]}\")\n",
    "print(f\"  fte:  {fte['traj_key'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T18:17:15.775876Z",
     "iopub.status.busy": "2026-01-03T18:17:15.775704Z",
     "iopub.status.idle": "2026-01-03T18:17:17.108939Z",
     "shell.execute_reply": "2026-01-03T18:17:17.108043Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying no trajectory key overlaps between source datasets:\n",
      "  fftr ∩ ftr:   0 (should be 0)\n",
      "  fftr ∩ ffte:  0 (should be 0)\n",
      "  fftr ∩ fte:   0 (should be 0)\n",
      "  ftr  ∩ ffte:  0 (should be 0)\n",
      "  ftr  ∩ fte:   0 (should be 0)\n",
      "  ffte ∩ fte:   0 (should be 0)\n",
      "\n",
      "✓ All source datasets have unique trajectory keys (no leakage)\n"
     ]
    }
   ],
   "source": [
    "# Verify no overlaps between source datasets\n",
    "fftr_keys = set(fftr['traj_key'])\n",
    "ftr_keys = set(ftr['traj_key'])\n",
    "ffte_keys = set(ffte['traj_key'])\n",
    "fte_keys = set(fte['traj_key'])\n",
    "\n",
    "print(\"Verifying no trajectory key overlaps between source datasets:\")\n",
    "print(f\"  fftr ∩ ftr:   {len(fftr_keys & ftr_keys)} (should be 0)\")\n",
    "print(f\"  fftr ∩ ffte:  {len(fftr_keys & ffte_keys)} (should be 0)\")\n",
    "print(f\"  fftr ∩ fte:   {len(fftr_keys & fte_keys)} (should be 0)\")\n",
    "print(f\"  ftr  ∩ ffte:  {len(ftr_keys & ffte_keys)} (should be 0)\")\n",
    "print(f\"  ftr  ∩ fte:   {len(ftr_keys & fte_keys)} (should be 0)\")\n",
    "print(f\"  ffte ∩ fte:   {len(ffte_keys & fte_keys)} (should be 0)\")\n",
    "\n",
    "all_overlaps_zero = all([\n",
    "    len(fftr_keys & ftr_keys) == 0,\n",
    "    len(fftr_keys & ffte_keys) == 0,\n",
    "    len(fftr_keys & fte_keys) == 0,\n",
    "    len(ftr_keys & ffte_keys) == 0,\n",
    "    len(ftr_keys & fte_keys) == 0,\n",
    "    len(ffte_keys & fte_keys) == 0\n",
    "])\n",
    "\n",
    "if all_overlaps_zero:\n",
    "    print(\"\\n✓ All source datasets have unique trajectory keys (no leakage)\")\n",
    "else:\n",
    "    print(\"\\n✗ WARNING: Overlapping trajectory keys detected!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sampling Function\n",
    "\n",
    "This function samples simulation runs from a dataset while:\n",
    "- Ensuring specified runs are used (for split control)\n",
    "- For normal operation (fault 0): using the entire trajectory\n",
    "- For faulty operation: using only post-fault samples (after fault_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T18:17:17.112490Z",
     "iopub.status.busy": "2026-01-03T18:17:17.112332Z",
     "iopub.status.idle": "2026-01-03T18:17:17.117736Z",
     "shell.execute_reply": "2026-01-03T18:17:17.117137Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling function defined.\n"
     ]
    }
   ],
   "source": [
    "def sample_runs(df, fault_number, allowed_runs, fault_start=0):\n",
    "    \"\"\"\n",
    "    Sample specific simulation runs for a given fault number.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Source dataframe (fftr, ftr, ffte, or fte)\n",
    "    fault_number : int/float\n",
    "        Fault number to filter (0 for normal, 1-20 for faults)\n",
    "    allowed_runs : array-like\n",
    "        List of simulationRun IDs to include\n",
    "    fault_start : int\n",
    "        For faulty data, only keep samples >= this value (default: 0)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Sampled data\n",
    "    \"\"\"\n",
    "    # Filter by fault number and allowed runs\n",
    "    selected_df = df[\n",
    "        (df['faultNumber'] == fault_number) &\n",
    "        (df['simulationRun'].isin(allowed_runs))\n",
    "    ]\n",
    "    \n",
    "    frames = []\n",
    "    for run in allowed_runs:\n",
    "        run_df = selected_df[selected_df['simulationRun'] == run].sort_values('sample')\n",
    "        \n",
    "        if run_df.empty:\n",
    "            continue\n",
    "        \n",
    "        if fault_number == 0:\n",
    "            # Normal operation: use full trajectory\n",
    "            frames.append(run_df)\n",
    "        else:\n",
    "            # Faulty operation: use only post-fault segment\n",
    "            frames.append(run_df[run_df['sample'] >= fault_start])\n",
    "    \n",
    "    if not frames:\n",
    "        return pd.DataFrame(columns=df.columns)\n",
    "    \n",
    "    return pd.concat(frames, ignore_index=True)\n",
    "\n",
    "print(\"Sampling function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Supervised Learning Datasets\n",
    "\n",
    "**Supervised datasets** contain both normal and faulty examples for multiclass classification.\n",
    "\n",
    "### BALANCED CLASS STRATEGY - MEDIUM SIZE:\n",
    "All 18 classes (1 normal + 17 faults) will have **EXACTLY THE SAME** number of samples.\n",
    "\n",
    "### Constraints:\n",
    "- Normal runs: 500 samples/run (full trajectory) → downsample to 480\n",
    "- Fault runs: 480 samples/run (post-fault segment from sample 21)\n",
    "- Memory: Keep total dataset size manageable for fast training\n",
    "- Target: ~5× more data than minimal, still quick to iterate\n",
    "\n",
    "### Calculation for Balanced Classes:\n",
    "To balance classes, we must ensure: `normal_runs × 480 = fault_runs × 480`\n",
    "\n",
    "**Medium allocation per class:**\n",
    "- **Training:** 100 normal runs × 480 = 48,000 samples = 100 fault runs × 480 = 48,000 samples ✓\n",
    "- **Validation:** 50 normal runs × 480 = 24,000 samples = 50 fault runs × 480 = 24,000 samples ✓\n",
    "- **Test:** 200 normal runs × 800 = 160,000 samples = 200 fault runs × 800 = 160,000 samples ✓\n",
    "\n",
    "This gives us:\n",
    "- Training: 18 classes × 48,000 samples = 864,000 total samples (~330 MB)\n",
    "- Validation: 18 classes × 24,000 samples = 432,000 total samples (~165 MB)\n",
    "- Test: 18 classes × 160,000 samples = 2,880,000 total samples (~1.1 GB)\n",
    "- **Total: ~4.2M samples (~1.6 GB - fast training, excellent statistics)**\n",
    "\n",
    "### Benefits of Medium Size:\n",
    "- 5× more training data than minimal (48k vs 9.6k per class)\n",
    "- 4× more test data than minimal (160k vs 40k per class)\n",
    "- Better generalization for deep learning models\n",
    "- Still trains in reasonable time\n",
    "- Uses only 0.3% of available RAM\n",
    "\n",
    "### Fault Timing:\n",
    "- Training/validation faulty runs (from `ftr`): Fault occurs at sample 21\n",
    "- Test faulty runs (from `fte`): Fault occurs at sample 161 (after 8 hours normal operation)\n",
    "\n",
    "### Excluded Faults:\n",
    "Faults 3, 9, and 15 are excluded as they are too subtle to detect reliably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T18:17:17.120897Z",
     "iopub.status.busy": "2026-01-03T18:17:17.120746Z",
     "iopub.status.idle": "2026-01-03T18:17:17.324904Z",
     "shell.execute_reply": "2026-01-03T18:17:17.323990Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating BALANCED Supervised Learning Datasets - MEDIUM SIZE\n",
      "======================================================================\n",
      "Excluded faults: [3, 9, 15]\n",
      "Included faults: [1, 2, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20]\n",
      "Total fault classes (including normal): 18\n",
      "\n",
      "BALANCED CLASS STRATEGY - MEDIUM SIZE:\n",
      "  Training:   100 normal runs (48,000 samples) = 100 fault runs (48,000 samples)\n",
      "  Validation:  50 normal runs (24,000 samples) =  50 fault runs (24,000 samples)\n",
      "  Test:       200 normal runs (160,000 samples) = 200 fault runs (160,000 samples)\n",
      "\n",
      "Normal run allocation:\n",
      "  Training:   100 runs\n",
      "  Validation: 50 runs\n",
      "\n",
      "Downsampling normal class to match fault class size (480 samples/run):\n",
      "  Train: (48000, 57) → 100 runs × 480 = 48,000 samples\n",
      "  Val:   (24000, 57) → 50 runs × 480 = 24,000 samples\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating BALANCED Supervised Learning Datasets - MEDIUM SIZE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Define excluded faults\n",
    "excluded_faults = [3, 9, 15]\n",
    "all_faults = [f for f in sorted(ftr['faultNumber'].unique()) \n",
    "              if f not in excluded_faults]\n",
    "\n",
    "print(f\"Excluded faults: {excluded_faults}\")\n",
    "print(f\"Included faults: {all_faults}\")\n",
    "print(f\"Total fault classes (including normal): {len(all_faults) + 1}\\n\")\n",
    "\n",
    "print(\"BALANCED CLASS STRATEGY - MEDIUM SIZE:\")\n",
    "print(\"  Training:   100 normal runs (48,000 samples) = 100 fault runs (48,000 samples)\")\n",
    "print(\"  Validation:  50 normal runs (24,000 samples) =  50 fault runs (24,000 samples)\")\n",
    "print(\"  Test:       200 normal runs (160,000 samples) = 200 fault runs (160,000 samples)\")\n",
    "print()\n",
    "\n",
    "# 1. Normal (fault-free) data split from fftr\n",
    "all_normal_runs = fftr['simulationRun'].unique()\n",
    "np.random.shuffle(all_normal_runs)\n",
    "\n",
    "# Allocate: 100 train, 50 val (total 150 from 500 available)\n",
    "train_normal_runs = all_normal_runs[:100]   # 100 runs × 480 = 48,000 samples\n",
    "val_normal_runs = all_normal_runs[100:150]  # 50 runs × 480 = 24,000 samples\n",
    "\n",
    "print(f\"Normal run allocation:\")\n",
    "print(f\"  Training:   {len(train_normal_runs)} runs\")\n",
    "print(f\"  Validation: {len(val_normal_runs)} runs\")\n",
    "\n",
    "# Sample normal runs\n",
    "supervised_train = sample_runs(fftr, fault_number=0, \n",
    "                               allowed_runs=train_normal_runs, \n",
    "                               fault_start=0)\n",
    "\n",
    "supervised_val = sample_runs(fftr, fault_number=0, \n",
    "                             allowed_runs=val_normal_runs, \n",
    "                             fault_start=0)\n",
    "\n",
    "# Downsample normal class to exactly match fault class size (480 samples)\n",
    "print(f\"\\nDownsampling normal class to match fault class size (480 samples/run):\")\n",
    "supervised_train = supervised_train.groupby('traj_key').apply(\n",
    "    lambda x: x.iloc[:480] if len(x) >= 480 else x\n",
    ").reset_index(drop=True)\n",
    "\n",
    "supervised_val = supervised_val.groupby('traj_key').apply(\n",
    "    lambda x: x.iloc[:480] if len(x) >= 480 else x\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(f\"  Train: {supervised_train.shape} → {len(train_normal_runs)} runs × 480 = {len(supervised_train):,} samples\")\n",
    "print(f\"  Val:   {supervised_val.shape} → {len(val_normal_runs)} runs × 480 = {len(supervised_val):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T18:17:17.328144Z",
     "iopub.status.busy": "2026-01-03T18:17:17.328005Z",
     "iopub.status.idle": "2026-01-03T18:17:22.609787Z",
     "shell.execute_reply": "2026-01-03T18:17:22.608919Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sampling faulty runs from training source (ftr):\n",
      "  Fault start time: sample 21\n",
      "  Strategy: 100 runs for training, 50 runs for validation per fault\n",
      "\n",
      "  Fault  1: Train 100 runs (48,000 samples), Val 50 runs (24,000 samples)\n",
      "  Fault  2: Train 100 runs (48,000 samples), Val 50 runs (24,000 samples)\n",
      "  Fault  4: Train 100 runs (48,000 samples), Val 50 runs (24,000 samples)\n",
      "  Fault  5: Train 100 runs (48,000 samples), Val 50 runs (24,000 samples)\n",
      "  Fault  6: Train 100 runs (48,000 samples), Val 50 runs (24,000 samples)\n",
      "  Fault  7: Train 100 runs (48,000 samples), Val 50 runs (24,000 samples)\n",
      "  Fault  8: Train 100 runs (48,000 samples), Val 50 runs (24,000 samples)\n",
      "  Fault 10: Train 100 runs (48,000 samples), Val 50 runs (24,000 samples)\n",
      "  Fault 11: Train 100 runs (48,000 samples), Val 50 runs (24,000 samples)\n",
      "  Fault 12: Train 100 runs (48,000 samples), Val 50 runs (24,000 samples)\n",
      "  Fault 13: Train 100 runs (48,000 samples), Val 50 runs (24,000 samples)\n",
      "  Fault 14: Train 100 runs (48,000 samples), Val 50 runs (24,000 samples)\n",
      "  Fault 16: Train 100 runs (48,000 samples), Val 50 runs (24,000 samples)\n",
      "  Fault 17: Train 100 runs (48,000 samples), Val 50 runs (24,000 samples)\n",
      "  Fault 18: Train 100 runs (48,000 samples), Val 50 runs (24,000 samples)\n",
      "  Fault 19: Train 100 runs (48,000 samples), Val 50 runs (24,000 samples)\n",
      "  Fault 20: Train 100 runs (48,000 samples), Val 50 runs (24,000 samples)\n",
      "\n",
      "Supervised datasets after adding faulty runs:\n",
      "  Train: (864000, 57) = 864,000 samples\n",
      "  Val:   (432000, 57) = 432,000 samples\n"
     ]
    }
   ],
   "source": [
    "# 2. Faulty data split from ftr (training source)\n",
    "print(\"\\nSampling faulty runs from training source (ftr):\")\n",
    "print(f\"  Fault start time: sample 21\")\n",
    "print(f\"  Strategy: 100 runs for training, 50 runs for validation per fault\\n\")\n",
    "\n",
    "for fault in all_faults:\n",
    "    fault_df = ftr[ftr['faultNumber'] == fault]\n",
    "    fault_runs = fault_df['simulationRun'].unique()\n",
    "    np.random.shuffle(fault_runs)\n",
    "    \n",
    "    # Split: 100 train, 50 val (total 150 from ftr)\n",
    "    train_fault_runs = fault_runs[:100]\n",
    "    val_fault_runs = fault_runs[100:150]\n",
    "    \n",
    "    # Sample post-fault data (sample >= 21)\n",
    "    train_fault_df = sample_runs(ftr, fault_number=fault,\n",
    "                                 allowed_runs=train_fault_runs,\n",
    "                                 fault_start=21)\n",
    "    \n",
    "    val_fault_df = sample_runs(ftr, fault_number=fault,\n",
    "                               allowed_runs=val_fault_runs,\n",
    "                               fault_start=21)\n",
    "    \n",
    "    supervised_train = pd.concat([supervised_train, train_fault_df], ignore_index=True)\n",
    "    supervised_val = pd.concat([supervised_val, val_fault_df], ignore_index=True)\n",
    "    \n",
    "    print(f\"  Fault {int(fault):2d}: Train {len(train_fault_runs)} runs ({len(train_fault_df):,} samples), \"\n",
    "          f\"Val {len(val_fault_runs)} runs ({len(val_fault_df):,} samples)\")\n",
    "\n",
    "print(f\"\\nSupervised datasets after adding faulty runs:\")\n",
    "print(f\"  Train: {supervised_train.shape} = {len(supervised_train):,} samples\")\n",
    "print(f\"  Val:   {supervised_val.shape} = {len(supervised_val):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T18:17:22.613365Z",
     "iopub.status.busy": "2026-01-03T18:17:22.613226Z",
     "iopub.status.idle": "2026-01-03T18:17:31.002417Z",
     "shell.execute_reply": "2026-01-03T18:17:31.001549Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating BALANCED test set from ffte and fte:\n",
      "  Strategy: Match sample counts exactly across all 18 classes\n",
      "  Normal runs: 200 from ffte (160,000 samples)\n",
      "  Faulty runs: 200 runs per fault from fte (160,000 samples)\n",
      "  Fault start time: sample 161 (after 8 hours normal operation)\n",
      "\n",
      "Normal test data (balanced): (160000, 57)\n",
      "  200 runs × 800 samples = 160,000 samples\n",
      "\n",
      "Using 200 runs per fault from fte (for balanced 160k samples per class):\n",
      "  Fault  1: 200 runs, 160,000 samples\n",
      "  Fault  2: 200 runs, 160,000 samples\n",
      "  Fault  4: 200 runs, 160,000 samples\n",
      "  Fault  5: 200 runs, 160,000 samples\n",
      "  Fault  6: 200 runs, 160,000 samples\n",
      "  Fault  7: 200 runs, 160,000 samples\n",
      "  Fault  8: 200 runs, 160,000 samples\n",
      "  Fault 10: 200 runs, 160,000 samples\n",
      "  Fault 11: 200 runs, 160,000 samples\n",
      "  Fault 12: 200 runs, 160,000 samples\n",
      "  Fault 13: 200 runs, 160,000 samples\n",
      "  Fault 14: 200 runs, 160,000 samples\n",
      "  Fault 16: 200 runs, 160,000 samples\n",
      "  Fault 17: 200 runs, 160,000 samples\n",
      "  Fault 18: 200 runs, 160,000 samples\n",
      "  Fault 19: 200 runs, 160,000 samples\n",
      "  Fault 20: 200 runs, 160,000 samples\n",
      "\n",
      "Final BALANCED test set: (2880000, 57)\n",
      "  Total test runs: 3600\n",
      "  Per class: 160,000 samples (perfectly balanced)\n"
     ]
    }
   ],
   "source": [
    "# 3. Test set from ffte (normal) and fte (faulty) - BALANCED\n",
    "print(\"\\nCreating BALANCED test set from ffte and fte:\")\n",
    "print(f\"  Strategy: Match sample counts exactly across all 18 classes\")\n",
    "print(f\"  Normal runs: 200 from ffte (160,000 samples)\")\n",
    "print(f\"  Faulty runs: 200 runs per fault from fte (160,000 samples)\")\n",
    "print(f\"  Fault start time: sample 161 (after 8 hours normal operation)\\n\")\n",
    "\n",
    "# Normal test runs - use 200 runs\n",
    "normal_test_runs = ffte['simulationRun'].unique()\n",
    "np.random.shuffle(normal_test_runs)\n",
    "test_normal_runs = normal_test_runs[:200]\n",
    "\n",
    "supervised_test_normal = sample_runs(ffte, fault_number=0,\n",
    "                                     allowed_runs=test_normal_runs,\n",
    "                                     fault_start=0)\n",
    "\n",
    "# Downsample normal test to 800 samples/run (to match fault test runs)\n",
    "# Each fault run has 800 samples (960 total - 160 pre-fault)\n",
    "supervised_test_normal = supervised_test_normal.groupby('traj_key').apply(\n",
    "    lambda x: x.iloc[:800] if len(x) >= 800 else x\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(f\"Normal test data (balanced): {supervised_test_normal.shape}\")\n",
    "print(f\"  200 runs × 800 samples = {len(supervised_test_normal):,} samples\")\n",
    "\n",
    "# Faulty test runs - use 200 runs per fault for BALANCE\n",
    "print(f\"\\nUsing 200 runs per fault from fte (for balanced 160k samples per class):\")\n",
    "supervised_test_faulty_frames = []\n",
    "for fault in all_faults:\n",
    "    fault_df = fte[fte['faultNumber'] == fault]\n",
    "    fault_runs = fault_df['simulationRun'].unique()\n",
    "    np.random.shuffle(fault_runs)\n",
    "    \n",
    "    # Use 200 runs × 800 samples = 160,000 per class\n",
    "    test_fault_runs = fault_runs[:200]\n",
    "    \n",
    "    test_fault_df = sample_runs(fte, fault_number=fault,\n",
    "                                allowed_runs=test_fault_runs,\n",
    "                                fault_start=161)\n",
    "    \n",
    "    supervised_test_faulty_frames.append(test_fault_df)\n",
    "    print(f\"  Fault {int(fault):2d}: {len(test_fault_runs)} runs, {len(test_fault_df):,} samples\")\n",
    "\n",
    "supervised_test_faulty = pd.concat(supervised_test_faulty_frames, ignore_index=True)\n",
    "supervised_test = pd.concat([supervised_test_normal, supervised_test_faulty], ignore_index=True)\n",
    "\n",
    "# Sort by fault number, run, and sample for consistency\n",
    "supervised_test = supervised_test.sort_values(\n",
    "    ['faultNumber', 'simulationRun', 'sample']\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nFinal BALANCED test set: {supervised_test.shape}\")\n",
    "print(f\"  Total test runs: {supervised_test['traj_key'].nunique()}\")\n",
    "print(f\"  Per class: {len(supervised_test) // (len(all_faults) + 1):,} samples (perfectly balanced)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Supervised Dataset Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T18:17:31.006058Z",
     "iopub.status.busy": "2026-01-03T18:17:31.005918Z",
     "iopub.status.idle": "2026-01-03T18:17:31.503157Z",
     "shell.execute_reply": "2026-01-03T18:17:31.502412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supervised Dataset Summary:\n",
      "======================================================================\n",
      "\n",
      "Training set:\n",
      "  Shape: (864000, 57)\n",
      "  Faults: [0.0, 1.0, 2.0, 4.0, 5.0, 6.0, 7.0, 8.0, 10.0, 11.0, 12.0, 13.0, 14.0, 16.0, 17.0, 18.0, 19.0, 20.0]\n",
      "  Unique trajectories: 1800\n",
      "\n",
      "Validation set:\n",
      "  Shape: (432000, 57)\n",
      "  Faults: [0.0, 1.0, 2.0, 4.0, 5.0, 6.0, 7.0, 8.0, 10.0, 11.0, 12.0, 13.0, 14.0, 16.0, 17.0, 18.0, 19.0, 20.0]\n",
      "  Unique trajectories: 900\n",
      "\n",
      "Test set:\n",
      "  Shape: (2880000, 57)\n",
      "  Faults: [0, 1, 2, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20]\n",
      "  Unique trajectories: 3600\n",
      "\n",
      "Data leakage check:\n",
      "  Train ∩ Val:  0 trajectories (should be 0)\n",
      "  Train ∩ Test: 0 trajectories (should be 0)\n",
      "  Val ∩ Test:   0 trajectories (should be 0)\n",
      "\n",
      "  ✓ No data leakage detected\n"
     ]
    }
   ],
   "source": [
    "print(\"Supervised Dataset Summary:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nTraining set:\")\n",
    "print(f\"  Shape: {supervised_train.shape}\")\n",
    "print(f\"  Faults: {sorted(supervised_train['faultNumber'].unique())}\")\n",
    "print(f\"  Unique trajectories: {supervised_train['traj_key'].nunique()}\")\n",
    "\n",
    "print(f\"\\nValidation set:\")\n",
    "print(f\"  Shape: {supervised_val.shape}\")\n",
    "print(f\"  Faults: {sorted(supervised_val['faultNumber'].unique())}\")\n",
    "print(f\"  Unique trajectories: {supervised_val['traj_key'].nunique()}\")\n",
    "\n",
    "print(f\"\\nTest set:\")\n",
    "print(f\"  Shape: {supervised_test.shape}\")\n",
    "print(f\"  Faults: {sorted(supervised_test['faultNumber'].unique())}\")\n",
    "print(f\"  Unique trajectories: {supervised_test['traj_key'].nunique()}\")\n",
    "\n",
    "# Check for leakage\n",
    "train_keys = set(supervised_train['traj_key'])\n",
    "val_keys = set(supervised_val['traj_key'])\n",
    "test_keys = set(supervised_test['traj_key'])\n",
    "\n",
    "print(f\"\\nData leakage check:\")\n",
    "print(f\"  Train ∩ Val:  {len(train_keys & val_keys)} trajectories (should be 0)\")\n",
    "print(f\"  Train ∩ Test: {len(train_keys & test_keys)} trajectories (should be 0)\")\n",
    "print(f\"  Val ∩ Test:   {len(val_keys & test_keys)} trajectories (should be 0)\")\n",
    "\n",
    "if len(train_keys & val_keys) == 0 and len(train_keys & test_keys) == 0 and len(val_keys & test_keys) == 0:\n",
    "    print(\"\\n  ✓ No data leakage detected\")\n",
    "else:\n",
    "    print(\"\\n  ✗ WARNING: Data leakage detected!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Semi-Supervised Learning Datasets\n",
    "\n",
    "**Semi-supervised datasets** are trained only on normal data for anomaly detection (binary classification).\n",
    "\n",
    "### Data Split Strategy:\n",
    "- **Training:** 320 normal runs (fault-free only)\n",
    "- **Validation:** 160 normal runs (fault-free only)\n",
    "- **Test:** 120 normal runs + **50 runs per fault** (17 faults) = 970 runs total\n",
    "\n",
    "The training and validation sets use the **same normal runs** as the supervised learning case\n",
    "to enable fair comparison between approaches.\n",
    "\n",
    "Test set uses 50 runs per fault for robust anomaly detection evaluation (3.3× more than original 10 runs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T18:17:31.506276Z",
     "iopub.status.busy": "2026-01-03T18:17:31.506134Z",
     "iopub.status.idle": "2026-01-03T18:17:35.569241Z",
     "shell.execute_reply": "2026-01-03T18:17:35.568093Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Semi-Supervised Learning Datasets\n",
      "======================================================================\n",
      "Train (normal only): (50000, 57)\n",
      "Val (normal only):   (25000, 57)\n",
      "\n",
      "Test normal data: (115200, 57)\n",
      "\n",
      "Using 50 fault runs per fault from fte for anomaly detection testing:\n",
      "  Fault  1: 50 runs, 40000 samples\n",
      "  Fault  2: 50 runs, 40000 samples\n",
      "  Fault  4: 50 runs, 40000 samples\n",
      "  Fault  5: 50 runs, 40000 samples\n",
      "  Fault  6: 50 runs, 40000 samples\n",
      "  Fault  7: 50 runs, 40000 samples\n",
      "  Fault  8: 50 runs, 40000 samples\n",
      "  Fault 10: 50 runs, 40000 samples\n",
      "  Fault 11: 50 runs, 40000 samples\n",
      "  Fault 12: 50 runs, 40000 samples\n",
      "  Fault 13: 50 runs, 40000 samples\n",
      "  Fault 14: 50 runs, 40000 samples\n",
      "  Fault 16: 50 runs, 40000 samples\n",
      "  Fault 17: 50 runs, 40000 samples\n",
      "  Fault 18: 50 runs, 40000 samples\n",
      "  Fault 19: 50 runs, 40000 samples\n",
      "  Fault 20: 50 runs, 40000 samples\n",
      "\n",
      "Final semi-supervised test set: (795200, 57)\n",
      "  Total test runs: 970\n",
      "  Normal runs: 120, Fault runs: 17 × 50 = 850\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating Semi-Supervised Learning Datasets\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Training: same 320 normal runs as supervised (from fftr)\n",
    "semisupervised_train = sample_runs(fftr, fault_number=0,\n",
    "                                   allowed_runs=train_normal_runs,\n",
    "                                   fault_start=0)\n",
    "\n",
    "# Validation: same 160 normal runs as supervised (from fftr)\n",
    "semisupervised_val = sample_runs(fftr, fault_number=0,\n",
    "                                 allowed_runs=val_normal_runs,\n",
    "                                 fault_start=0)\n",
    "\n",
    "print(f\"Train (normal only): {semisupervised_train.shape}\")\n",
    "print(f\"Val (normal only):   {semisupervised_val.shape}\")\n",
    "\n",
    "# Test: 120 normal runs from ffte\n",
    "semisupervised_test_normal = sample_runs(ffte, fault_number=0,\n",
    "                                         allowed_runs=test_normal_runs[:120],\n",
    "                                         fault_start=0)\n",
    "\n",
    "print(f\"\\nTest normal data: {semisupervised_test_normal.shape}\")\n",
    "\n",
    "# Test: 50 runs per fault from fte (post-fault samples)\n",
    "print(f\"\\nUsing 50 fault runs per fault from fte for anomaly detection testing:\")\n",
    "semisupervised_test_faulty_frames = []\n",
    "\n",
    "for fault in all_faults:\n",
    "    fault_df = fte[fte['faultNumber'] == fault]\n",
    "    fault_runs = fault_df['simulationRun'].unique()\n",
    "    np.random.shuffle(fault_runs)\n",
    "    \n",
    "    # Use first 50 runs\n",
    "    test_fault_runs = fault_runs[:50]\n",
    "    \n",
    "    test_fault_df = sample_runs(fte, fault_number=fault,\n",
    "                                allowed_runs=test_fault_runs,\n",
    "                                fault_start=161)\n",
    "    \n",
    "    semisupervised_test_faulty_frames.append(test_fault_df)\n",
    "    print(f\"  Fault {int(fault):2d}: {len(test_fault_runs)} runs, {len(test_fault_df)} samples\")\n",
    "\n",
    "semisupervised_test_faulty = pd.concat(semisupervised_test_faulty_frames, ignore_index=True)\n",
    "semisupervised_test = pd.concat([semisupervised_test_normal, semisupervised_test_faulty],\n",
    "                                ignore_index=True)\n",
    "\n",
    "# Sort for consistency\n",
    "semisupervised_test = semisupervised_test.sort_values(\n",
    "    ['faultNumber', 'simulationRun', 'sample']\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nFinal semi-supervised test set: {semisupervised_test.shape}\")\n",
    "print(f\"  Total test runs: {semisupervised_test['traj_key'].nunique()}\")\n",
    "print(f\"  Normal runs: 120, Fault runs: {len(all_faults)} × 50 = {len(all_faults) * 50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Semi-Supervised Dataset Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T18:17:35.572515Z",
     "iopub.status.busy": "2026-01-03T18:17:35.572369Z",
     "iopub.status.idle": "2026-01-03T18:17:35.683025Z",
     "shell.execute_reply": "2026-01-03T18:17:35.682055Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semi-Supervised Dataset Summary:\n",
      "======================================================================\n",
      "\n",
      "Training set (normal only):\n",
      "  Shape: (50000, 57)\n",
      "  Faults: [0.0]\n",
      "  Unique trajectories: 100\n",
      "\n",
      "Validation set (normal only):\n",
      "  Shape: (25000, 57)\n",
      "  Faults: [0.0]\n",
      "  Unique trajectories: 50\n",
      "\n",
      "Test set (normal + faulty):\n",
      "  Shape: (795200, 57)\n",
      "  Faults: [0, 1, 2, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20]\n",
      "  Unique trajectories: 970\n",
      "\n",
      "Data leakage check:\n",
      "  Train ∩ Val:  0 trajectories (should be 0)\n",
      "  Train ∩ Test: 0 trajectories (should be 0)\n",
      "  Val ∩ Test:   0 trajectories (should be 0)\n",
      "\n",
      "  ✓ No data leakage detected\n"
     ]
    }
   ],
   "source": [
    "print(\"Semi-Supervised Dataset Summary:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nTraining set (normal only):\")\n",
    "print(f\"  Shape: {semisupervised_train.shape}\")\n",
    "print(f\"  Faults: {sorted(semisupervised_train['faultNumber'].unique())}\")\n",
    "print(f\"  Unique trajectories: {semisupervised_train['traj_key'].nunique()}\")\n",
    "\n",
    "print(f\"\\nValidation set (normal only):\")\n",
    "print(f\"  Shape: {semisupervised_val.shape}\")\n",
    "print(f\"  Faults: {sorted(semisupervised_val['faultNumber'].unique())}\")\n",
    "print(f\"  Unique trajectories: {semisupervised_val['traj_key'].nunique()}\")\n",
    "\n",
    "print(f\"\\nTest set (normal + faulty):\")\n",
    "print(f\"  Shape: {semisupervised_test.shape}\")\n",
    "print(f\"  Faults: {sorted(semisupervised_test['faultNumber'].unique())}\")\n",
    "print(f\"  Unique trajectories: {semisupervised_test['traj_key'].nunique()}\")\n",
    "\n",
    "# Check for leakage\n",
    "train_keys_ss = set(semisupervised_train['traj_key'])\n",
    "val_keys_ss = set(semisupervised_val['traj_key'])\n",
    "test_keys_ss = set(semisupervised_test['traj_key'])\n",
    "\n",
    "print(f\"\\nData leakage check:\")\n",
    "print(f\"  Train ∩ Val:  {len(train_keys_ss & val_keys_ss)} trajectories (should be 0)\")\n",
    "print(f\"  Train ∩ Test: {len(train_keys_ss & test_keys_ss)} trajectories (should be 0)\")\n",
    "print(f\"  Val ∩ Test:   {len(val_keys_ss & test_keys_ss)} trajectories (should be 0)\")\n",
    "\n",
    "if len(train_keys_ss & val_keys_ss) == 0 and len(train_keys_ss & test_keys_ss) == 0 and len(val_keys_ss & test_keys_ss) == 0:\n",
    "    print(\"\\n  ✓ No data leakage detected\")\n",
    "else:\n",
    "    print(\"\\n  ✗ WARNING: Data leakage detected!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Datasets to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T18:17:35.686673Z",
     "iopub.status.busy": "2026-01-03T18:17:35.686527Z",
     "iopub.status.idle": "2026-01-03T18:20:02.667971Z",
     "shell.execute_reply": "2026-01-03T18:20:02.667120Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving datasets to ../data/\n",
      "======================================================================\n",
      "✓ Saved: multiclass_train.csv ((864000, 57))\n",
      "✓ Saved: multiclass_val.csv ((432000, 57))\n",
      "✓ Saved: multiclass_test.csv ((2880000, 57))\n",
      "✓ Saved: binary_train.csv ((50000, 57))\n",
      "✓ Saved: binary_val.csv ((25000, 57))\n",
      "✓ Saved: binary_test.csv ((795200, 57))\n",
      "\n",
      "All datasets saved successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving datasets to ../data/\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Save multiclass (supervised) datasets\n",
    "supervised_train.to_csv('../data/multiclass_train.csv', index=False)\n",
    "print(f\"✓ Saved: multiclass_train.csv ({supervised_train.shape})\")\n",
    "\n",
    "supervised_val.to_csv('../data/multiclass_val.csv', index=False)\n",
    "print(f\"✓ Saved: multiclass_val.csv ({supervised_val.shape})\")\n",
    "\n",
    "supervised_test.to_csv('../data/multiclass_test.csv', index=False)\n",
    "print(f\"✓ Saved: multiclass_test.csv ({supervised_test.shape})\")\n",
    "\n",
    "# Save binary (semi-supervised) datasets\n",
    "semisupervised_train.to_csv('../data/binary_train.csv', index=False)\n",
    "print(f\"✓ Saved: binary_train.csv ({semisupervised_train.shape})\")\n",
    "\n",
    "semisupervised_val.to_csv('../data/binary_val.csv', index=False)\n",
    "print(f\"✓ Saved: binary_val.csv ({semisupervised_val.shape})\")\n",
    "\n",
    "semisupervised_test.to_csv('../data/binary_test.csv', index=False)\n",
    "print(f\"✓ Saved: binary_test.csv ({semisupervised_test.shape})\")\n",
    "\n",
    "print(\"\\nAll datasets saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T18:20:02.672193Z",
     "iopub.status.busy": "2026-01-03T18:20:02.672044Z",
     "iopub.status.idle": "2026-01-03T18:20:03.012519Z",
     "shell.execute_reply": "2026-01-03T18:20:03.011735Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FINAL DATASET SUMMARY\n",
      "======================================================================\n",
      "\n",
      "MULTICLASS (18-way classification) - PERFECTLY BALANCED:\n",
      "  Training:    864,000 samples,  1800 runs\n",
      "    Per class: 48,000 samples (all 18 classes equal)\n",
      "  Validation:  432,000 samples,   900 runs\n",
      "    Per class: 24,000 samples (all 18 classes equal)\n",
      "  Test:       2,880,000 samples,  3600 runs\n",
      "    Per class: 160,000 samples (all 18 classes equal)\n",
      "  Total:      4,176,000 samples\n",
      "\n",
      "BINARY (anomaly detection):\n",
      "  Training:     50,000 samples,   100 runs (normal only)\n",
      "  Validation:   25,000 samples,    50 runs (normal only)\n",
      "  Test:        795,200 samples,   970 runs\n",
      "  Total:       870,200 samples\n",
      "\n",
      "BALANCE VERIFICATION:\n",
      "  Training class balance: 48000 to 48000 (ratio: 1.00)\n",
      "  Validation class balance: 24000 to 24000 (ratio: 1.00)\n",
      "  Test class balance: 160000 to 160000 (ratio: 1.00)\n",
      "  ✓ PERFECT BALANCE ACHIEVED (zero variance across classes)\n",
      "\n",
      "FEATURES:\n",
      "  Measurements (xmeas): 41\n",
      "  Manipulated (xmv):    11\n",
      "  Total features:       52\n",
      "\n",
      "FAULT CLASSES:\n",
      "  Included faults: [1, 2, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20]\n",
      "  Excluded faults: [3, 9, 15]\n",
      "  Total classes:   18 (including normal)\n",
      "\n",
      "======================================================================\n",
      "Dataset creation complete - ALL CLASSES PERFECTLY BALANCED!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL DATASET SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Calculate per-class statistics\n",
    "train_per_class = supervised_train.groupby('faultNumber').size()\n",
    "val_per_class = supervised_val.groupby('faultNumber').size()\n",
    "test_per_class = supervised_test.groupby('faultNumber').size()\n",
    "\n",
    "print(\"\\nMULTICLASS (18-way classification) - PERFECTLY BALANCED:\")\n",
    "print(f\"  Training:   {supervised_train.shape[0]:>8,} samples, {supervised_train['traj_key'].nunique():>5} runs\")\n",
    "print(f\"    Per class: {train_per_class.values[0]:>6,} samples (all {len(train_per_class)} classes equal)\")\n",
    "print(f\"  Validation: {supervised_val.shape[0]:>8,} samples, {supervised_val['traj_key'].nunique():>5} runs\")\n",
    "print(f\"    Per class: {val_per_class.values[0]:>6,} samples (all {len(val_per_class)} classes equal)\")\n",
    "print(f\"  Test:       {supervised_test.shape[0]:>8,} samples, {supervised_test['traj_key'].nunique():>5} runs\")\n",
    "print(f\"    Per class: {test_per_class.values[0]:>6,} samples (all {len(test_per_class)} classes equal)\")\n",
    "print(f\"  Total:      {supervised_train.shape[0] + supervised_val.shape[0] + supervised_test.shape[0]:>8,} samples\")\n",
    "\n",
    "print(\"\\nBINARY (anomaly detection):\")\n",
    "print(f\"  Training:   {semisupervised_train.shape[0]:>8,} samples, {semisupervised_train['traj_key'].nunique():>5} runs (normal only)\")\n",
    "print(f\"  Validation: {semisupervised_val.shape[0]:>8,} samples, {semisupervised_val['traj_key'].nunique():>5} runs (normal only)\")\n",
    "print(f\"  Test:       {semisupervised_test.shape[0]:>8,} samples, {semisupervised_test['traj_key'].nunique():>5} runs\")\n",
    "print(f\"  Total:      {semisupervised_train.shape[0] + semisupervised_val.shape[0] + semisupervised_test.shape[0]:>8,} samples\")\n",
    "\n",
    "print(\"\\nBALANCE VERIFICATION:\")\n",
    "print(f\"  Training class balance: {train_per_class.min()} to {train_per_class.max()} (ratio: {train_per_class.max()/train_per_class.min():.2f})\")\n",
    "print(f\"  Validation class balance: {val_per_class.min()} to {val_per_class.max()} (ratio: {val_per_class.max()/val_per_class.min():.2f})\")\n",
    "print(f\"  Test class balance: {test_per_class.min()} to {test_per_class.max()} (ratio: {test_per_class.max()/test_per_class.min():.2f})\")\n",
    "if train_per_class.std() == 0 and val_per_class.std() == 0 and test_per_class.std() == 0:\n",
    "    print(\"  ✓ PERFECT BALANCE ACHIEVED (zero variance across classes)\")\n",
    "else:\n",
    "    print(f\"  Variance: Train={train_per_class.std():.1f}, Val={val_per_class.std():.1f}, Test={test_per_class.std():.1f}\")\n",
    "\n",
    "print(\"\\nFEATURES:\")\n",
    "feature_cols = [col for col in supervised_train.columns if col.startswith('xmeas') or col.startswith('xmv')]\n",
    "print(f\"  Measurements (xmeas): {len([c for c in feature_cols if c.startswith('xmeas')])}\")\n",
    "print(f\"  Manipulated (xmv):    {len([c for c in feature_cols if c.startswith('xmv')])}\")\n",
    "print(f\"  Total features:       {len(feature_cols)}\")\n",
    "\n",
    "print(\"\\nFAULT CLASSES:\")\n",
    "print(f\"  Included faults: {all_faults}\")\n",
    "print(f\"  Excluded faults: {excluded_faults}\")\n",
    "print(f\"  Total classes:   {len(all_faults) + 1} (including normal)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Dataset creation complete - ALL CLASSES PERFECTLY BALANCED!\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
