{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": "# LSTM-FCN with HMM Classification Filter\n\nThis notebook evaluates the effect of applying an HMM classification filter to LSTM-FCN predictions.\n\n**Method**: The HMM filter exploits temporal consistency using a sticky transition model.\n\n**Important**: The confusion matrix for the emission model is computed from the **validation set**, \nnot the test set. This ensures no test-time information leakage - the HMM filter parameters are \ndetermined entirely from data available during model development.\n\n**Stickiness values tested**: 0.7, 0.85, 0.95"
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## Configuration & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "config",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T20:57:12.968695Z",
     "iopub.status.busy": "2026-01-07T20:57:12.968457Z",
     "iopub.status.idle": "2026-01-07T20:57:15.953098Z",
     "shell.execute_reply": "2026-01-07T20:57:15.951758Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LSTM-FCN with HMM Classification Filter\n",
      "Device: cuda\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DATA_DIR = Path('../data')\n",
    "MODEL_DIR = Path('../outputs/models')\n",
    "METRICS_DIR = Path('../outputs/metrics')\n",
    "FIGURES_DIR = Path('../outputs/figures')\n",
    "\n",
    "METRICS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "STICKINESS_VALUES = [0.7, 0.85, 0.95]\n",
    "\n",
    "print('='*60)\n",
    "print('LSTM-FCN with HMM Classification Filter')\n",
    "print(f'Device: {device}')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filter-header",
   "metadata": {},
   "source": [
    "## HMM Classification Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "filter-class",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T20:57:15.957666Z",
     "iopub.status.busy": "2026-01-07T20:57:15.957259Z",
     "iopub.status.idle": "2026-01-07T20:57:15.965999Z",
     "shell.execute_reply": "2026-01-07T20:57:15.964826Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassificationFilter defined\n"
     ]
    }
   ],
   "source": [
    "class ClassificationFilter:\n",
    "    def __init__(self, n_classes, confusion_matrix=None, stickiness=0.9):\n",
    "        self.n_classes = n_classes\n",
    "        self.stickiness = stickiness\n",
    "        \n",
    "        if confusion_matrix is not None:\n",
    "            self.emission = confusion_matrix / confusion_matrix.sum(axis=1, keepdims=True)\n",
    "        else:\n",
    "            accuracy = 0.8\n",
    "            self.emission = np.full((n_classes, n_classes), (1 - accuracy) / (n_classes - 1))\n",
    "            np.fill_diagonal(self.emission, accuracy)\n",
    "        \n",
    "        self.transition = np.full((n_classes, n_classes), (1 - stickiness) / (n_classes - 1))\n",
    "        np.fill_diagonal(self.transition, stickiness)\n",
    "        \n",
    "        self.belief = np.ones(n_classes) / n_classes\n",
    "    \n",
    "    def reset(self):\n",
    "        self.belief = np.ones(self.n_classes) / self.n_classes\n",
    "    \n",
    "    def update(self, observation):\n",
    "        predicted_belief = self.transition.T @ self.belief\n",
    "        likelihood = self.emission[:, observation]\n",
    "        posterior = predicted_belief * likelihood\n",
    "        posterior_sum = posterior.sum()\n",
    "        if posterior_sum > 0:\n",
    "            posterior /= posterior_sum\n",
    "        else:\n",
    "            posterior = np.ones(self.n_classes) / self.n_classes\n",
    "        self.belief = posterior\n",
    "        return np.argmax(posterior), posterior\n",
    "    \n",
    "    def filter_sequence(self, observations, reset_on_run=None):\n",
    "        self.reset()\n",
    "        estimates = []\n",
    "        prev_run = None\n",
    "        \n",
    "        for i, obs in enumerate(observations):\n",
    "            if reset_on_run is not None:\n",
    "                current_run = reset_on_run[i]\n",
    "                if prev_run is not None and current_run != prev_run:\n",
    "                    self.reset()\n",
    "                prev_run = current_run\n",
    "            \n",
    "            est, _ = self.update(obs)\n",
    "            estimates.append(est)\n",
    "        \n",
    "        return np.array(estimates)\n",
    "\n",
    "print('ClassificationFilter defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-header",
   "metadata": {},
   "source": [
    "## Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "model-def",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T20:57:15.969217Z",
     "iopub.status.busy": "2026-01-07T20:57:15.969052Z",
     "iopub.status.idle": "2026-01-07T20:57:15.978068Z",
     "shell.execute_reply": "2026-01-07T20:57:15.976470Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model class defined\n"
     ]
    }
   ],
   "source": [
    "class LSTMFCN(nn.Module):\n",
    "    \"\"\"LSTM-FCN: Combines LSTM with 1D CNN for time series classification.\n",
    "    \n",
    "    Architecture matches 22-lstm-fcn-final-training.ipynb:\n",
    "    - Hierarchical conv filters (32→24→16) \n",
    "    - Dilation rates (1→2→4) for multi-scale feature extraction\n",
    "    - SpatialDropout after first conv layer\n",
    "    - Two-layer classifier head (fc1 + fc2)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, lstm_hidden, lstm_layers, dropout, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # LSTM branch\n",
    "        self.lstm = nn.LSTM(input_size, lstm_hidden, lstm_layers,\n",
    "                           batch_first=True, dropout=dropout if lstm_layers > 1 else 0)\n",
    "        \n",
    "        # FCN branch with hierarchical filters and dilation\n",
    "        self.conv1 = nn.Conv1d(input_size, 32, kernel_size=7, padding='same', dilation=1)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.spatial_dropout = nn.Dropout2d(0.3)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(32, 24, kernel_size=5, padding='same', dilation=2)\n",
    "        self.bn2 = nn.BatchNorm1d(24)\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(24, 16, kernel_size=3, padding='same', dilation=4)\n",
    "        self.bn3 = nn.BatchNorm1d(16)\n",
    "        \n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        # Classifier head (two layers)\n",
    "        combined_dim = lstm_hidden + 16\n",
    "        hidden_dim = max(32, num_classes * 2)\n",
    "        self.fc1 = nn.Linear(combined_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # LSTM branch: (batch, seq, features)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        \n",
    "        # FCN branch: (batch, features, seq)\n",
    "        x_cnn = x.permute(0, 2, 1)\n",
    "        \n",
    "        x_cnn = torch.relu(self.bn1(self.conv1(x_cnn)))\n",
    "        x_cnn = self.spatial_dropout(x_cnn.unsqueeze(-1)).squeeze(-1)\n",
    "        \n",
    "        x_cnn = torch.relu(self.bn2(self.conv2(x_cnn)))\n",
    "        x_cnn = torch.relu(self.bn3(self.conv3(x_cnn)))\n",
    "        \n",
    "        x_cnn = self.gap(x_cnn).squeeze(-1)\n",
    "        \n",
    "        combined = torch.cat([lstm_out, x_cnn], dim=1)\n",
    "        out = torch.relu(self.fc1(combined))\n",
    "        out = self.dropout(out)\n",
    "        return self.fc2(out)\n",
    "\n",
    "print('Model class defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-header",
   "metadata": {},
   "source": [
    "## Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "load-model",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T20:57:15.981162Z",
     "iopub.status.busy": "2026-01-07T20:57:15.980967Z",
     "iopub.status.idle": "2026-01-07T20:57:16.183323Z",
     "shell.execute_reply": "2026-01-07T20:57:16.181847Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Step 1/5] Loading trained LSTM-FCN model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: 52 features, 18 classes\n"
     ]
    }
   ],
   "source": [
    "print('\\n[Step 1/5] Loading trained LSTM-FCN model...')\n",
    "\n",
    "model_path = MODEL_DIR / 'lstm_fcn_final.pt'\n",
    "model_data = torch.load(model_path, map_location=device, weights_only=False)\n",
    "\n",
    "config = model_data['model_config']\n",
    "features = model_data['features']\n",
    "label_classes = model_data['label_encoder_classes']\n",
    "scaler_mean = model_data['scaler_mean']\n",
    "scaler_scale = model_data['scaler_scale']\n",
    "\n",
    "num_classes = len(label_classes)\n",
    "class_names = [str(int(c)) for c in label_classes]\n",
    "\n",
    "model = LSTMFCN(\n",
    "    input_size=len(features),\n",
    "    lstm_hidden=config['lstm_hidden'],\n",
    "    lstm_layers=config['lstm_layers'],\n",
    "    dropout=config['dropout'],\n",
    "    num_classes=num_classes\n",
    ")\n",
    "model.load_state_dict(model_data['model_state_dict'])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f'Model loaded: {len(features)} features, {num_classes} classes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T20:57:16.186687Z",
     "iopub.status.busy": "2026-01-07T20:57:16.186499Z",
     "iopub.status.idle": "2026-01-07T20:57:27.592346Z",
     "shell.execute_reply": "2026-01-07T20:57:27.589904Z"
    }
   },
   "outputs": [],
   "source": "print('\\n[Step 2/5] Loading validation and test datasets...')\nstart_time = time.time()\n\n# Load validation set for building HMM emission model\nval_data = pd.read_csv(DATA_DIR / 'multiclass_val.csv')\nprint(f'Validation data loaded: {val_data.shape}')\n\n# Load test set for final evaluation\ntest_data = pd.read_csv(DATA_DIR / 'multiclass_test.csv')\nprint(f'Test data loaded: {test_data.shape}')\n\nprint(f'Total load time: {time.time() - start_time:.2f}s')"
  },
  {
   "cell_type": "markdown",
   "id": "dataset-header",
   "metadata": {},
   "source": [
    "## Create Dataset and Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dataset-class",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T20:57:27.596718Z",
     "iopub.status.busy": "2026-01-07T20:57:27.596467Z",
     "iopub.status.idle": "2026-01-07T20:57:27.604492Z",
     "shell.execute_reply": "2026-01-07T20:57:27.603225Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset class defined\n"
     ]
    }
   ],
   "source": [
    "class SimulationRunDataset(Dataset):\n",
    "    def __init__(self, df, features, scaler_mean, scaler_scale, label_classes, sequence_length=10):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.class_to_idx = {c: i for i, c in enumerate(label_classes)}\n",
    "        \n",
    "        self.sequences = []\n",
    "        self.labels = []\n",
    "        self.run_ids = []\n",
    "        \n",
    "        for (fault, run), group in df.groupby(['faultNumber', 'simulationRun']):\n",
    "            group = group.sort_values('sample')\n",
    "            X = group[features].values\n",
    "            X = (X - scaler_mean) / scaler_scale\n",
    "            \n",
    "            run_id = f'{fault}_{run}'\n",
    "            for i in range(len(X) - sequence_length + 1):\n",
    "                self.sequences.append(X[i:i+sequence_length])\n",
    "                self.labels.append(self.class_to_idx[fault])\n",
    "                self.run_ids.append(run_id)\n",
    "        \n",
    "        self.sequences = np.array(self.sequences, dtype=np.float32)\n",
    "        self.labels = np.array(self.labels, dtype=np.int64)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sequences[idx]), torch.tensor(self.labels[idx])\n",
    "\n",
    "print('Dataset class defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-predictions",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T20:57:27.607311Z",
     "iopub.status.busy": "2026-01-07T20:57:27.607150Z",
     "iopub.status.idle": "2026-01-07T20:58:30.478979Z",
     "shell.execute_reply": "2026-01-07T20:58:30.477326Z"
    }
   },
   "outputs": [],
   "source": "print('\\n[Step 3/5] Generating predictions on validation and test sets...')\npred_start = time.time()\n\nseq_len = config['sequence_length']\n\n# === Validation set predictions (for HMM emission model) ===\nval_dataset = SimulationRunDataset(val_data, features, scaler_mean, scaler_scale, label_classes, seq_len)\nval_loader = DataLoader(val_dataset, batch_size=512, shuffle=False, num_workers=0)\n\nval_preds = []\nval_labels = []\n\nwith torch.no_grad():\n    for X_batch, y_batch in val_loader:\n        X_batch = X_batch.to(device)\n        outputs = model(X_batch)\n        preds = outputs.argmax(dim=1).cpu().numpy()\n        val_preds.extend(preds)\n        val_labels.extend(y_batch.numpy())\n\ny_pred_val = np.array(val_preds)\ny_val = np.array(val_labels)\n\nprint(f'Validation predictions: {accuracy_score(y_val, y_pred_val):.4f} accuracy')\n\n# === Test set predictions (for final evaluation) ===\ntest_dataset = SimulationRunDataset(test_data, features, scaler_mean, scaler_scale, label_classes, seq_len)\ntest_loader = DataLoader(test_dataset, batch_size=512, shuffle=False, num_workers=0)\n\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():\n    for X_batch, y_batch in test_loader:\n        X_batch = X_batch.to(device)\n        outputs = model(X_batch)\n        preds = outputs.argmax(dim=1).cpu().numpy()\n        all_preds.extend(preds)\n        all_labels.extend(y_batch.numpy())\n\ny_pred_raw = np.array(all_preds)\ny_test = np.array(all_labels)\nrun_ids = np.array(test_dataset.run_ids)\n\nprint(f'Test predictions: {accuracy_score(y_test, y_pred_raw):.4f} accuracy')\nprint(f'Total prediction time: {time.time() - pred_start:.2f}s')"
  },
  {
   "cell_type": "markdown",
   "id": "apply-header",
   "metadata": {},
   "source": [
    "## Apply HMM Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apply-filter",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T20:58:30.482464Z",
     "iopub.status.busy": "2026-01-07T20:58:30.482272Z",
     "iopub.status.idle": "2026-01-07T20:59:35.695360Z",
     "shell.execute_reply": "2026-01-07T20:59:35.693609Z"
    }
   },
   "outputs": [],
   "source": "print('\\n[Step 4/5] Applying HMM filter...')\n\n# Compute confusion matrix from VALIDATION set for HMM emission model\n# This ensures no test-time information leakage\ncm_val = confusion_matrix(y_val, y_pred_val)\nprint(f'Validation confusion matrix computed (accuracy: {np.diag(cm_val).sum() / cm_val.sum():.4f})')\n\n# Also compute test confusion matrix for visualization later\ncm_test_raw = confusion_matrix(y_test, y_pred_raw)\n\nfiltered_predictions = {}\n\nfor stickiness in STICKINESS_VALUES:\n    filter_start = time.time()\n    \n    # Use VALIDATION confusion matrix as emission model\n    hmm_filter = ClassificationFilter(\n        n_classes=num_classes,\n        confusion_matrix=cm_val.astype(float),  # Use validation CM, not test CM\n        stickiness=stickiness\n    )\n    \n    y_pred_filtered = hmm_filter.filter_sequence(y_pred_raw, reset_on_run=run_ids)\n    filtered_predictions[stickiness] = y_pred_filtered\n    \n    acc = accuracy_score(y_test, y_pred_filtered)\n    print(f'  Stickiness {stickiness}: Accuracy = {acc:.4f} (time: {time.time() - filter_start:.2f}s)')\n\nprint('\\nHMM filtering complete')"
  },
  {
   "cell_type": "markdown",
   "id": "metrics-header",
   "metadata": {},
   "source": [
    "## Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "compute-metrics",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T20:59:35.700166Z",
     "iopub.status.busy": "2026-01-07T20:59:35.699967Z",
     "iopub.status.idle": "2026-01-07T20:59:37.838503Z",
     "shell.execute_reply": "2026-01-07T20:59:37.836487Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Step 5/5] Computing metrics...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "METRICS COMPARISON: RAW vs HMM-FILTERED\n",
      "================================================================================\n",
      "\n",
      "Method                      Accuracy    Bal.Acc      F1(W)\n",
      "-------------------------------------------------------\n",
      "Raw::::::::::::::::::::::     0.9937     0.9937     0.9937\n",
      "HMM (y=0.7)                   0.9937     0.9937     0.9937\n",
      "HMM (y=0.85)                  0.9937     0.9937     0.9937\n",
      "HMM (y=0.95)                  0.9937     0.9937     0.9937\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print('\\n[Step 5/5] Computing metrics...')\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'balanced_accuracy': balanced_accuracy_score(y_true, y_pred),\n",
    "        'f1_weighted': f1_score(y_true, y_pred, average='weighted'),\n",
    "        'f1_macro': f1_score(y_true, y_pred, average='macro'),\n",
    "        'per_class_f1': dict(zip(class_names, f1_score(y_true, y_pred, average=None)))\n",
    "    }\n",
    "\n",
    "raw_metrics = compute_metrics(y_test, y_pred_raw)\n",
    "filtered_metrics = {s: compute_metrics(y_test, filtered_predictions[s]) for s in STICKINESS_VALUES}\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('METRICS COMPARISON: RAW vs HMM-FILTERED')\n",
    "print('='*80)\n",
    "print(f\"\\n{'Method':<25} {'Accuracy':>10} {'Bal.Acc':>10} {'F1(W)':>10}\")\n",
    "print('-'*55)\n",
    "print(f\"{'Raw'::<25} {raw_metrics['accuracy']:>10.4f} {raw_metrics['balanced_accuracy']:>10.4f} {raw_metrics['f1_weighted']:>10.4f}\")\n",
    "\n",
    "for stickiness in STICKINESS_VALUES:\n",
    "    m = filtered_metrics[stickiness]\n",
    "    print(f\"{'HMM (y=' + str(stickiness) + ')':<25} {m['accuracy']:>10.4f} {m['balanced_accuracy']:>10.4f} {m['f1_weighted']:>10.4f}\")\n",
    "\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-header",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confusion-viz",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T20:59:37.842319Z",
     "iopub.status.busy": "2026-01-07T20:59:37.842130Z",
     "iopub.status.idle": "2026-01-07T20:59:40.885078Z",
     "shell.execute_reply": "2026-01-07T20:59:40.883767Z"
    }
   },
   "outputs": [],
   "source": "best_stickiness = max(STICKINESS_VALUES, key=lambda s: filtered_metrics[s]['accuracy'])\n\nfig, axes = plt.subplots(1, 2, figsize=(16, 7))\n\n# Use test confusion matrix for visualization\ncm_raw_norm = cm_test_raw.astype('float') / cm_test_raw.sum(axis=1)[:, np.newaxis]\nsns.heatmap(cm_raw_norm, annot=True, fmt='.2f', cmap='Blues',\n            xticklabels=class_names, yticklabels=class_names, ax=axes[0], vmin=0, vmax=1)\naxes[0].set_xlabel('Predicted')\naxes[0].set_ylabel('Actual')\naxes[0].set_title(f'Raw LSTM-FCN (Acc: {raw_metrics[\"accuracy\"]:.4f})')\n\ncm_hmm = confusion_matrix(y_test, filtered_predictions[best_stickiness])\ncm_hmm_norm = cm_hmm.astype('float') / cm_hmm.sum(axis=1)[:, np.newaxis]\nsns.heatmap(cm_hmm_norm, annot=True, fmt='.2f', cmap='Blues',\n            xticklabels=class_names, yticklabels=class_names, ax=axes[1], vmin=0, vmax=1)\naxes[1].set_xlabel('Predicted')\naxes[1].set_ylabel('Actual')\naxes[1].set_title(f'HMM Filtered (γ={best_stickiness}, Acc: {filtered_metrics[best_stickiness][\"accuracy\"]:.4f})')\n\nplt.suptitle('LSTM-FCN Confusion Matrices: Raw vs HMM-Filtered', fontsize=14, y=1.02)\nplt.tight_layout()\noutput_file = FIGURES_DIR / 'lstm_fcn_hmm_confusion_matrices.png'\nplt.savefig(output_file, dpi=150, bbox_inches='tight')\nplt.show()\nprint(f'Saved to {output_file}')"
  },
  {
   "cell_type": "markdown",
   "id": "save-header",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-results",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T20:59:40.888758Z",
     "iopub.status.busy": "2026-01-07T20:59:40.888592Z",
     "iopub.status.idle": "2026-01-07T20:59:40.904487Z",
     "shell.execute_reply": "2026-01-07T20:59:40.903231Z"
    }
   },
   "outputs": [],
   "source": "best_metrics = filtered_metrics[best_stickiness]\n\nresults = {\n    'model': 'LSTM-FCN',\n    'method': 'HMM Classification Filter',\n    'emission_source': 'validation_set',  # Document that we used validation CM\n    'test_samples': len(y_test),\n    'val_samples': len(y_val),\n    'n_classes': num_classes,\n    'raw_metrics': raw_metrics,\n    'filtered_metrics': {str(k): v for k, v in filtered_metrics.items()},\n    'best_stickiness': best_stickiness,\n    'best_improvement': {\n        'accuracy_delta': best_metrics['accuracy'] - raw_metrics['accuracy'],\n        'f1_weighted_delta': best_metrics['f1_weighted'] - raw_metrics['f1_weighted'],\n        'error_reduction_pct': ((1 - raw_metrics['accuracy']) - (1 - best_metrics['accuracy'])) / (1 - raw_metrics['accuracy']) * 100 if raw_metrics['accuracy'] < 1 else 0\n    },\n    'stickiness_values_tested': STICKINESS_VALUES\n}\n\noutput_file = METRICS_DIR / 'lstm_fcn_hmm_filter_results.json'\nwith open(output_file, 'w') as f:\n    json.dump(results, f, indent=2)\nprint(f'Saved to {output_file}')"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "summary",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T20:59:40.907473Z",
     "iopub.status.busy": "2026-01-07T20:59:40.907316Z",
     "iopub.status.idle": "2026-01-07T20:59:40.912181Z",
     "shell.execute_reply": "2026-01-07T20:59:40.910738Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LSTM-FCN HMM Classification Filter Complete!\n",
      "============================================================\n",
      "\n",
      "Raw Accuracy: 0.9937\n",
      "Best HMM (y=0.7): 0.9937\n",
      "Improvement: +0.0000\n",
      "Error Reduction: 0.1%\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print('\\n' + '='*60)\n",
    "print('LSTM-FCN HMM Classification Filter Complete!')\n",
    "print('='*60)\n",
    "print(f'\\nRaw Accuracy: {raw_metrics[\"accuracy\"]:.4f}')\n",
    "print(f'Best HMM (y={best_stickiness}): {best_metrics[\"accuracy\"]:.4f}')\n",
    "print(f'Improvement: {results[\"best_improvement\"][\"accuracy_delta\"]:+.4f}')\n",
    "print(f'Error Reduction: {results[\"best_improvement\"][\"error_reduction_pct\"]:.1f}%')\n",
    "print('='*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}