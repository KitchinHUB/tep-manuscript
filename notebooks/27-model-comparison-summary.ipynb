{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Model Comparison Summary\n",
    "\n",
    "This notebook compares all trained models and generates summary tables and figures for the manuscript.\n",
    "\n",
    "**Models Compared**:\n",
    "- **Multiclass Classification** (18 fault classes):\n",
    "  - XGBoost\n",
    "  - LSTM\n",
    "  - LSTM-FCN\n",
    "  - CNN-Transformer\n",
    "  - TransKal (Transformer + Kalman Filter)\n",
    "  \n",
    "- **Binary Anomaly Detection** (normal vs fault):\n",
    "  - LSTM Autoencoder\n",
    "  - Convolutional Autoencoder\n",
    "\n",
    "**Outputs**:\n",
    "- Summary tables: `outputs/metrics/model_comparison_*.csv`\n",
    "- Comparison figures: `outputs/figures/model_comparison_*.png`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Model Comparison Summary\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Paths\n",
    "OUTPUT_DIR = Path('../outputs')\n",
    "METRICS_DIR = OUTPUT_DIR / 'metrics'\n",
    "FIGURES_DIR = OUTPUT_DIR / 'figures'\n",
    "\n",
    "# Model definitions\n",
    "MULTICLASS_MODELS = ['xgboost', 'lstm', 'lstm_fcn', 'cnn_transformer', 'transkal']\n",
    "BINARY_MODELS = ['lstm_autoencoder', 'conv_autoencoder']\n",
    "\n",
    "MODEL_DISPLAY_NAMES = {\n",
    "    'xgboost': 'XGBoost',\n",
    "    'lstm': 'LSTM',\n",
    "    'lstm_fcn': 'LSTM-FCN',\n",
    "    'cnn_transformer': 'CNN-Transformer',\n",
    "    'transkal': 'TransKal',\n",
    "    'lstm_autoencoder': 'LSTM-AE',\n",
    "    'conv_autoencoder': 'Conv-AE'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-header",
   "metadata": {},
   "source": [
    "## Load Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-multiclass",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLoading multiclass model metrics...\")\n",
    "\n",
    "multiclass_metrics = {}\n",
    "for model in MULTICLASS_MODELS:\n",
    "    metrics_file = METRICS_DIR / f'{model}_metrics.json'\n",
    "    if metrics_file.exists():\n",
    "        with open(metrics_file) as f:\n",
    "            multiclass_metrics[model] = json.load(f)\n",
    "        print(f\"  ✓ Loaded {model}\")\n",
    "    else:\n",
    "        print(f\"  ✗ Missing {model} (file: {metrics_file})\")\n",
    "\n",
    "print(f\"\\nLoaded {len(multiclass_metrics)}/{len(MULTICLASS_MODELS)} multiclass models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-binary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLoading binary model metrics...\")\n",
    "\n",
    "binary_metrics = {}\n",
    "for model in BINARY_MODELS:\n",
    "    metrics_file = METRICS_DIR / f'{model}_metrics.json'\n",
    "    if metrics_file.exists():\n",
    "        with open(metrics_file) as f:\n",
    "            binary_metrics[model] = json.load(f)\n",
    "        print(f\"  ✓ Loaded {model}\")\n",
    "    else:\n",
    "        print(f\"  ✗ Missing {model} (file: {metrics_file})\")\n",
    "\n",
    "print(f\"\\nLoaded {len(binary_metrics)}/{len(BINARY_MODELS)} binary models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiclass-header",
   "metadata": {},
   "source": [
    "## Multiclass Classification Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiclass-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MULTICLASS CLASSIFICATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Build comparison table\n",
    "rows = []\n",
    "for model in MULTICLASS_MODELS:\n",
    "    if model not in multiclass_metrics:\n",
    "        continue\n",
    "    m = multiclass_metrics[model]\n",
    "    rows.append({\n",
    "        'Model': MODEL_DISPLAY_NAMES[model],\n",
    "        'Accuracy': m['accuracy'],\n",
    "        'Balanced Acc': m['balanced_accuracy'],\n",
    "        'F1 (Weighted)': m['f1_weighted'],\n",
    "        'F1 (Macro)': m['f1_macro'],\n",
    "        'Precision': m['precision_weighted'],\n",
    "        'Recall': m['recall_weighted'],\n",
    "        'Training Time (s)': m.get('training_time_seconds', 0)\n",
    "    })\n",
    "\n",
    "multiclass_df = pd.DataFrame(rows)\n",
    "multiclass_df = multiclass_df.set_index('Model')\n",
    "\n",
    "# Format for display\n",
    "display_df = multiclass_df.copy()\n",
    "for col in ['Accuracy', 'Balanced Acc', 'F1 (Weighted)', 'F1 (Macro)', 'Precision', 'Recall']:\n",
    "    display_df[col] = display_df[col].apply(lambda x: f\"{x:.4f}\")\n",
    "display_df['Training Time (s)'] = display_df['Training Time (s)'].apply(lambda x: f\"{x:.1f}\")\n",
    "\n",
    "print(\"\\n\" + display_df.to_string())\n",
    "\n",
    "# Save to CSV\n",
    "multiclass_df.to_csv(METRICS_DIR / 'model_comparison_multiclass.csv')\n",
    "print(f\"\\n✓ Saved to {METRICS_DIR / 'model_comparison_multiclass.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiclass-best",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highlight best model for each metric\n",
    "print(\"\\nBest Model by Metric:\")\n",
    "print(\"-\" * 40)\n",
    "for col in ['Accuracy', 'Balanced Acc', 'F1 (Weighted)', 'F1 (Macro)', 'Precision', 'Recall']:\n",
    "    best_model = multiclass_df[col].idxmax()\n",
    "    best_value = multiclass_df[col].max()\n",
    "    print(f\"  {col}: {best_model} ({best_value:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiclass-bar-chart",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "models = multiclass_df.index.tolist()\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "ax = axes[0]\n",
    "bars1 = ax.bar(x - width/2, multiclass_df['Accuracy'], width, label='Accuracy', color='steelblue')\n",
    "bars2 = ax.bar(x + width/2, multiclass_df['Balanced Acc'], width, label='Balanced Accuracy', color='coral')\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Accuracy Comparison (Multiclass)')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models, rotation=15, ha='right')\n",
    "ax.legend()\n",
    "ax.set_ylim(0.8, 1.0)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# F1 comparison\n",
    "ax = axes[1]\n",
    "bars1 = ax.bar(x - width/2, multiclass_df['F1 (Weighted)'], width, label='F1 (Weighted)', color='steelblue')\n",
    "bars2 = ax.bar(x + width/2, multiclass_df['F1 (Macro)'], width, label='F1 (Macro)', color='coral')\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('F1 Score')\n",
    "ax.set_title('F1 Score Comparison (Multiclass)')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models, rotation=15, ha='right')\n",
    "ax.legend()\n",
    "ax.set_ylim(0.8, 1.0)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'model_comparison_multiclass_bars.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"✓ Saved to {FIGURES_DIR / 'model_comparison_multiclass_bars.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiclass-radar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radar chart\n",
    "from math import pi\n",
    "\n",
    "metrics_cols = ['Accuracy', 'Balanced Acc', 'F1 (Weighted)', 'F1 (Macro)', 'Precision', 'Recall']\n",
    "N = len(metrics_cols)\n",
    "\n",
    "angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "angles += angles[:1]  # Complete the loop\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))\n",
    "\n",
    "colors = plt.cm.husl(np.linspace(0, 1, len(multiclass_df)))\n",
    "\n",
    "for idx, (model, row) in enumerate(multiclass_df.iterrows()):\n",
    "    values = [row[col] for col in metrics_cols]\n",
    "    values += values[:1]  # Complete the loop\n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=model, color=colors[idx])\n",
    "    ax.fill(angles, values, alpha=0.1, color=colors[idx])\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(metrics_cols)\n",
    "ax.set_ylim(0.85, 1.0)\n",
    "ax.set_title('Multiclass Model Comparison', size=14, y=1.08)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'model_comparison_multiclass_radar.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"✓ Saved to {FIGURES_DIR / 'model_comparison_multiclass_radar.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perclass-header",
   "metadata": {},
   "source": [
    "## Per-Class F1 Score Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perclass-heatmap",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract per-class F1 scores\n",
    "print(\"\\nPer-Class F1 Scores:\")\n",
    "\n",
    "perclass_data = {}\n",
    "for model in MULTICLASS_MODELS:\n",
    "    if model not in multiclass_metrics:\n",
    "        continue\n",
    "    m = multiclass_metrics[model]\n",
    "    if 'per_class_f1' in m:\n",
    "        perclass_data[MODEL_DISPLAY_NAMES[model]] = m['per_class_f1']\n",
    "\n",
    "if perclass_data:\n",
    "    perclass_df = pd.DataFrame(perclass_data)\n",
    "    perclass_df.index.name = 'Fault Class'\n",
    "    \n",
    "    # Heatmap\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    sns.heatmap(perclass_df, annot=True, fmt='.3f', cmap='RdYlGn', \n",
    "                vmin=0.7, vmax=1.0, ax=ax, cbar_kws={'label': 'F1 Score'})\n",
    "    ax.set_xlabel('Model')\n",
    "    ax.set_ylabel('Fault Class')\n",
    "    ax.set_title('Per-Class F1 Scores by Model')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / 'model_comparison_perclass_f1_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"✓ Saved to {FIGURES_DIR / 'model_comparison_perclass_f1_heatmap.png'}\")\n",
    "    \n",
    "    # Save to CSV\n",
    "    perclass_df.to_csv(METRICS_DIR / 'model_comparison_perclass_f1.csv')\n",
    "    print(f\"✓ Saved to {METRICS_DIR / 'model_comparison_perclass_f1.csv'}\")\n",
    "else:\n",
    "    print(\"No per-class F1 data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-classes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify difficult classes\n",
    "if perclass_data:\n",
    "    print(\"\\nMost Difficult Classes (lowest average F1):\")\n",
    "    avg_f1 = perclass_df.mean(axis=1).sort_values()\n",
    "    print(avg_f1.head(5).to_string())\n",
    "    \n",
    "    print(\"\\n\\nEasiest Classes (highest average F1):\")\n",
    "    print(avg_f1.tail(5).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-header",
   "metadata": {},
   "source": [
    "## Binary Anomaly Detection Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BINARY ANOMALY DETECTION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Build comparison table\n",
    "rows = []\n",
    "for model in BINARY_MODELS:\n",
    "    if model not in binary_metrics:\n",
    "        continue\n",
    "    m = binary_metrics[model]\n",
    "    rows.append({\n",
    "        'Model': MODEL_DISPLAY_NAMES[model],\n",
    "        'Accuracy': m['accuracy'],\n",
    "        'Balanced Acc': m['balanced_accuracy'],\n",
    "        'F1 (Weighted)': m['f1_weighted'],\n",
    "        'F1 (Binary)': m['f1_binary'],\n",
    "        'Precision': m['precision'],\n",
    "        'Recall': m['recall'],\n",
    "        'ROC-AUC': m['roc_auc'],\n",
    "        'PR-AUC': m['pr_auc'],\n",
    "        'Training Time (s)': m.get('training_time_seconds', 0)\n",
    "    })\n",
    "\n",
    "if rows:\n",
    "    binary_df = pd.DataFrame(rows)\n",
    "    binary_df = binary_df.set_index('Model')\n",
    "    \n",
    "    # Format for display\n",
    "    display_df = binary_df.copy()\n",
    "    for col in ['Accuracy', 'Balanced Acc', 'F1 (Weighted)', 'F1 (Binary)', 'Precision', 'Recall', 'ROC-AUC', 'PR-AUC']:\n",
    "        display_df[col] = display_df[col].apply(lambda x: f\"{x:.4f}\")\n",
    "    display_df['Training Time (s)'] = display_df['Training Time (s)'].apply(lambda x: f\"{x:.1f}\")\n",
    "    \n",
    "    print(\"\\n\" + display_df.to_string())\n",
    "    \n",
    "    # Save to CSV\n",
    "    binary_df.to_csv(METRICS_DIR / 'model_comparison_binary.csv')\n",
    "    print(f\"\\n✓ Saved to {METRICS_DIR / 'model_comparison_binary.csv'}\")\n",
    "else:\n",
    "    print(\"No binary model metrics available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-bar-chart",
   "metadata": {},
   "outputs": [],
   "source": [
    "if rows:\n",
    "    # Bar chart comparison\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    models = binary_df.index.tolist()\n",
    "    x = np.arange(len(models))\n",
    "    width = 0.2\n",
    "    \n",
    "    # Classification metrics\n",
    "    ax = axes[0]\n",
    "    bars1 = ax.bar(x - width, binary_df['Accuracy'], width, label='Accuracy', color='steelblue')\n",
    "    bars2 = ax.bar(x, binary_df['F1 (Weighted)'], width, label='F1 (Weighted)', color='coral')\n",
    "    bars3 = ax.bar(x + width, binary_df['Balanced Acc'], width, label='Balanced Acc', color='green')\n",
    "    ax.set_xlabel('Model')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title('Classification Metrics (Binary)')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(models)\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0.8, 1.0)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # AUC metrics\n",
    "    ax = axes[1]\n",
    "    bars1 = ax.bar(x - width/2, binary_df['ROC-AUC'], width, label='ROC-AUC', color='steelblue')\n",
    "    bars2 = ax.bar(x + width/2, binary_df['PR-AUC'], width, label='PR-AUC', color='coral')\n",
    "    ax.set_xlabel('Model')\n",
    "    ax.set_ylabel('AUC')\n",
    "    ax.set_title('AUC Metrics (Binary)')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(models)\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0.8, 1.0)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / 'model_comparison_binary_bars.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"✓ Saved to {FIGURES_DIR / 'model_comparison_binary_bars.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-header",
   "metadata": {},
   "source": [
    "## Combined Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMBINED MODEL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Build unified summary\n",
    "rows = []\n",
    "\n",
    "# Multiclass models\n",
    "for model in MULTICLASS_MODELS:\n",
    "    if model not in multiclass_metrics:\n",
    "        continue\n",
    "    m = multiclass_metrics[model]\n",
    "    rows.append({\n",
    "        'Model': MODEL_DISPLAY_NAMES[model],\n",
    "        'Task': 'Multiclass',\n",
    "        'Accuracy': m['accuracy'],\n",
    "        'F1 (Weighted)': m['f1_weighted'],\n",
    "        'Precision': m['precision_weighted'],\n",
    "        'Recall': m['recall_weighted'],\n",
    "        'Training Time (s)': m.get('training_time_seconds', 0)\n",
    "    })\n",
    "\n",
    "# Binary models\n",
    "for model in BINARY_MODELS:\n",
    "    if model not in binary_metrics:\n",
    "        continue\n",
    "    m = binary_metrics[model]\n",
    "    rows.append({\n",
    "        'Model': MODEL_DISPLAY_NAMES[model],\n",
    "        'Task': 'Binary',\n",
    "        'Accuracy': m['accuracy'],\n",
    "        'F1 (Weighted)': m['f1_weighted'],\n",
    "        'Precision': m['precision'],\n",
    "        'Recall': m['recall'],\n",
    "        'Training Time (s)': m.get('training_time_seconds', 0)\n",
    "    })\n",
    "\n",
    "combined_df = pd.DataFrame(rows)\n",
    "\n",
    "# Format for display\n",
    "display_df = combined_df.copy()\n",
    "for col in ['Accuracy', 'F1 (Weighted)', 'Precision', 'Recall']:\n",
    "    display_df[col] = display_df[col].apply(lambda x: f\"{x:.4f}\")\n",
    "display_df['Training Time (s)'] = display_df['Training Time (s)'].apply(lambda x: f\"{x:.1f}\")\n",
    "\n",
    "print(\"\\n\" + display_df.to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "combined_df.to_csv(METRICS_DIR / 'model_comparison_combined.csv', index=False)\n",
    "print(f\"\\n✓ Saved to {METRICS_DIR / 'model_comparison_combined.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined visualization\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Group by task\n",
    "multiclass_models = combined_df[combined_df['Task'] == 'Multiclass']\n",
    "binary_models = combined_df[combined_df['Task'] == 'Binary']\n",
    "\n",
    "all_models = combined_df['Model'].tolist()\n",
    "x = np.arange(len(all_models))\n",
    "width = 0.35\n",
    "\n",
    "# Color by task\n",
    "colors = ['steelblue' if t == 'Multiclass' else 'coral' for t in combined_df['Task']]\n",
    "\n",
    "bars = ax.bar(x, combined_df['F1 (Weighted)'], color=colors, edgecolor='black')\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, combined_df['F1 (Weighted)']):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, \n",
    "            f'{val:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('F1 Score (Weighted)')\n",
    "ax.set_title('Model Performance Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(all_models, rotation=15, ha='right')\n",
    "ax.set_ylim(0.85, 1.0)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor='steelblue', edgecolor='black', label='Multiclass'),\n",
    "                   Patch(facecolor='coral', edgecolor='black', label='Binary')]\n",
    "ax.legend(handles=legend_elements, loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'model_comparison_combined.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"✓ Saved to {FIGURES_DIR / 'model_comparison_combined.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latex-header",
   "metadata": {},
   "source": [
    "## LaTeX Tables for Manuscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latex-multiclass",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LATEX TABLE: MULTICLASS RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if len(multiclass_metrics) > 0:\n",
    "    latex_rows = []\n",
    "    for model in MULTICLASS_MODELS:\n",
    "        if model not in multiclass_metrics:\n",
    "            continue\n",
    "        m = multiclass_metrics[model]\n",
    "        latex_rows.append(\n",
    "            f\"{MODEL_DISPLAY_NAMES[model]} & {m['accuracy']:.4f} & {m['balanced_accuracy']:.4f} & \"\n",
    "            f\"{m['f1_weighted']:.4f} & {m['f1_macro']:.4f} & {m['precision_weighted']:.4f} & {m['recall_weighted']:.4f} \\\\\\\\\"\n",
    "        )\n",
    "    \n",
    "    latex_table = \"\"\"\\\\begin{table}[htbp]\n",
    "\\\\centering\n",
    "\\\\caption{Multiclass Fault Classification Results}\n",
    "\\\\label{tab:multiclass-results}\n",
    "\\\\begin{tabular}{lcccccc}\n",
    "\\\\toprule\n",
    "Model & Accuracy & Balanced Acc & F1 (Weighted) & F1 (Macro) & Precision & Recall \\\\\\\\\n",
    "\\\\midrule\n",
    "\"\"\" + \"\\n\".join(latex_rows) + \"\"\"\n",
    "\\\\bottomrule\n",
    "\\\\end{tabular}\n",
    "\\\\end{table}\"\"\"\n",
    "    \n",
    "    print(latex_table)\n",
    "    \n",
    "    # Save to file\n",
    "    with open(METRICS_DIR / 'table_multiclass_results.tex', 'w') as f:\n",
    "        f.write(latex_table)\n",
    "    print(f\"\\n✓ Saved to {METRICS_DIR / 'table_multiclass_results.tex'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latex-binary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LATEX TABLE: BINARY RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if len(binary_metrics) > 0:\n",
    "    latex_rows = []\n",
    "    for model in BINARY_MODELS:\n",
    "        if model not in binary_metrics:\n",
    "            continue\n",
    "        m = binary_metrics[model]\n",
    "        latex_rows.append(\n",
    "            f\"{MODEL_DISPLAY_NAMES[model]} & {m['accuracy']:.4f} & {m['f1_weighted']:.4f} & \"\n",
    "            f\"{m['precision']:.4f} & {m['recall']:.4f} & {m['roc_auc']:.4f} & {m['pr_auc']:.4f} \\\\\\\\\"\n",
    "        )\n",
    "    \n",
    "    latex_table = \"\"\"\\\\begin{table}[htbp]\n",
    "\\\\centering\n",
    "\\\\caption{Binary Anomaly Detection Results}\n",
    "\\\\label{tab:binary-results}\n",
    "\\\\begin{tabular}{lcccccc}\n",
    "\\\\toprule\n",
    "Model & Accuracy & F1 (Weighted) & Precision & Recall & ROC-AUC & PR-AUC \\\\\\\\\n",
    "\\\\midrule\n",
    "\"\"\" + \"\\n\".join(latex_rows) + \"\"\"\n",
    "\\\\bottomrule\n",
    "\\\\end{tabular}\n",
    "\\\\end{table}\"\"\"\n",
    "    \n",
    "    print(latex_table)\n",
    "    \n",
    "    # Save to file\n",
    "    with open(METRICS_DIR / 'table_binary_results.tex', 'w') as f:\n",
    "        f.write(latex_table)\n",
    "    print(f\"\\n✓ Saved to {METRICS_DIR / 'table_binary_results.tex'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## Key Findings Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n### Multiclass Fault Classification ###\")\n",
    "if len(multiclass_metrics) > 0:\n",
    "    best_acc_model = max(multiclass_metrics.items(), key=lambda x: x[1]['accuracy'])\n",
    "    best_f1_model = max(multiclass_metrics.items(), key=lambda x: x[1]['f1_weighted'])\n",
    "    \n",
    "    print(f\"\\nBest Accuracy: {MODEL_DISPLAY_NAMES[best_acc_model[0]]} ({best_acc_model[1]['accuracy']:.4f})\")\n",
    "    print(f\"Best F1 (Weighted): {MODEL_DISPLAY_NAMES[best_f1_model[0]]} ({best_f1_model[1]['f1_weighted']:.4f})\")\n",
    "    \n",
    "    # Range of performance\n",
    "    accs = [m['accuracy'] for m in multiclass_metrics.values()]\n",
    "    f1s = [m['f1_weighted'] for m in multiclass_metrics.values()]\n",
    "    print(f\"\\nAccuracy range: {min(accs):.4f} - {max(accs):.4f}\")\n",
    "    print(f\"F1 (Weighted) range: {min(f1s):.4f} - {max(f1s):.4f}\")\n",
    "\n",
    "print(\"\\n### Binary Anomaly Detection ###\")\n",
    "if len(binary_metrics) > 0:\n",
    "    best_f1_model = max(binary_metrics.items(), key=lambda x: x[1]['f1_weighted'])\n",
    "    best_auc_model = max(binary_metrics.items(), key=lambda x: x[1]['roc_auc'])\n",
    "    \n",
    "    print(f\"\\nBest F1 (Weighted): {MODEL_DISPLAY_NAMES[best_f1_model[0]]} ({best_f1_model[1]['f1_weighted']:.4f})\")\n",
    "    print(f\"Best ROC-AUC: {MODEL_DISPLAY_NAMES[best_auc_model[0]]} ({best_auc_model[1]['roc_auc']:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ Model Comparison Summary Complete!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
