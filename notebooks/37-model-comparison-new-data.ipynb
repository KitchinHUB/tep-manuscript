{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Model Comparison on New TEP Dataset\n",
    "\n",
    "This notebook compares all trained models on the newly generated independent TEP dataset.\n",
    "\n",
    "**Models Compared**:\n",
    "- Multiclass: XGBoost, LSTM, LSTM-FCN, CNN-Transformer, TransKal\n",
    "- Binary: LSTM-Autoencoder, Conv-Autoencoder\n",
    "\n",
    "**Purpose**: Summarize generalization performance across all models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## Configuration & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "METRICS_DIR = Path('../outputs/metrics')\n",
    "FIGURES_DIR = Path('../outputs/figures')\n",
    "\n",
    "QUICK_MODE = os.environ.get('QUICK_MODE', '').lower() in ('true', '1', 'yes')\n",
    "FILE_SUFFIX = '_quick' if QUICK_MODE else ''\n",
    "\n",
    "print('='*60)\n",
    "print('Model Comparison on New TEP Dataset')\n",
    "if QUICK_MODE:\n",
    "    print('QUICK MODE')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-header",
   "metadata": {},
   "source": [
    "## Load All Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nLoading evaluation metrics...')\n",
    "\n",
    "multiclass_models = ['xgboost', 'lstm', 'lstm_fcn', 'cnn_transformer', 'transkal']\n",
    "binary_models = ['lstm_autoencoder', 'conv_autoencoder']\n",
    "\n",
    "multiclass_metrics = {}\n",
    "binary_metrics = {}\n",
    "\n",
    "for model in multiclass_models:\n",
    "    metrics_file = METRICS_DIR / f'{model}_new_eval_metrics{FILE_SUFFIX}.json'\n",
    "    if metrics_file.exists():\n",
    "        with open(metrics_file) as f:\n",
    "            multiclass_metrics[model] = json.load(f)\n",
    "        print(f'  Loaded {model}')\n",
    "    else:\n",
    "        print(f'  Missing {model}')\n",
    "\n",
    "for model in binary_models:\n",
    "    metrics_file = METRICS_DIR / f'{model}_new_eval_metrics{FILE_SUFFIX}.json'\n",
    "    if metrics_file.exists():\n",
    "        with open(metrics_file) as f:\n",
    "            binary_metrics[model] = json.load(f)\n",
    "        print(f'  Loaded {model}')\n",
    "    else:\n",
    "        print(f'  Missing {model}')\n",
    "\n",
    "print(f'\\nLoaded: {len(multiclass_metrics)} multiclass, {len(binary_metrics)} binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multi-header",
   "metadata": {},
   "source": [
    "## Multiclass Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiclass-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nMulticlass Model Comparison (New Evaluation Dataset):')\n",
    "print('='*90)\n",
    "\n",
    "rows = []\n",
    "for model, metrics in multiclass_metrics.items():\n",
    "    rows.append({\n",
    "        'Model': metrics['model'],\n",
    "        'Accuracy': metrics['accuracy'],\n",
    "        'Balanced Acc': metrics['balanced_accuracy'],\n",
    "        'F1 (weighted)': metrics['f1_weighted'],\n",
    "        'F1 (macro)': metrics['f1_macro'],\n",
    "        'Orig Accuracy': metrics['comparison_with_original']['original_accuracy'],\n",
    "        'Delta': metrics['comparison_with_original']['accuracy_delta']\n",
    "    })\n",
    "\n",
    "df_multi = pd.DataFrame(rows)\n",
    "df_multi = df_multi.sort_values('Accuracy', ascending=False)\n",
    "print(df_multi.to_string(index=False))\n",
    "\n",
    "# Save\n",
    "df_multi.to_csv(METRICS_DIR / f'multiclass_comparison_new_eval{FILE_SUFFIX}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiclass-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart: Accuracy comparison\n",
    "ax = axes[0]\n",
    "models = df_multi['Model'].tolist()\n",
    "orig_acc = df_multi['Orig Accuracy'].tolist()\n",
    "new_acc = df_multi['Accuracy'].tolist()\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, orig_acc, width, label='Original Test', color='steelblue', alpha=0.7)\n",
    "bars2 = ax.bar(x + width/2, new_acc, width, label='New Eval', color='coral', alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Multiclass Accuracy: Original vs New Evaluation')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Delta plot\n",
    "ax = axes[1]\n",
    "deltas = df_multi['Delta'].tolist()\n",
    "colors = ['green' if d >= 0 else 'red' for d in deltas]\n",
    "ax.bar(models, deltas, color=colors, edgecolor='black', alpha=0.7)\n",
    "ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Accuracy Delta (New - Original)')\n",
    "ax.set_title('Generalization Gap')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "output_file = FIGURES_DIR / f'multiclass_comparison_new_eval{FILE_SUFFIX}.png'\n",
    "plt.savefig(output_file, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f'Saved to {output_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-header",
   "metadata": {},
   "source": [
    "## Binary Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nBinary Model Comparison (New Evaluation Dataset):')\n",
    "print('='*80)\n",
    "\n",
    "if binary_metrics:\n",
    "    rows = []\n",
    "    for model, metrics in binary_metrics.items():\n",
    "        rows.append({\n",
    "            'Model': metrics['model'],\n",
    "            'Accuracy': metrics['accuracy'],\n",
    "            'Precision': metrics['precision'],\n",
    "            'Recall': metrics['recall'],\n",
    "            'F1': metrics['f1'],\n",
    "            'AUC-ROC': metrics['auc_roc'],\n",
    "            'Orig Accuracy': metrics['comparison_with_original']['original_accuracy'],\n",
    "            'Delta': metrics['comparison_with_original']['accuracy_delta']\n",
    "        })\n",
    "\n",
    "    df_binary = pd.DataFrame(rows)\n",
    "    df_binary = df_binary.sort_values('F1', ascending=False)\n",
    "    print(df_binary.to_string(index=False))\n",
    "    \n",
    "    df_binary.to_csv(METRICS_DIR / f'binary_comparison_new_eval{FILE_SUFFIX}.csv', index=False)\n",
    "else:\n",
    "    print('No binary model metrics available.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*60)\n",
    "print('MODEL COMPARISON SUMMARY - NEW TEP DATASET')\n",
    "if QUICK_MODE:\n",
    "    print('(Quick mode)')\n",
    "print('='*60)\n",
    "\n",
    "if multiclass_metrics:\n",
    "    best_multi = max(multiclass_metrics.items(), key=lambda x: x[1]['accuracy'])\n",
    "    print(f'\\nBest Multiclass Model: {best_multi[1][\"model\"]}')\n",
    "    print(f'  Accuracy: {best_multi[1][\"accuracy\"]:.4f}')\n",
    "    print(f'  F1 (weighted): {best_multi[1][\"f1_weighted\"]:.4f}')\n",
    "    print(f'  Generalization gap: {best_multi[1][\"comparison_with_original\"][\"accuracy_delta\"]:+.4f}')\n",
    "\n",
    "if binary_metrics:\n",
    "    best_binary = max(binary_metrics.items(), key=lambda x: x[1]['f1'])\n",
    "    print(f'\\nBest Binary Model: {best_binary[1][\"model\"]}')\n",
    "    print(f'  F1: {best_binary[1][\"f1\"]:.4f}')\n",
    "    print(f'  AUC-ROC: {best_binary[1][\"auc_roc\"]:.4f}')\n",
    "\n",
    "print('\\nOutputs:')\n",
    "print(f'  - {METRICS_DIR / f\"multiclass_comparison_new_eval{FILE_SUFFIX}.csv\"}')\n",
    "if binary_metrics:\n",
    "    print(f'  - {METRICS_DIR / f\"binary_comparison_new_eval{FILE_SUFFIX}.csv\"}')\n",
    "print(f'  - {FIGURES_DIR / f\"multiclass_comparison_new_eval{FILE_SUFFIX}.png\"}')\n",
    "print('='*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
