{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": "# LSTM with HMM Classification Filter\n\nThis notebook evaluates the effect of applying an HMM classification filter to LSTM predictions.\n\n**Method**: The HMM filter exploits temporal consistency using a sticky transition model.\n\n**Important**: The confusion matrix for the emission model is computed from the **validation set**, \nnot the test set. This ensures no test-time information leakage - the HMM filter parameters are \ndetermined entirely from data available during model development.\n\n**Stickiness values tested**: 0.7, 0.85, 0.95"
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## Configuration & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "config",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T20:54:47.921751Z",
     "iopub.status.busy": "2026-01-07T20:54:47.921445Z",
     "iopub.status.idle": "2026-01-07T20:54:50.664271Z",
     "shell.execute_reply": "2026-01-07T20:54:50.663064Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LSTM with HMM Classification Filter\n",
      "Device: cuda\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DATA_DIR = Path('../data')\n",
    "MODEL_DIR = Path('../outputs/models')\n",
    "METRICS_DIR = Path('../outputs/metrics')\n",
    "FIGURES_DIR = Path('../outputs/figures')\n",
    "\n",
    "METRICS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "STICKINESS_VALUES = [0.7, 0.85, 0.95]\n",
    "\n",
    "print('='*60)\n",
    "print('LSTM with HMM Classification Filter')\n",
    "print(f'Device: {device}')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filter-header",
   "metadata": {},
   "source": [
    "## HMM Classification Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "filter-class",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T20:54:50.667985Z",
     "iopub.status.busy": "2026-01-07T20:54:50.667685Z",
     "iopub.status.idle": "2026-01-07T20:54:50.675149Z",
     "shell.execute_reply": "2026-01-07T20:54:50.674147Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassificationFilter defined\n"
     ]
    }
   ],
   "source": [
    "class ClassificationFilter:\n",
    "    def __init__(self, n_classes, confusion_matrix=None, stickiness=0.9):\n",
    "        self.n_classes = n_classes\n",
    "        self.stickiness = stickiness\n",
    "        \n",
    "        if confusion_matrix is not None:\n",
    "            self.emission = confusion_matrix / confusion_matrix.sum(axis=1, keepdims=True)\n",
    "        else:\n",
    "            accuracy = 0.8\n",
    "            self.emission = np.full((n_classes, n_classes), (1 - accuracy) / (n_classes - 1))\n",
    "            np.fill_diagonal(self.emission, accuracy)\n",
    "        \n",
    "        self.transition = np.full((n_classes, n_classes), (1 - stickiness) / (n_classes - 1))\n",
    "        np.fill_diagonal(self.transition, stickiness)\n",
    "        \n",
    "        self.belief = np.ones(n_classes) / n_classes\n",
    "    \n",
    "    def reset(self):\n",
    "        self.belief = np.ones(self.n_classes) / self.n_classes\n",
    "    \n",
    "    def update(self, observation):\n",
    "        predicted_belief = self.transition.T @ self.belief\n",
    "        likelihood = self.emission[:, observation]\n",
    "        posterior = predicted_belief * likelihood\n",
    "        posterior_sum = posterior.sum()\n",
    "        if posterior_sum > 0:\n",
    "            posterior /= posterior_sum\n",
    "        else:\n",
    "            posterior = np.ones(self.n_classes) / self.n_classes\n",
    "        self.belief = posterior\n",
    "        return np.argmax(posterior), posterior\n",
    "    \n",
    "    def filter_sequence(self, observations, reset_on_run=None):\n",
    "        self.reset()\n",
    "        estimates = []\n",
    "        prev_run = None\n",
    "        \n",
    "        for i, obs in enumerate(observations):\n",
    "            if reset_on_run is not None:\n",
    "                current_run = reset_on_run[i]\n",
    "                if prev_run is not None and current_run != prev_run:\n",
    "                    self.reset()\n",
    "                prev_run = current_run\n",
    "            \n",
    "            est, _ = self.update(obs)\n",
    "            estimates.append(est)\n",
    "        \n",
    "        return np.array(estimates)\n",
    "\n",
    "print('ClassificationFilter defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-header",
   "metadata": {},
   "source": [
    "## Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "model-def",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T20:54:50.677894Z",
     "iopub.status.busy": "2026-01-07T20:54:50.677732Z",
     "iopub.status.idle": "2026-01-07T20:54:50.682675Z",
     "shell.execute_reply": "2026-01-07T20:54:50.681554Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model class defined\n"
     ]
    }
   ],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                           num_layers=num_layers, batch_first=True,\n",
    "                           dropout=dropout if num_layers > 1 else 0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = self.dropout(lstm_out[:, -1, :])\n",
    "        return self.fc(out)\n",
    "\n",
    "print('Model class defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-header",
   "metadata": {},
   "source": [
    "## Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "load-model",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T20:54:50.685202Z",
     "iopub.status.busy": "2026-01-07T20:54:50.685059Z",
     "iopub.status.idle": "2026-01-07T20:54:50.878789Z",
     "shell.execute_reply": "2026-01-07T20:54:50.877724Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Step 1/5] Loading trained LSTM model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: 52 features, 18 classes\n"
     ]
    }
   ],
   "source": [
    "print('\\n[Step 1/5] Loading trained LSTM model...')\n",
    "\n",
    "model_path = MODEL_DIR / 'lstm_final.pt'\n",
    "model_data = torch.load(model_path, map_location=device, weights_only=False)\n",
    "\n",
    "config = model_data['model_config']\n",
    "features = model_data['features']\n",
    "label_classes = model_data['label_encoder_classes']\n",
    "scaler_mean = model_data['scaler_mean']\n",
    "scaler_scale = model_data['scaler_scale']\n",
    "\n",
    "num_classes = len(label_classes)\n",
    "class_names = [str(int(c)) for c in label_classes]\n",
    "\n",
    "model = LSTMClassifier(\n",
    "    input_size=len(features),\n",
    "    hidden_size=config['hidden_size'],\n",
    "    num_layers=config['num_layers'],\n",
    "    num_classes=num_classes,\n",
    "    dropout=config['dropout']\n",
    ")\n",
    "model.load_state_dict(model_data['model_state_dict'])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f'Model loaded: {len(features)} features, {num_classes} classes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T20:54:50.882725Z",
     "iopub.status.busy": "2026-01-07T20:54:50.882563Z",
     "iopub.status.idle": "2026-01-07T20:55:00.826764Z",
     "shell.execute_reply": "2026-01-07T20:55:00.825245Z"
    }
   },
   "outputs": [],
   "source": "print('\\n[Step 2/5] Loading validation and test datasets...')\nstart_time = time.time()\n\n# Load validation set for building HMM emission model\nval_data = pd.read_csv(DATA_DIR / 'multiclass_val.csv')\nprint(f'Validation data loaded: {val_data.shape}')\n\n# Load test set for final evaluation\ntest_data = pd.read_csv(DATA_DIR / 'multiclass_test.csv')\nprint(f'Test data loaded: {test_data.shape}')\n\nprint(f'Total load time: {time.time() - start_time:.2f}s')"
  },
  {
   "cell_type": "markdown",
   "id": "dataset-header",
   "metadata": {},
   "source": [
    "## Create Dataset and Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dataset-class",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T20:55:00.830250Z",
     "iopub.status.busy": "2026-01-07T20:55:00.830055Z",
     "iopub.status.idle": "2026-01-07T20:55:00.836933Z",
     "shell.execute_reply": "2026-01-07T20:55:00.835847Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset class defined\n"
     ]
    }
   ],
   "source": [
    "class SimulationRunDataset(Dataset):\n",
    "    def __init__(self, df, features, scaler_mean, scaler_scale, label_classes, sequence_length=10):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.class_to_idx = {c: i for i, c in enumerate(label_classes)}\n",
    "        \n",
    "        self.sequences = []\n",
    "        self.labels = []\n",
    "        self.run_ids = []\n",
    "        \n",
    "        for (fault, run), group in df.groupby(['faultNumber', 'simulationRun']):\n",
    "            group = group.sort_values('sample')\n",
    "            X = group[features].values\n",
    "            X = (X - scaler_mean) / scaler_scale\n",
    "            \n",
    "            run_id = f'{fault}_{run}'\n",
    "            for i in range(len(X) - sequence_length + 1):\n",
    "                self.sequences.append(X[i:i+sequence_length])\n",
    "                self.labels.append(self.class_to_idx[fault])\n",
    "                self.run_ids.append(run_id)\n",
    "        \n",
    "        self.sequences = np.array(self.sequences, dtype=np.float32)\n",
    "        self.labels = np.array(self.labels, dtype=np.int64)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sequences[idx]), torch.tensor(self.labels[idx])\n",
    "\n",
    "print('Dataset class defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-predictions",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T20:55:00.839716Z",
     "iopub.status.busy": "2026-01-07T20:55:00.839582Z",
     "iopub.status.idle": "2026-01-07T20:55:57.560566Z",
     "shell.execute_reply": "2026-01-07T20:55:57.558775Z"
    }
   },
   "outputs": [],
   "source": "print('\\n[Step 3/5] Generating predictions on validation and test sets...')\npred_start = time.time()\n\nseq_len = config['sequence_length']\n\n# === Validation set predictions (for HMM emission model) ===\nval_dataset = SimulationRunDataset(val_data, features, scaler_mean, scaler_scale, label_classes, seq_len)\nval_loader = DataLoader(val_dataset, batch_size=512, shuffle=False, num_workers=0)\n\nval_preds = []\nval_labels = []\n\nwith torch.no_grad():\n    for X_batch, y_batch in val_loader:\n        X_batch = X_batch.to(device)\n        outputs = model(X_batch)\n        preds = outputs.argmax(dim=1).cpu().numpy()\n        val_preds.extend(preds)\n        val_labels.extend(y_batch.numpy())\n\ny_pred_val = np.array(val_preds)\ny_val = np.array(val_labels)\n\nprint(f'Validation predictions: {accuracy_score(y_val, y_pred_val):.4f} accuracy')\n\n# === Test set predictions (for final evaluation) ===\ntest_dataset = SimulationRunDataset(test_data, features, scaler_mean, scaler_scale, label_classes, seq_len)\ntest_loader = DataLoader(test_dataset, batch_size=512, shuffle=False, num_workers=0)\n\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():\n    for X_batch, y_batch in test_loader:\n        X_batch = X_batch.to(device)\n        outputs = model(X_batch)\n        preds = outputs.argmax(dim=1).cpu().numpy()\n        all_preds.extend(preds)\n        all_labels.extend(y_batch.numpy())\n\ny_pred_raw = np.array(all_preds)\ny_test = np.array(all_labels)\nrun_ids = np.array(test_dataset.run_ids)\n\nprint(f'Test predictions: {accuracy_score(y_test, y_pred_raw):.4f} accuracy')\nprint(f'Total prediction time: {time.time() - pred_start:.2f}s')"
  },
  {
   "cell_type": "markdown",
   "id": "apply-header",
   "metadata": {},
   "source": [
    "## Apply HMM Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apply-filter",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T20:55:57.563853Z",
     "iopub.status.busy": "2026-01-07T20:55:57.563681Z",
     "iopub.status.idle": "2026-01-07T20:57:03.132784Z",
     "shell.execute_reply": "2026-01-07T20:57:03.131481Z"
    }
   },
   "outputs": [],
   "source": "print('\\n[Step 4/5] Applying HMM filter...')\n\n# Compute confusion matrix from VALIDATION set for HMM emission model\n# This ensures no test-time information leakage\ncm_val = confusion_matrix(y_val, y_pred_val)\nprint(f'Validation confusion matrix computed (accuracy: {np.diag(cm_val).sum() / cm_val.sum():.4f})')\n\n# Also compute test confusion matrix for visualization later\ncm_test_raw = confusion_matrix(y_test, y_pred_raw)\n\nfiltered_predictions = {}\n\nfor stickiness in STICKINESS_VALUES:\n    filter_start = time.time()\n    \n    # Use VALIDATION confusion matrix as emission model\n    hmm_filter = ClassificationFilter(\n        n_classes=num_classes,\n        confusion_matrix=cm_val.astype(float),  # Use validation CM, not test CM\n        stickiness=stickiness\n    )\n    \n    y_pred_filtered = hmm_filter.filter_sequence(y_pred_raw, reset_on_run=run_ids)\n    filtered_predictions[stickiness] = y_pred_filtered\n    \n    acc = accuracy_score(y_test, y_pred_filtered)\n    print(f'  Stickiness {stickiness}: Accuracy = {acc:.4f} (time: {time.time() - filter_start:.2f}s)')\n\nprint('\\nHMM filtering complete')"
  },
  {
   "cell_type": "markdown",
   "id": "metrics-header",
   "metadata": {},
   "source": [
    "## Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "compute-metrics",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T20:57:03.136574Z",
     "iopub.status.busy": "2026-01-07T20:57:03.136406Z",
     "iopub.status.idle": "2026-01-07T20:57:05.279994Z",
     "shell.execute_reply": "2026-01-07T20:57:05.278723Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Step 5/5] Computing metrics...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "METRICS COMPARISON: RAW vs HMM-FILTERED\n",
      "================================================================================\n",
      "\n",
      "Method                      Accuracy    Bal.Acc      F1(W)\n",
      "-------------------------------------------------------\n",
      "Raw::::::::::::::::::::::     0.9914     0.9914     0.9914\n",
      "HMM (y=0.7)                   0.9914     0.9914     0.9914\n",
      "HMM (y=0.85)                  0.9914     0.9914     0.9915\n",
      "HMM (y=0.95)                  0.9914     0.9914     0.9915\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print('\\n[Step 5/5] Computing metrics...')\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'balanced_accuracy': balanced_accuracy_score(y_true, y_pred),\n",
    "        'f1_weighted': f1_score(y_true, y_pred, average='weighted'),\n",
    "        'f1_macro': f1_score(y_true, y_pred, average='macro'),\n",
    "        'per_class_f1': dict(zip(class_names, f1_score(y_true, y_pred, average=None)))\n",
    "    }\n",
    "\n",
    "raw_metrics = compute_metrics(y_test, y_pred_raw)\n",
    "filtered_metrics = {s: compute_metrics(y_test, filtered_predictions[s]) for s in STICKINESS_VALUES}\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('METRICS COMPARISON: RAW vs HMM-FILTERED')\n",
    "print('='*80)\n",
    "print(f\"\\n{'Method':<25} {'Accuracy':>10} {'Bal.Acc':>10} {'F1(W)':>10}\")\n",
    "print('-'*55)\n",
    "print(f\"{'Raw'::<25} {raw_metrics['accuracy']:>10.4f} {raw_metrics['balanced_accuracy']:>10.4f} {raw_metrics['f1_weighted']:>10.4f}\")\n",
    "\n",
    "for stickiness in STICKINESS_VALUES:\n",
    "    m = filtered_metrics[stickiness]\n",
    "    print(f\"{'HMM (y=' + str(stickiness) + ')':<25} {m['accuracy']:>10.4f} {m['balanced_accuracy']:>10.4f} {m['f1_weighted']:>10.4f}\")\n",
    "\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-header",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confusion-viz",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T20:57:05.283946Z",
     "iopub.status.busy": "2026-01-07T20:57:05.283747Z",
     "iopub.status.idle": "2026-01-07T20:57:08.306384Z",
     "shell.execute_reply": "2026-01-07T20:57:08.305636Z"
    }
   },
   "outputs": [],
   "source": "best_stickiness = max(STICKINESS_VALUES, key=lambda s: filtered_metrics[s]['accuracy'])\n\nfig, axes = plt.subplots(1, 2, figsize=(16, 7))\n\n# Use test confusion matrix for visualization\ncm_raw_norm = cm_test_raw.astype('float') / cm_test_raw.sum(axis=1)[:, np.newaxis]\nsns.heatmap(cm_raw_norm, annot=True, fmt='.2f', cmap='Blues',\n            xticklabels=class_names, yticklabels=class_names, ax=axes[0], vmin=0, vmax=1)\naxes[0].set_xlabel('Predicted')\naxes[0].set_ylabel('Actual')\naxes[0].set_title(f'Raw LSTM (Acc: {raw_metrics[\"accuracy\"]:.4f})')\n\ncm_hmm = confusion_matrix(y_test, filtered_predictions[best_stickiness])\ncm_hmm_norm = cm_hmm.astype('float') / cm_hmm.sum(axis=1)[:, np.newaxis]\nsns.heatmap(cm_hmm_norm, annot=True, fmt='.2f', cmap='Blues',\n            xticklabels=class_names, yticklabels=class_names, ax=axes[1], vmin=0, vmax=1)\naxes[1].set_xlabel('Predicted')\naxes[1].set_ylabel('Actual')\naxes[1].set_title(f'HMM Filtered (Î³={best_stickiness}, Acc: {filtered_metrics[best_stickiness][\"accuracy\"]:.4f})')\n\nplt.suptitle('LSTM Confusion Matrices: Raw vs HMM-Filtered', fontsize=14, y=1.02)\nplt.tight_layout()\noutput_file = FIGURES_DIR / 'lstm_hmm_confusion_matrices.png'\nplt.savefig(output_file, dpi=150, bbox_inches='tight')\nplt.show()\nprint(f'Saved to {output_file}')"
  },
  {
   "cell_type": "markdown",
   "id": "save-header",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-results",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T20:57:08.309883Z",
     "iopub.status.busy": "2026-01-07T20:57:08.309709Z",
     "iopub.status.idle": "2026-01-07T20:57:08.325000Z",
     "shell.execute_reply": "2026-01-07T20:57:08.323758Z"
    }
   },
   "outputs": [],
   "source": "best_metrics = filtered_metrics[best_stickiness]\n\nresults = {\n    'model': 'LSTM',\n    'method': 'HMM Classification Filter',\n    'emission_source': 'validation_set',  # Document that we used validation CM\n    'test_samples': len(y_test),\n    'val_samples': len(y_val),\n    'n_classes': num_classes,\n    'raw_metrics': raw_metrics,\n    'filtered_metrics': {str(k): v for k, v in filtered_metrics.items()},\n    'best_stickiness': best_stickiness,\n    'best_improvement': {\n        'accuracy_delta': best_metrics['accuracy'] - raw_metrics['accuracy'],\n        'f1_weighted_delta': best_metrics['f1_weighted'] - raw_metrics['f1_weighted'],\n        'error_reduction_pct': ((1 - raw_metrics['accuracy']) - (1 - best_metrics['accuracy'])) / (1 - raw_metrics['accuracy']) * 100 if raw_metrics['accuracy'] < 1 else 0\n    },\n    'stickiness_values_tested': STICKINESS_VALUES\n}\n\noutput_file = METRICS_DIR / 'lstm_hmm_filter_results.json'\nwith open(output_file, 'w') as f:\n    json.dump(results, f, indent=2)\nprint(f'Saved to {output_file}')"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "summary",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T20:57:08.327404Z",
     "iopub.status.busy": "2026-01-07T20:57:08.327256Z",
     "iopub.status.idle": "2026-01-07T20:57:08.331795Z",
     "shell.execute_reply": "2026-01-07T20:57:08.330691Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LSTM HMM Classification Filter Complete!\n",
      "============================================================\n",
      "\n",
      "Raw Accuracy: 0.9914\n",
      "Best HMM (y=0.95): 0.9914\n",
      "Improvement: +0.0001\n",
      "Error Reduction: 0.7%\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print('\\n' + '='*60)\n",
    "print('LSTM HMM Classification Filter Complete!')\n",
    "print('='*60)\n",
    "print(f'\\nRaw Accuracy: {raw_metrics[\"accuracy\"]:.4f}')\n",
    "print(f'Best HMM (y={best_stickiness}): {best_metrics[\"accuracy\"]:.4f}')\n",
    "print(f'Improvement: {results[\"best_improvement\"][\"accuracy_delta\"]:+.4f}')\n",
    "print(f'Error Reduction: {results[\"best_improvement\"][\"error_reduction_pct\"]:.1f}%')\n",
    "print('='*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}