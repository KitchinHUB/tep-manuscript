{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Generate New TEP Evaluation Dataset\n",
    "\n",
    "This notebook generates a completely new, independent TEP dataset using `tep-sim` for evaluating trained models on unseen data.\n",
    "\n",
    "**Purpose**: Create an independent evaluation dataset to test model generalization beyond the original test set.\n",
    "\n",
    "**Dataset Specifications**:\n",
    "- **Duration**: 48 hours per simulation (same as original test set)\n",
    "- **Sampling interval**: 3 minutes (180 seconds) → 960 samples per simulation\n",
    "- **Fault introduction**: At hour 8 (sample 161), same as original test set\n",
    "- **Fault classes**: 18 total (0=normal, 1, 2, 4-8, 10-14, 16-20; excluding 3, 9, 15)\n",
    "\n",
    "**Output Files**:\n",
    "- `data/new_multiclass_eval.csv` - Balanced multiclass evaluation (2.88M samples)\n",
    "- `data/new_binary_eval.csv` - Binary anomaly detection evaluation (~795K samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T18:11:55.559246Z",
     "iopub.status.busy": "2026-01-05T18:11:55.558912Z",
     "iopub.status.idle": "2026-01-05T18:11:55.856415Z",
     "shell.execute_reply": "2026-01-05T18:11:55.854913Z"
    }
   },
   "outputs": [],
   "source": "import os\nimport time\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom tep import TEPSimulator\n\n# =============================================================================\n# QUICK MODE: Set to True for fast testing with minimal data\n# Can be set via environment variable or directly here\n# =============================================================================\nQUICK_MODE = os.environ.get('QUICK_MODE', 'False').lower() in ('true', '1', 'yes')\n\n# Paths\nDATA_DIR = Path('../data')\nDATA_DIR.mkdir(exist_ok=True)\n\n# Simulation parameters\nFAULT_ONSET_HOURS = 8.0         # Fault introduced at hour 8\nFAULT_ONSET_SAMPLE = 161        # Sample 161 (0-indexed: 160) = 8 hours at 3-min sampling\n\n# Fault classes (matching original dataset, excluding 3, 9, 15)\nFAULT_CLASSES = [0, 1, 2, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20]\nNUM_CLASSES = len(FAULT_CLASSES)\n\n# Random seed offset (different from original which used 42)\nSEED_OFFSET = 1000\n\nif QUICK_MODE:\n    # Quick mode: all 18 classes but minimal runs for fast testing\n    DURATION_HOURS = 10.0           # 10 hours (just past fault onset)\n    RECORD_INTERVAL = 180           # 3 minutes\n    RUNS_PER_CLASS = 2              # 2 runs per class (36 total runs)\n    NORMAL_RUNS_BINARY = 2          # 2 normal runs\n    FAULT_RUNS_BINARY = 1           # 1 run per fault\n    FILE_SUFFIX = '_quick'\n    print(\"=\"*60)\n    print(\"⚡ QUICK MODE ENABLED - All 18 classes, minimal runs\")\n    print(\"=\"*60)\nelse:\n    # Full mode: ~25K samples total for multiclass\n    # 48h simulation = 960 samples, post-fault = 800 samples per run\n    # Target ~25K: 2 runs × 18 classes × 800 = 28,800 samples\n    DURATION_HOURS = 48.0           # 48 hours per simulation\n    RECORD_INTERVAL = 180           # 3 minutes = 180 seconds\n    RUNS_PER_CLASS = 2              # 2 runs per fault class for multiclass (~28.8K samples)\n    NORMAL_RUNS_BINARY = 15         # 15 normal runs for binary\n    FAULT_RUNS_BINARY = 2           # 2 runs per fault for binary\n    FILE_SUFFIX = ''\n    print(\"=\"*60)\n    print(\"TEP New Dataset Generation (FULL MODE)\")\n    print(\"=\"*60)\n\n# Calculate expected samples\nsamples_per_run = int((DURATION_HOURS * 3600 / RECORD_INTERVAL)) + 1\npost_fault_samples = samples_per_run - FAULT_ONSET_SAMPLE + 1\nexpected_multiclass = RUNS_PER_CLASS * NUM_CLASSES * post_fault_samples\nexpected_binary_normal = NORMAL_RUNS_BINARY * samples_per_run\nexpected_binary_fault = FAULT_RUNS_BINARY * (NUM_CLASSES - 1) * post_fault_samples\nexpected_binary = expected_binary_normal + expected_binary_fault\n\nprint(f\"Duration: {DURATION_HOURS} hours per simulation\")\nprint(f\"Sampling: {RECORD_INTERVAL} seconds ({RECORD_INTERVAL/60:.0f} minutes)\")\nprint(f\"Samples per run: {samples_per_run} total, {post_fault_samples} post-fault\")\nprint(f\"Fault onset: Hour {FAULT_ONSET_HOURS} (sample {FAULT_ONSET_SAMPLE})\")\nprint(f\"Fault classes: {NUM_CLASSES} ({FAULT_CLASSES})\")\nprint(f\"Runs per class: {RUNS_PER_CLASS}\")\nprint(f\"Total multiclass runs: {RUNS_PER_CLASS * NUM_CLASSES}\")\nprint(f\"Expected multiclass samples: ~{expected_multiclass:,}\")\nprint(f\"Expected binary samples: ~{expected_binary:,}\")\nif QUICK_MODE:\n    print(f\"Output files will have '{FILE_SUFFIX}' suffix\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "id": "gen-header",
   "metadata": {},
   "source": [
    "## Dataset Generation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "gen-functions",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T18:11:55.859645Z",
     "iopub.status.busy": "2026-01-05T18:11:55.859371Z",
     "iopub.status.idle": "2026-01-05T18:11:55.867189Z",
     "shell.execute_reply": "2026-01-05T18:11:55.866036Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Functions defined\n"
     ]
    }
   ],
   "source": [
    "def run_simulation(fault_number, seed, duration_hours=DURATION_HOURS, \n",
    "                   fault_onset_hours=FAULT_ONSET_HOURS, record_interval=RECORD_INTERVAL):\n",
    "    \"\"\"\n",
    "    Run a single TEP simulation with optional fault injection.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    fault_number : int\n",
    "        Fault ID (0 = normal, 1-20 = faults)\n",
    "    seed : int\n",
    "        Random seed for reproducibility\n",
    "    duration_hours : float\n",
    "        Total simulation duration in hours\n",
    "    fault_onset_hours : float\n",
    "        Time at which to introduce fault (hours)\n",
    "    record_interval : int\n",
    "        Recording interval in seconds\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict with keys: 'measurements', 'manipulated', 'time', 'shutdown'\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    sim = TEPSimulator()\n",
    "    sim.initialize()\n",
    "    \n",
    "    # Set up disturbances\n",
    "    if fault_number == 0:\n",
    "        disturbances = None\n",
    "    else:\n",
    "        # Introduce fault at specified time\n",
    "        disturbances = {fault_number: (fault_onset_hours, 1)}\n",
    "    \n",
    "    result = sim.simulate(\n",
    "        duration_hours=duration_hours,\n",
    "        disturbances=disturbances,\n",
    "        record_interval=record_interval\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'measurements': result.measurements,           # Shape: (samples, 41)\n",
    "        'manipulated': result.manipulated_vars[:, :11], # Shape: (samples, 11) - use first 11\n",
    "        'time': result.time,\n",
    "        'shutdown': result.shutdown\n",
    "    }\n",
    "\n",
    "\n",
    "def simulation_to_dataframe(sim_result, fault_number, run_number, origin='new'):\n",
    "    \"\"\"\n",
    "    Convert simulation result to DataFrame matching original data format.\n",
    "    \n",
    "    Columns: faultNumber, simulationRun, sample, xmeas_1...xmeas_41, xmv_1...xmv_11, origin, traj_key\n",
    "    \"\"\"\n",
    "    n_samples = len(sim_result['time'])\n",
    "    \n",
    "    # Build data dictionary\n",
    "    data = {\n",
    "        'faultNumber': [fault_number] * n_samples,\n",
    "        'simulationRun': [float(run_number)] * n_samples,\n",
    "        'sample': list(range(1, n_samples + 1))  # 1-indexed like original\n",
    "    }\n",
    "    \n",
    "    # Add XMEAS columns (41 measurements)\n",
    "    for i in range(41):\n",
    "        data[f'xmeas_{i+1}'] = sim_result['measurements'][:, i]\n",
    "    \n",
    "    # Add XMV columns (11 manipulated variables)\n",
    "    for i in range(11):\n",
    "        data[f'xmv_{i+1}'] = sim_result['manipulated'][:, i]\n",
    "    \n",
    "    # Add metadata\n",
    "    data['origin'] = [origin] * n_samples\n",
    "    data['traj_key'] = [f'{origin}_f{fault_number}_r{run_number}'] * n_samples\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "print(\"✓ Functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-header",
   "metadata": {},
   "source": [
    "## Test Single Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "test-sim",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T18:11:55.869736Z",
     "iopub.status.busy": "2026-01-05T18:11:55.869579Z",
     "iopub.status.idle": "2026-01-05T18:12:06.899350Z",
     "shell.execute_reply": "2026-01-05T18:12:06.897172Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing single simulation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal simulation: 201 samples, shutdown=False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fault 1 simulation: 201 samples, shutdown=False\n",
      "\n",
      "DataFrame shape: (201, 57)\n",
      "Columns: ['faultNumber', 'simulationRun', 'sample', 'xmeas_1', 'xmeas_2', 'xmeas_3', 'xmeas_4', 'xmeas_5']...['xmv_11', 'origin', 'traj_key']\n",
      "\n",
      "✓ Test complete in 11.02s\n",
      "Estimated time for full dataset: 6.6 minutes\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing single simulation...\")\n",
    "test_start = time.time()\n",
    "\n",
    "# Test normal operation\n",
    "result_normal = run_simulation(fault_number=0, seed=SEED_OFFSET)\n",
    "print(f\"Normal simulation: {result_normal['measurements'].shape[0]} samples, \"\n",
    "      f\"shutdown={result_normal['shutdown']}\")\n",
    "\n",
    "# Test fault 1\n",
    "result_fault = run_simulation(fault_number=1, seed=SEED_OFFSET + 1)\n",
    "print(f\"Fault 1 simulation: {result_fault['measurements'].shape[0]} samples, \"\n",
    "      f\"shutdown={result_fault['shutdown']}\")\n",
    "\n",
    "# Convert to DataFrame and verify format\n",
    "df_test = simulation_to_dataframe(result_fault, fault_number=1, run_number=1)\n",
    "print(f\"\\nDataFrame shape: {df_test.shape}\")\n",
    "print(f\"Columns: {list(df_test.columns)[:8]}...{list(df_test.columns)[-3:]}\")\n",
    "\n",
    "test_time = time.time() - test_start\n",
    "print(f\"\\n✓ Test complete in {test_time:.2f}s\")\n",
    "print(f\"Estimated time for full dataset: {test_time * RUNS_PER_CLASS * NUM_CLASSES / 60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiclass-header",
   "metadata": {},
   "source": [
    "## Generate Multiclass Evaluation Dataset\n",
    "\n",
    "Generate 200 runs per fault class (18 classes = 3,600 total runs). Use only post-fault samples (161-960) for balanced classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "gen-multiclass",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T18:12:06.904099Z",
     "iopub.status.busy": "2026-01-05T18:12:06.903892Z",
     "iopub.status.idle": "2026-01-05T18:15:24.538153Z",
     "shell.execute_reply": "2026-01-05T18:15:24.536348Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Generating Multiclass Evaluation Dataset\n",
      "============================================================\n",
      "\n",
      "Fault 0 (1/18)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Completed in 11.0s\n",
      "\n",
      "Fault 1 (2/18)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Completed in 11.0s\n",
      "\n",
      "Fault 2 (3/18)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Completed in 10.9s\n",
      "\n",
      "Fault 4 (4/18)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Completed in 10.9s\n",
      "\n",
      "Fault 5 (5/18)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Completed in 10.9s\n",
      "\n",
      "Fault 6 (6/18)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Completed in 11.0s\n",
      "\n",
      "Fault 7 (7/18)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Completed in 11.1s\n",
      "\n",
      "Fault 8 (8/18)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Completed in 11.0s\n",
      "\n",
      "Fault 10 (9/18)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Completed in 11.0s\n",
      "\n",
      "Fault 11 (10/18)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Completed in 11.0s\n",
      "\n",
      "Fault 12 (11/18)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Completed in 11.0s\n",
      "\n",
      "Fault 13 (12/18)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Completed in 11.0s\n",
      "\n",
      "Fault 14 (13/18)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Completed in 10.9s\n",
      "\n",
      "Fault 16 (14/18)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Completed in 11.0s\n",
      "\n",
      "Fault 17 (15/18)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Completed in 10.9s\n",
      "\n",
      "Fault 18 (16/18)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Completed in 11.0s\n",
      "\n",
      "Fault 19 (17/18)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Completed in 10.9s\n",
      "\n",
      "Fault 20 (18/18)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Completed in 11.0s\n",
      "\n",
      "Combining DataFrames...\n",
      "\n",
      "✓ Multiclass dataset generated in 3.3 minutes\n",
      "  Shape: (1476, 57)\n",
      "  Samples per class: [82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82]\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Generating Multiclass Evaluation Dataset\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = time.time()\n",
    "all_dfs = []\n",
    "total_runs = RUNS_PER_CLASS * NUM_CLASSES\n",
    "completed = 0\n",
    "\n",
    "for fault_idx, fault_number in enumerate(FAULT_CLASSES):\n",
    "    fault_start = time.time()\n",
    "    print(f\"\\nFault {fault_number} ({fault_idx+1}/{NUM_CLASSES})...\")\n",
    "    \n",
    "    for run in range(1, RUNS_PER_CLASS + 1):\n",
    "        # Unique seed for each simulation\n",
    "        seed = SEED_OFFSET + fault_number * 1000 + run\n",
    "        \n",
    "        try:\n",
    "            result = run_simulation(fault_number=fault_number, seed=seed)\n",
    "            df = simulation_to_dataframe(result, fault_number, run, origin='new_eval')\n",
    "            \n",
    "            # For multiclass: use only post-fault samples (161 onwards)\n",
    "            # This gives 800 samples per run (961 - 161 = 800)\n",
    "            df_post_fault = df[df['sample'] >= FAULT_ONSET_SAMPLE].copy()\n",
    "            all_dfs.append(df_post_fault)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Fault {fault_number} run {run} failed: {e}\")\n",
    "        \n",
    "        completed += 1\n",
    "        if run % 50 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            eta = elapsed / completed * (total_runs - completed)\n",
    "            print(f\"  Run {run}/{RUNS_PER_CLASS} - \"\n",
    "                  f\"Progress: {completed}/{total_runs} ({100*completed/total_runs:.1f}%) - \"\n",
    "                  f\"ETA: {eta/60:.1f} min\")\n",
    "    \n",
    "    fault_time = time.time() - fault_start\n",
    "    print(f\"  Completed in {fault_time:.1f}s\")\n",
    "\n",
    "# Combine all DataFrames\n",
    "print(\"\\nCombining DataFrames...\")\n",
    "multiclass_df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\n✓ Multiclass dataset generated in {total_time/60:.1f} minutes\")\n",
    "print(f\"  Shape: {multiclass_df.shape}\")\n",
    "print(f\"  Samples per class: {multiclass_df.groupby('faultNumber').size().values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "save-multiclass",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T18:15:24.541808Z",
     "iopub.status.busy": "2026-01-05T18:15:24.541612Z",
     "iopub.status.idle": "2026-01-05T18:15:24.756140Z",
     "shell.execute_reply": "2026-01-05T18:15:24.754878Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving multiclass dataset to ../data/new_multiclass_eval_quick.csv...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved (1.5 MB)\n",
      "\n",
      "Class distribution:\n",
      "faultNumber\n",
      "0     82\n",
      "1     82\n",
      "2     82\n",
      "4     82\n",
      "5     82\n",
      "6     82\n",
      "7     82\n",
      "8     82\n",
      "10    82\n",
      "11    82\n",
      "12    82\n",
      "13    82\n",
      "14    82\n",
      "16    82\n",
      "17    82\n",
      "18    82\n",
      "19    82\n",
      "20    82\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Save multiclass dataset\n",
    "output_file = DATA_DIR / f'new_multiclass_eval{FILE_SUFFIX}.csv'\n",
    "print(f\"Saving multiclass dataset to {output_file}...\")\n",
    "multiclass_df.to_csv(output_file, index=False)\n",
    "file_size = output_file.stat().st_size\n",
    "if file_size > 1e9:\n",
    "    print(f\"✓ Saved ({file_size / 1e9:.2f} GB)\")\n",
    "else:\n",
    "    print(f\"✓ Saved ({file_size / 1e6:.1f} MB)\")\n",
    "\n",
    "# Verify class balance\n",
    "print(\"\\nClass distribution:\")\n",
    "print(multiclass_df['faultNumber'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-header",
   "metadata": {},
   "source": [
    "## Generate Binary Evaluation Dataset\n",
    "\n",
    "Generate:\n",
    "- 120 normal runs (all samples)\n",
    "- 50 runs per fault (17 faults, post-fault samples only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "gen-binary",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T18:15:24.759492Z",
     "iopub.status.busy": "2026-01-05T18:15:24.759323Z",
     "iopub.status.idle": "2026-01-05T18:17:09.417016Z",
     "shell.execute_reply": "2026-01-05T18:17:09.415168Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Generating Binary Evaluation Dataset\n",
      "============================================================\n",
      "\n",
      "Generating normal runs (120 runs, all samples)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Normal runs complete\n",
      "\n",
      "Generating fault runs (17 faults × 1 runs)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 5/17 faults\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 10/17 faults\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 15/17 faults\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combining DataFrames...\n",
      "\n",
      "✓ Binary dataset generated in 1.7 minutes\n",
      "  Shape: (1099, 57)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Generating Binary Evaluation Dataset\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = time.time()\n",
    "binary_dfs = []\n",
    "\n",
    "# Generate normal runs (all samples)\n",
    "print(\"\\nGenerating normal runs (120 runs, all samples)...\")\n",
    "for run in range(1, NORMAL_RUNS_BINARY + 1):\n",
    "    seed = SEED_OFFSET + 50000 + run  # Different seed range from multiclass\n",
    "    \n",
    "    try:\n",
    "        result = run_simulation(fault_number=0, seed=seed)\n",
    "        df = simulation_to_dataframe(result, fault_number=0, run_number=run, origin='new_binary')\n",
    "        binary_dfs.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"  Warning: Normal run {run} failed: {e}\")\n",
    "    \n",
    "    if run % 30 == 0:\n",
    "        print(f\"  Progress: {run}/{NORMAL_RUNS_BINARY}\")\n",
    "\n",
    "print(f\"  ✓ Normal runs complete\")\n",
    "\n",
    "# Generate fault runs (post-fault samples only)\n",
    "fault_classes_no_normal = [f for f in FAULT_CLASSES if f != 0]\n",
    "print(f\"\\nGenerating fault runs ({len(fault_classes_no_normal)} faults × {FAULT_RUNS_BINARY} runs)...\")\n",
    "\n",
    "for fault_idx, fault_number in enumerate(fault_classes_no_normal):\n",
    "    for run in range(1, FAULT_RUNS_BINARY + 1):\n",
    "        seed = SEED_OFFSET + 60000 + fault_number * 100 + run\n",
    "        \n",
    "        try:\n",
    "            result = run_simulation(fault_number=fault_number, seed=seed)\n",
    "            df = simulation_to_dataframe(result, fault_number=fault_number, run_number=run, origin='new_binary')\n",
    "            # Use only post-fault samples\n",
    "            df_post_fault = df[df['sample'] >= FAULT_ONSET_SAMPLE].copy()\n",
    "            binary_dfs.append(df_post_fault)\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Fault {fault_number} run {run} failed: {e}\")\n",
    "    \n",
    "    if (fault_idx + 1) % 5 == 0:\n",
    "        print(f\"  Progress: {fault_idx+1}/{len(fault_classes_no_normal)} faults\")\n",
    "\n",
    "# Combine\n",
    "print(\"\\nCombining DataFrames...\")\n",
    "binary_df = pd.concat(binary_dfs, ignore_index=True)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\n✓ Binary dataset generated in {total_time/60:.1f} minutes\")\n",
    "print(f\"  Shape: {binary_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "save-binary",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T18:17:09.420464Z",
     "iopub.status.busy": "2026-01-05T18:17:09.420289Z",
     "iopub.status.idle": "2026-01-05T18:17:09.589058Z",
     "shell.execute_reply": "2026-01-05T18:17:09.587709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving binary dataset to ../data/new_binary_eval_quick.csv...\n",
      "✓ Saved (1.1 MB)\n",
      "\n",
      "Binary distribution:\n",
      "  Normal: 402 (36.6%)\n",
      "  Fault:  697 (63.4%)\n"
     ]
    }
   ],
   "source": [
    "# Add binary label column\n",
    "binary_df['label'] = (binary_df['faultNumber'] != 0).astype(int)\n",
    "\n",
    "# Save binary dataset\n",
    "output_file = DATA_DIR / f'new_binary_eval{FILE_SUFFIX}.csv'\n",
    "print(f\"Saving binary dataset to {output_file}...\")\n",
    "binary_df.to_csv(output_file, index=False)\n",
    "print(f\"✓ Saved ({output_file.stat().st_size / 1e6:.1f} MB)\")\n",
    "\n",
    "# Statistics\n",
    "n_normal = (binary_df['label'] == 0).sum()\n",
    "n_fault = (binary_df['label'] == 1).sum()\n",
    "print(f\"\\nBinary distribution:\")\n",
    "print(f\"  Normal: {n_normal:,} ({100*n_normal/len(binary_df):.1f}%)\")\n",
    "print(f\"  Fault:  {n_fault:,} ({100*n_fault/len(binary_df):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verify-header",
   "metadata": {},
   "source": [
    "## Verify Dataset Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "verify",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T18:17:09.592193Z",
     "iopub.status.busy": "2026-01-05T18:17:09.592014Z",
     "iopub.status.idle": "2026-01-05T18:17:09.626326Z",
     "shell.execute_reply": "2026-01-05T18:17:09.625207Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Dataset Verification\n",
      "============================================================\n",
      "\n",
      "Multiclass Dataset:\n",
      "  Shape: (1476, 57)\n",
      "  Classes: [np.int64(0), np.int64(1), np.int64(2), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(10), np.int64(11), np.int64(12), np.int64(13), np.int64(14), np.int64(16), np.int64(17), np.int64(18), np.int64(19), np.int64(20)]\n",
      "  Samples per class: 82 - 82\n",
      "  Missing values: 0\n",
      "\n",
      "Binary Dataset:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Shape: (1099, 58)\n",
      "  Normal samples: 402\n",
      "  Fault samples: 697\n",
      "  Missing values: 0\n",
      "\n",
      "============================================================\n",
      "✓ Quick Dataset Generation Complete!\n",
      "  Files saved with '_quick' suffix\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Dataset Verification\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load and verify multiclass\n",
    "print(\"\\nMulticlass Dataset:\")\n",
    "mc = pd.read_csv(DATA_DIR / f'new_multiclass_eval{FILE_SUFFIX}.csv')\n",
    "print(f\"  Shape: {mc.shape}\")\n",
    "print(f\"  Classes: {sorted(mc['faultNumber'].unique())}\")\n",
    "print(f\"  Samples per class: {mc.groupby('faultNumber').size().min()} - {mc.groupby('faultNumber').size().max()}\")\n",
    "print(f\"  Missing values: {mc.isnull().sum().sum()}\")\n",
    "\n",
    "# Load and verify binary\n",
    "print(\"\\nBinary Dataset:\")\n",
    "bn = pd.read_csv(DATA_DIR / f'new_binary_eval{FILE_SUFFIX}.csv')\n",
    "print(f\"  Shape: {bn.shape}\")\n",
    "print(f\"  Normal samples: {(bn['label'] == 0).sum():,}\")\n",
    "print(f\"  Fault samples: {(bn['label'] == 1).sum():,}\")\n",
    "print(f\"  Missing values: {bn.isnull().sum().sum()}\")\n",
    "\n",
    "# Compare with original test sets (only in full mode)\n",
    "if not QUICK_MODE:\n",
    "    print(\"\\nComparison with Original Test Sets:\")\n",
    "    orig_mc = pd.read_csv(DATA_DIR / 'multiclass_test.csv')\n",
    "    orig_bn = pd.read_csv(DATA_DIR / 'binary_test.csv')\n",
    "    print(f\"  Original multiclass: {orig_mc.shape}\")\n",
    "    print(f\"  New multiclass:      {mc.shape}\")\n",
    "    print(f\"  Original binary:     {orig_bn.shape}\")\n",
    "    print(f\"  New binary:          {bn.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "if QUICK_MODE:\n",
    "    print(\"✓ Quick Dataset Generation Complete!\")\n",
    "    print(f\"  Files saved with '{FILE_SUFFIX}' suffix\")\n",
    "else:\n",
    "    print(\"✓ Dataset Generation Complete!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}