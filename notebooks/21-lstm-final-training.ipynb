{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": "# LSTM Final Training and Evaluation\n\nThis notebook trains LSTM with the best hyperparameters found during tuning and evaluates on the test set.\n\n**Task**: Multiclass fault classification (18 classes)\n\n**Data Split**:\n- Train: Model fitting\n- Validation: Early stopping monitoring\n- Test: Final evaluation (never seen during training)\n\n**Modes**:\n- QUICK_MODE: Uses 1% of training data, 1 epoch (for testing pipeline)\n- FULL MODE: Uses all training data with tuned hyperparameters\n\n**Outputs**:\n- Trained model: `outputs/models/lstm_final[_quick].pt`\n- Metrics: `outputs/metrics/lstm_metrics[_quick].json`\n- Confusion matrix: `outputs/figures/lstm_confusion_matrix[_quick].png`"
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"=\"*60)\n",
    "print(\"LSTM Final Training and Evaluation\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Started at: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Quick mode configuration\n",
    "QUICK_MODE = os.getenv('QUICK_MODE', 'False').lower() in ('true', '1', 'yes')\n",
    "\n",
    "if QUICK_MODE:\n",
    "    TRAIN_FRACTION = 0.01\n",
    "    MAX_EPOCHS = 1\n",
    "    PATIENCE = 1\n",
    "    print(\"ðŸš€ QUICK MODE (1% data, 1 epoch)\")\n",
    "else:\n",
    "    TRAIN_FRACTION = 1.0\n",
    "    MAX_EPOCHS = 100\n",
    "    PATIENCE = 10\n",
    "    print(\"ðŸ”¬ FULL MODE (100% data, up to 100 epochs)\")\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path('../data')\n",
    "OUTPUT_DIR = Path('../outputs')\n",
    "HYPERPARAM_DIR = OUTPUT_DIR / 'hyperparams'\n",
    "MODEL_DIR = OUTPUT_DIR / 'models'\n",
    "METRICS_DIR = OUTPUT_DIR / 'metrics'\n",
    "FIGURES_DIR = OUTPUT_DIR / 'figures'\n",
    "\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "METRICS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "MODE_SUFFIX = '_quick' if QUICK_MODE else ''\n",
    "\n",
    "# Load best hyperparameters\n",
    "if (HYPERPARAM_DIR / 'lstm_best.json').exists():\n",
    "    hp_file = HYPERPARAM_DIR / 'lstm_best.json'\n",
    "    print(\"Using FULL mode hyperparameters\")\n",
    "else:\n",
    "    hp_file = HYPERPARAM_DIR / 'lstm_best_quick.json'\n",
    "    print(\"Using QUICK mode hyperparameters\")\n",
    "\n",
    "with open(hp_file) as f:\n",
    "    hp_data = json.load(f)\n",
    "    best_params = hp_data['best_params']\n",
    "\n",
    "print(f\"\\nHyperparameters:\")\n",
    "for k, v in best_params.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-header",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[Step 1/6] Loading libraries...\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    classification_report, confusion_matrix, balanced_accuracy_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"âœ“ Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-header",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n[Step 2/6] Loading datasets...\")\ndata_load_start = time.time()\n\ntrain = pd.read_csv(DATA_DIR / 'multiclass_train.csv')\nval = pd.read_csv(DATA_DIR / 'multiclass_val.csv')\ntest = pd.read_csv(DATA_DIR / 'multiclass_test.csv')\n\nprint(f\"âœ“ Train: {train.shape}\")\nprint(f\"âœ“ Val: {val.shape}\")\nprint(f\"âœ“ Test: {test.shape}\")\nprint(f\"âœ“ Data loading time: {time.time() - data_load_start:.2f}s\")\n\n# Get feature columns\nfeatures = [col for col in train.columns if 'xmeas' in col or 'xmv' in col]\nnum_features = len(features)\nprint(f\"âœ“ Number of features: {num_features}\")\n\n# Subsample if in quick mode (subsample runs, not random rows, to preserve sequences)\nif TRAIN_FRACTION < 1.0:\n    # Get unique simulation runs and sample them\n    train_runs = train[['faultNumber', 'simulationRun']].drop_duplicates()\n    val_runs = val[['faultNumber', 'simulationRun']].drop_duplicates()\n    \n    # Sample runs proportionally\n    n_train_runs = max(1, int(len(train_runs) * TRAIN_FRACTION))\n    n_val_runs = max(1, int(len(val_runs) * TRAIN_FRACTION))\n    \n    sampled_train_runs = train_runs.sample(n=n_train_runs, random_state=RANDOM_SEED)\n    sampled_val_runs = val_runs.sample(n=n_val_runs, random_state=RANDOM_SEED)\n    \n    train = train.merge(sampled_train_runs, on=['faultNumber', 'simulationRun'])\n    val = val.merge(sampled_val_runs, on=['faultNumber', 'simulationRun'])\n    \n    print(f\"âœ“ Subsampled train to {TRAIN_FRACTION*100:.1f}%: {train.shape}\")\n    print(f\"âœ“ Subsampled val to {TRAIN_FRACTION*100:.1f}%: {val.shape}\")\n\n# Fit scaler on training data only\nscaler = StandardScaler()\nscaler.fit(train[features])\n\n# Fit label encoder on training data only\nlabel_encoder = LabelEncoder()\nlabel_encoder.fit(train['faultNumber'])\nnum_classes = len(label_encoder.classes_)\nclass_names = [str(int(c)) for c in label_encoder.classes_]\nprint(f\"âœ“ Number of classes: {num_classes}\")"
  },
  {
   "cell_type": "markdown",
   "id": "model-header",
   "metadata": {},
   "source": [
    "## Model and Dataset Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[Step 3/6] Defining model and dataset...\")\n",
    "\n",
    "class SimulationRunDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset that creates windows WITHIN simulation runs only.\n",
    "    No windows cross simulation run boundaries.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, features, scaler, label_encoder, sequence_length=10):\n",
    "        self.seq_len = sequence_length\n",
    "        self.windows = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for (fault, run), group in df.groupby(['faultNumber', 'simulationRun']):\n",
    "            group = group.sort_values('sample')\n",
    "            X = scaler.transform(group[features].values)\n",
    "            y = label_encoder.transform(group['faultNumber'].values)\n",
    "            \n",
    "            for i in range(len(X) - sequence_length + 1):\n",
    "                self.windows.append(X[i:i+sequence_length])\n",
    "                self.labels.append(y[i+sequence_length-1])\n",
    "        \n",
    "        self.windows = np.array(self.windows, dtype=np.float32)\n",
    "        self.labels = np.array(self.labels, dtype=np.int64)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.windows)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.windows[idx]), torch.tensor(self.labels[idx])\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, \n",
    "                           batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = self.dropout(lstm_out[:, -1, :])\n",
    "        return self.fc(out)\n",
    "\n",
    "print(\"âœ“ Model and dataset classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-datasets",
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n[Step 4/6] Creating datasets...\")\ndataset_start = time.time()\n\nsequence_length = best_params['sequence_length']\nbatch_size = best_params['batch_size']\n\n# Create separate datasets for train, val, and test\ntrain_dataset = SimulationRunDataset(train, features, scaler, label_encoder, sequence_length)\nval_dataset = SimulationRunDataset(val, features, scaler, label_encoder, sequence_length)\ntest_dataset = SimulationRunDataset(test, features, scaler, label_encoder, sequence_length)\n\nprint(f\"âœ“ Train dataset: {len(train_dataset)} windows\")\nprint(f\"âœ“ Val dataset: {len(val_dataset)} windows\")\nprint(f\"âœ“ Test dataset: {len(test_dataset)} windows\")\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n\nprint(f\"âœ“ Dataset creation time: {time.time() - dataset_start:.2f}s\")"
  },
  {
   "cell_type": "markdown",
   "id": "train-header",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-model",
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n[Step 5/6] Training final model with early stopping on validation set...\")\ntrain_start = time.time()\n\n# Build model\nmodel = LSTMClassifier(\n    input_size=num_features,\n    hidden_size=best_params['hidden_size'],\n    num_layers=best_params['num_layers'],\n    num_classes=num_classes,\n    dropout=best_params['dropout']\n).to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=best_params['learning_rate'])\n\n# Training with early stopping on validation loss\nbest_val_loss = float('inf')\npatience_counter = 0\nbest_model_state = None\nhistory = {'train_loss': [], 'val_loss': [], 'epoch': []}\n\nprint(f\"Training for up to {MAX_EPOCHS} epochs with patience {PATIENCE}...\")\n\nfor epoch in range(MAX_EPOCHS):\n    # Training phase\n    model.train()\n    train_loss = 0.0\n    \n    for X_batch, y_batch in train_loader:\n        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(X_batch)\n        loss = criterion(outputs, y_batch)\n        loss.backward()\n        optimizer.step()\n        \n        train_loss += loss.item()\n    \n    avg_train_loss = train_loss / len(train_loader)\n    \n    # Validation phase\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for X_batch, y_batch in val_loader:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            outputs = model(X_batch)\n            loss = criterion(outputs, y_batch)\n            val_loss += loss.item()\n    \n    avg_val_loss = val_loss / len(val_loader)\n    \n    history['train_loss'].append(avg_train_loss)\n    history['val_loss'].append(avg_val_loss)\n    history['epoch'].append(epoch + 1)\n    \n    if (epoch + 1) % 5 == 0 or epoch == 0:\n        print(f\"Epoch {epoch+1}/{MAX_EPOCHS}: Train Loss = {avg_train_loss:.6f}, Val Loss = {avg_val_loss:.6f}\")\n    \n    # Early stopping on validation loss\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        patience_counter = 0\n        best_model_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n    else:\n        patience_counter += 1\n        if patience_counter >= PATIENCE:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n\n# Restore best model\nif best_model_state is not None:\n    model.load_state_dict({k: v.to(device) for k, v in best_model_state.items()})\n\ntrain_time = time.time() - train_start\nbest_epoch = history['epoch'][history['val_loss'].index(min(history['val_loss']))]\nprint(f\"\\nâœ“ Training complete in {train_time:.2f}s ({epoch+1} epochs)\")\nprint(f\"âœ“ Best epoch: {best_epoch} (val_loss = {best_val_loss:.6f})\")"
  },
  {
   "cell_type": "markdown",
   "id": "eval-header",
   "metadata": {},
   "source": [
    "## Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[Step 6/6] Evaluating on test set...\")\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(y_batch.numpy())\n",
    "\n",
    "y_test = np.array(all_labels)\n",
    "y_pred = np.array(all_preds)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "f1_weighted = f1_score(y_test, y_pred, average='weighted')\n",
    "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "precision_weighted = precision_score(y_test, y_pred, average='weighted')\n",
    "recall_weighted = recall_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"TEST SET RESULTS {'(QUICK MODE)' if QUICK_MODE else ''}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Accuracy:          {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"Balanced Accuracy: {balanced_acc:.4f} ({balanced_acc*100:.2f}%)\")\n",
    "print(f\"F1 (weighted):     {f1_weighted:.4f}\")\n",
    "print(f\"F1 (macro):        {f1_macro:.4f}\")\n",
    "print(f\"Precision (weighted): {precision_weighted:.4f}\")\n",
    "print(f\"Recall (weighted):    {recall_weighted:.4f}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classification-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPer-Class Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=class_names, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-header",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-history",
   "metadata": {},
   "outputs": [],
   "source": "# Training history\nfig, ax = plt.subplots(figsize=(10, 5))\nax.plot(history['epoch'], history['train_loss'], 'b-', linewidth=2, label='Train Loss')\nax.plot(history['epoch'], history['val_loss'], 'r-', linewidth=2, label='Val Loss')\nax.axvline(x=best_epoch, color='g', linestyle='--', alpha=0.7, label=f'Best Epoch ({best_epoch})')\nax.set_xlabel('Epoch')\nax.set_ylabel('Loss')\nax.set_title(f'LSTM Training History{\" - QUICK\" if QUICK_MODE else \"\"}')\nax.legend()\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig(FIGURES_DIR / f'lstm_training_history{MODE_SUFFIX}.png', dpi=150, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confusion-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "sns.heatmap(cm, annot=False, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names, ax=axes[0])\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_title(f'LSTM Confusion Matrix (Counts){\" - QUICK\" if QUICK_MODE else \"\"}')\n",
    "\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names, ax=axes[1])\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "axes[1].set_title(f'LSTM Confusion Matrix (Normalized){\" - QUICK\" if QUICK_MODE else \"\"}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / f'lstm_confusion_matrix{MODE_SUFFIX}.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"âœ“ Saved confusion matrix to {FIGURES_DIR / f'lstm_confusion_matrix{MODE_SUFFIX}.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "per-class-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class F1 scores\n",
    "f1_per_class = f1_score(y_test, y_pred, average=None)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars = ax.bar(class_names, f1_per_class, color='steelblue', edgecolor='black')\n",
    "ax.axhline(y=f1_weighted, color='red', linestyle='--', label=f'Weighted Avg: {f1_weighted:.4f}')\n",
    "ax.set_xlabel('Fault Class')\n",
    "ax.set_ylabel('F1 Score')\n",
    "ax.set_title(f'LSTM Per-Class F1 Scores{\" - QUICK\" if QUICK_MODE else \"\"}')\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bar, f1 in zip(bars, f1_per_class):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "            f'{f1:.3f}', ha='center', va='bottom', fontsize=8, rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / f'lstm_per_class_f1{MODE_SUFFIX}.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-header",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-results",
   "metadata": {},
   "outputs": [],
   "source": "end_time = time.time()\ntotal_runtime = end_time - start_time\n\n# Compile all metrics\nmetrics = {\n    'model': 'LSTM',\n    'task': 'multiclass',\n    'quick_mode': QUICK_MODE,\n    'train_fraction': TRAIN_FRACTION,\n    'train_samples': len(train_dataset),\n    'val_samples': len(val_dataset),\n    'test_samples': len(test_dataset),\n    'best_epoch': best_epoch,\n    'best_val_loss': float(best_val_loss),\n    'accuracy': float(accuracy),\n    'balanced_accuracy': float(balanced_acc),\n    'f1_weighted': float(f1_weighted),\n    'f1_macro': float(f1_macro),\n    'precision_weighted': float(precision_weighted),\n    'recall_weighted': float(recall_weighted),\n    'per_class_f1': {class_names[i]: float(f1_per_class[i]) for i in range(num_classes)},\n    'hyperparameters': best_params,\n    'epochs_trained': len(history['epoch']),\n    'training_time_seconds': float(train_time),\n    'total_runtime_seconds': float(total_runtime),\n    'random_seed': RANDOM_SEED\n}\n\n# Save metrics\nwith open(METRICS_DIR / f'lstm_metrics{MODE_SUFFIX}.json', 'w') as f:\n    json.dump(metrics, f, indent=2)\nprint(f\"âœ“ Saved metrics to {METRICS_DIR / f'lstm_metrics{MODE_SUFFIX}.json'}\")\n\n# Save model\ntorch.save({\n    'model_state_dict': model.state_dict(),\n    'model_config': {\n        'input_size': num_features,\n        'hidden_size': best_params['hidden_size'],\n        'num_layers': best_params['num_layers'],\n        'num_classes': num_classes,\n        'dropout': best_params['dropout'],\n        'sequence_length': sequence_length\n    },\n    'scaler_mean': scaler.mean_.tolist(),\n    'scaler_scale': scaler.scale_.tolist(),\n    'label_encoder_classes': label_encoder.classes_.tolist(),\n    'features': features\n}, MODEL_DIR / f'lstm_final{MODE_SUFFIX}.pt')\nprint(f\"âœ“ Saved model to {MODEL_DIR / f'lstm_final{MODE_SUFFIX}.pt'}\")\n\n# Save confusion matrix\ncm_df = pd.DataFrame(cm, index=class_names, columns=class_names)\ncm_df.to_csv(METRICS_DIR / f'lstm_confusion_matrix{MODE_SUFFIX}.csv')\nprint(f\"âœ“ Saved confusion matrix to {METRICS_DIR / f'lstm_confusion_matrix{MODE_SUFFIX}.csv'}\")\n\nprint(f\"\\n{'='*60}\")\nprint(f\"âœ“ LSTM Final Training Complete! {'(QUICK MODE)' if QUICK_MODE else ''}\")\nprint(f\"{'='*60}\")\nprint(f\"Total runtime: {int(total_runtime // 60)}m {int(total_runtime % 60)}s\")\nprint(f\"Best epoch: {best_epoch}\")\nprint(f\"Test Accuracy: {accuracy:.4f}\")\nprint(f\"Test F1 (weighted): {f1_weighted:.4f}\")\nprint(f\"{'='*60}\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}