{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# LSTM Evaluation on New TEP Dataset\n\n",
    "This notebook evaluates the trained LSTM model on the newly generated independent TEP dataset.\n\n",
    "**Purpose**: Test model generalization on completely unseen data generated from `tep-sim`.\n\n",
    "**Model**: LSTM (multiclass fault classification)\n\n",
    "**Evaluation Dataset**: `data/new_multiclass_eval.csv` (generated by notebook 03)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## Configuration & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, balanced_accuracy_score,\n",
    "    f1_score, precision_score, recall_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n\n",
    "DATA_DIR = Path('../data')\n",
    "MODEL_DIR = Path('../outputs/models')\n",
    "METRICS_DIR = Path('../outputs/metrics')\n",
    "FIGURES_DIR = Path('../outputs/figures')\n\n",
    "METRICS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n",
    "QUICK_MODE = False\n",
    "if (DATA_DIR / 'new_multiclass_eval_quick.csv').exists():\n",
    "    if not (DATA_DIR / 'new_multiclass_eval.csv').exists():\n",
    "        QUICK_MODE = True\n",
    "    else:\n",
    "        quick_mtime = (DATA_DIR / 'new_multiclass_eval_quick.csv').stat().st_mtime\n",
    "        full_mtime = (DATA_DIR / 'new_multiclass_eval.csv').stat().st_mtime\n",
    "        QUICK_MODE = quick_mtime > full_mtime\n\n",
    "if os.environ.get('QUICK_MODE', '').lower() in ('true', '1', 'yes'):\n",
    "    QUICK_MODE = True\n\n",
    "FILE_SUFFIX = '_quick' if QUICK_MODE else ''\n\n",
    "print('='*60)\n",
    "print('LSTM Evaluation on New TEP Dataset')\n",
    "if QUICK_MODE:\n",
    "    print('QUICK MODE - Using limited test dataset')\n",
    "print(f'Device: {device}')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-header",
   "metadata": {},
   "source": [
    "## Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-def",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.2):\n        super().__init__()\n        self.lstm = nn.LSTM(\n            input_size=input_size,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=dropout if num_layers > 1 else 0\n        )\n        self.dropout = nn.Dropout(dropout)\n        self.fc = nn.Linear(hidden_size, num_classes)\n\n    def forward(self, x):\n        lstm_out, _ = self.lstm(x)\n        last_output = lstm_out[:, -1, :]\n        out = self.dropout(last_output)\n        out = self.fc(out)\n        return out\n\nprint('Model class defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-header",
   "metadata": {},
   "source": [
    "## Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n[Step 1/5] Loading trained LSTM model...')\n\n",
    "model_path = MODEL_DIR / 'lstm_final.pt'\n",
    "model_data = torch.load(model_path, map_location=device, weights_only=False)\n\n",
    "config = model_data['model_config']\n",
    "features = model_data['features']\n",
    "label_classes = model_data['label_encoder_classes']\n",
    "scaler_mean = model_data['scaler_mean']\n",
    "scaler_scale = model_data['scaler_scale']\n\n",
    "num_classes = len(label_classes)\n",
    "class_names = [str(int(c)) for c in label_classes]\n\n",
    "model = LSTMClassifier(\n    input_size=len(features),\n    hidden_size=config['hidden_size'],\n    num_layers=config['num_layers'],\n    num_classes=num_classes,\n    dropout=config['dropout']\n)\n",
    "model.load_state_dict(model_data['model_state_dict'])\n",
    "model.to(device)\n",
    "model.eval()\n\n",
    "print(f'Model loaded from {model_path}')\n",
    "print(f'  Features: {len(features)}')\n",
    "print(f'  Classes: {num_classes}')\n\n",
    "orig_metrics_path = METRICS_DIR / 'lstm_metrics.json'\n",
    "with open(orig_metrics_path) as f:\n",
    "    orig_metrics = json.load(f)\n",
    "print('Original metrics loaded for comparison')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-header",
   "metadata": {},
   "source": [
    "## Load New Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n[Step 2/5] Loading new evaluation dataset...')\n",
    "start_time = time.time()\n\n",
    "eval_file = DATA_DIR / f'new_multiclass_eval{FILE_SUFFIX}.csv'\n",
    "eval_data = pd.read_csv(eval_file)\n\n",
    "print(f'Dataset loaded in {time.time() - start_time:.2f}s')\n",
    "print(f'  File: {eval_file.name}')\n",
    "print(f'  Shape: {eval_data.shape}')\n",
    "print(f'  Unique runs: {eval_data.groupby([\"faultNumber\", \"simulationRun\"]).ngroups}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset-header",
   "metadata": {},
   "source": [
    "## Create Sequence Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimulationRunDataset(Dataset):\n",
    "    def __init__(self, df, features, scaler_mean, scaler_scale, label_classes, sequence_length=10):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.features = features\n",
    "        self.label_classes = label_classes\n",
    "        self.class_to_idx = {c: i for i, c in enumerate(label_classes)}\n",
    "        \n",
    "        self.sequences = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for (fault, run), group in df.groupby(['faultNumber', 'simulationRun']):\n",
    "            group = group.sort_values('sample')\n",
    "            X = group[features].values\n",
    "            X = (X - scaler_mean) / scaler_scale\n",
    "            \n",
    "            for i in range(len(X) - sequence_length + 1):\n",
    "                seq = X[i:i+sequence_length]\n",
    "                self.sequences.append(seq)\n",
    "                self.labels.append(self.class_to_idx[fault])\n",
    "        \n",
    "        self.sequences = np.array(self.sequences, dtype=np.float32)\n",
    "        self.labels = np.array(self.labels, dtype=np.int64)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sequences[idx]), torch.tensor(self.labels[idx])\n\n",
    "print('Dataset class defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n[Step 3/5] Creating evaluation dataset...')\n",
    "seq_len = config['sequence_length']\n\n",
    "eval_dataset = SimulationRunDataset(\n",
    "    eval_data, features, scaler_mean, scaler_scale, label_classes, seq_len\n",
    ")\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=512, shuffle=False, num_workers=0)\n\n",
    "print(f'Evaluation sequences: {len(eval_dataset):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "predict-header",
   "metadata": {},
   "source": [
    "## Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "predict",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n[Step 4/5] Generating predictions...')\n",
    "pred_start = time.time()\n\n",
    "all_preds = []\n",
    "all_labels = []\n\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in eval_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(y_batch.numpy())\n\n",
    "y_pred = np.array(all_preds)\n",
    "y_eval = np.array(all_labels)\n\n",
    "print(f'Predictions generated in {time.time() - pred_start:.2f}s')\n",
    "print(f'  Predictions shape: {y_pred.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metrics-header",
   "metadata": {},
   "source": [
    "## Compute Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compute-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n[Step 5/5] Computing evaluation metrics...')\n\n",
    "accuracy = accuracy_score(y_eval, y_pred)\n",
    "balanced_acc = balanced_accuracy_score(y_eval, y_pred)\n",
    "f1_weighted = f1_score(y_eval, y_pred, average='weighted')\n",
    "f1_macro = f1_score(y_eval, y_pred, average='macro')\n",
    "precision_weighted = precision_score(y_eval, y_pred, average='weighted')\n",
    "recall_weighted = recall_score(y_eval, y_pred, average='weighted')\n",
    "f1_per_class = f1_score(y_eval, y_pred, average=None)\n",
    "cm = confusion_matrix(y_eval, y_pred)\n\n",
    "print('\\n' + '='*60)\n",
    "print('NEW EVALUATION DATASET RESULTS')\n",
    "if QUICK_MODE:\n",
    "    print('(Quick mode - limited samples per class)')\n",
    "print('='*60)\n",
    "print(f'Accuracy:          {accuracy:.4f} ({accuracy*100:.2f}%)')\n",
    "print(f'Balanced Accuracy: {balanced_acc:.4f}')\n",
    "print(f'F1 (weighted):     {f1_weighted:.4f}')\n",
    "print(f'F1 (macro):        {f1_macro:.4f}')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-original",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nComparison with Original Test Set:')\n",
    "print('-' * 50)\n",
    "print(f'{\"Metric\":<20} {\"Original\":>12} {\"New Eval\":>12} {\"Delta\":>10}')\n",
    "print('-' * 50)\n\n",
    "comparisons = [\n",
    "    ('Accuracy', orig_metrics['accuracy'], accuracy),\n",
    "    ('Balanced Acc', orig_metrics['balanced_accuracy'], balanced_acc),\n",
    "    ('F1 (weighted)', orig_metrics['f1_weighted'], f1_weighted),\n",
    "    ('F1 (macro)', orig_metrics['f1_macro'], f1_macro),\n",
    "]\n\n",
    "for metric_name, orig_val, new_val in comparisons:\n",
    "    delta = new_val - orig_val\n",
    "    print(f'{metric_name:<20} {orig_val:>12.4f} {new_val:>12.4f} {delta:>+10.4f}')\n\n",
    "print('-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classification-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nPer-Class Classification Report:')\n",
    "print(classification_report(y_eval, y_pred, target_names=class_names, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-header",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confusion-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n\n",
    "sns.heatmap(cm, annot=False, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names, ax=axes[0])\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_title('LSTM Confusion Matrix - New Eval (Counts)')\n\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names, ax=axes[1])\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "axes[1].set_title('LSTM Confusion Matrix - New Eval (Normalized)')\n\n",
    "plt.tight_layout()\n",
    "output_file = FIGURES_DIR / f'lstm_new_eval_confusion_matrix{FILE_SUFFIX}.png'\n",
    "plt.savefig(output_file, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f'Saved to {output_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "per-class-f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 6))\n\n",
    "x = np.arange(len(class_names))\n",
    "width = 0.35\n\n",
    "orig_f1_per_class = [orig_metrics['per_class_f1'][c] for c in class_names]\n\n",
    "bars1 = ax.bar(x - width/2, orig_f1_per_class, width, label='Original Test Set', color='steelblue', alpha=0.7)\n",
    "bars2 = ax.bar(x + width/2, f1_per_class, width, label='New Evaluation Set', color='coral', alpha=0.7)\n\n",
    "ax.axhline(y=orig_metrics['f1_weighted'], color='steelblue', linestyle='--', alpha=0.5)\n",
    "ax.axhline(y=f1_weighted, color='coral', linestyle='--', alpha=0.5)\n\n",
    "ax.set_xlabel('Fault Class')\n",
    "ax.set_ylabel('F1 Score')\n",
    "ax.set_title('LSTM Per-Class F1 Scores: Original vs New Evaluation')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(class_names)\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.grid(axis='y', alpha=0.3)\n\n",
    "plt.tight_layout()\n",
    "output_file = FIGURES_DIR / f'lstm_new_eval_per_class_f1{FILE_SUFFIX}.png'\n",
    "plt.savefig(output_file, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f'Saved to {output_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-header",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_eval_metrics = {\n",
    "    'model': 'LSTM',\n",
    "    'dataset': f'new_multiclass_eval{FILE_SUFFIX}',\n",
    "    'task': 'multiclass',\n",
    "    'quick_mode': QUICK_MODE,\n",
    "    'num_samples': len(y_eval),\n",
    "    'num_classes': num_classes,\n",
    "    'accuracy': float(accuracy),\n",
    "    'balanced_accuracy': float(balanced_acc),\n",
    "    'f1_weighted': float(f1_weighted),\n",
    "    'f1_macro': float(f1_macro),\n",
    "    'precision_weighted': float(precision_weighted),\n",
    "    'recall_weighted': float(recall_weighted),\n",
    "    'per_class_f1': {class_names[i]: float(f1_per_class[i]) for i in range(num_classes)},\n",
    "    'comparison_with_original': {\n",
    "        'accuracy_delta': float(accuracy - orig_metrics['accuracy']),\n",
    "        'f1_weighted_delta': float(f1_weighted - orig_metrics['f1_weighted']),\n",
    "        'original_accuracy': orig_metrics['accuracy'],\n",
    "        'original_f1_weighted': orig_metrics['f1_weighted']\n",
    "    }\n",
    "}\n\n",
    "output_file = METRICS_DIR / f'lstm_new_eval_metrics{FILE_SUFFIX}.json'\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(new_eval_metrics, f, indent=2)\n",
    "print(f'Saved metrics to {output_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*60)\n",
    "print('LSTM New Data Evaluation Complete!')\n",
    "if QUICK_MODE:\n",
    "    print('(Quick mode)')\n",
    "print('='*60)\n",
    "print(f'\\nSummary:')\n",
    "print(f'  Evaluation samples: {len(y_eval):,}')\n",
    "print(f'  Accuracy: {accuracy:.4f} (original: {orig_metrics[\"accuracy\"]:.4f}, delta: {accuracy - orig_metrics[\"accuracy\"]:+.4f})')\n",
    "print(f'  F1 (weighted): {f1_weighted:.4f} (original: {orig_metrics[\"f1_weighted\"]:.4f}, delta: {f1_weighted - orig_metrics[\"f1_weighted\"]:+.4f})')\n",
    "print('='*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}