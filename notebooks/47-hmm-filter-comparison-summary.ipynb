{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# HMM Filter Comparison Summary\n\nCompare HMM filter effectiveness across all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\nimport json\nfrom pathlib import Path\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\n\nMETRICS_DIR = Path('../outputs/metrics')\nFIGURES_DIR = Path('../outputs/figures')\n\nQUICK_MODE = os.environ.get('QUICK_MODE', '').lower() in ('true', '1', 'yes')\nFILE_SUFFIX = '_quick' if QUICK_MODE else ''\n\nprint('='*60)\n",
    "print('HMM Filter Comparison Summary')\nif QUICK_MODE:\n    print('QUICK MODE')\nprint('='*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nLoading metrics...')\n\nmodels = ['xgboost', 'lstm', 'lstm_fcn', 'cnn_transformer', 'transkal', 'lstm_autoencoder', 'conv_autoencoder']\nmetrics = {}\n\nfor model in models:\n",
    "    file = METRICS_DIR / f'{model}_hmm_filter_results{FILE_SUFFIX}.json'\n    if file.exists():\n        with open(file) as f:\n            metrics[model] = json.load(f)\n        print(f'  Loaded {model}')\n    else:\n        print(f'  Missing {model}')\n\nprint(f'\\nLoaded {len(metrics)} models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nModel Comparison:')\nprint('='*80)\n\nrows = []\nfor model, m in metrics.items():\n    if 'overall_metrics' in m:  # Detector format\n        rows.append({\n            'Model': m['model'],\n            'Accuracy': m['overall_metrics']['accuracy'],\n            'F1 (weighted)': m['overall_metrics'].get('f1_weighted', 0),\n            'Recall': m['overall_metrics'].get('fault_detection_recall', m.get('metrics', {}).get('recall', 0))\n        })\n    elif 'raw_metrics' in m:  # HMM filter format\n        best = m['filtered_metrics'][str(m['best_stickiness'])]\n        rows.append({\n            'Model': m['model'],\n            'Raw Accuracy': m['raw_metrics']['accuracy'],\n            'HMM Accuracy': best['accuracy'],\n            'Improvement': m['best_improvement']['accuracy_delta'],\n            'Best Stickiness': m['best_stickiness']\n        })\n\ndf = pd.DataFrame(rows)\nprint(df.to_string(index=False))\n\n",
    "df.to_csv(METRICS_DIR / f'hmm_filter_comparison{FILE_SUFFIX}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*60)\nprint('Comparison Complete!')\nprint('='*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}