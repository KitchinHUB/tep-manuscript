{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Conv-Autoencoder Evaluation on New TEP Dataset\n\n",
    "This notebook evaluates the trained Conv-Autoencoder model on the newly generated independent TEP dataset.\n\n",
    "**Purpose**: Test model generalization on completely unseen data generated from `tep-sim`.\n\n",
    "**Model**: Conv-Autoencoder (binary anomaly detection)\n\n",
    "**Evaluation Dataset**: `data/new_binary_eval.csv` (generated by notebook 03)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## Configuration & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n\n",
    "DATA_DIR = Path('../data')\n",
    "MODEL_DIR = Path('../outputs/models')\n",
    "METRICS_DIR = Path('../outputs/metrics')\n",
    "FIGURES_DIR = Path('../outputs/figures')\n\n",
    "METRICS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n",
    "QUICK_MODE = False\n",
    "if (DATA_DIR / 'new_binary_eval_quick.csv').exists():\n",
    "    if not (DATA_DIR / 'new_binary_eval.csv').exists():\n",
    "        QUICK_MODE = True\n",
    "    else:\n",
    "        quick_mtime = (DATA_DIR / 'new_binary_eval_quick.csv').stat().st_mtime\n",
    "        full_mtime = (DATA_DIR / 'new_binary_eval.csv').stat().st_mtime\n",
    "        QUICK_MODE = quick_mtime > full_mtime\n\n",
    "if os.environ.get('QUICK_MODE', '').lower() in ('true', '1', 'yes'):\n",
    "    QUICK_MODE = True\n\n",
    "FILE_SUFFIX = '_quick' if QUICK_MODE else ''\n\n",
    "print('='*60)\n",
    "print('Conv-Autoencoder Evaluation on New TEP Dataset')\n",
    "if QUICK_MODE:\n",
    "    print('QUICK MODE - Using limited test dataset')\n",
    "print(f'Device: {device}')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-header",
   "metadata": {},
   "source": [
    "## Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-def",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAutoencoder(nn.Module):\n    def __init__(self, input_size, seq_len, hidden_channels=64, latent_size=32, use_transformer=False):\n        super().__init__()\n        self.use_transformer = use_transformer\n        self.encoder = nn.Sequential(\n            nn.Conv1d(input_size, hidden_channels, 3, padding=1),\n            nn.ReLU(),\n            nn.Conv1d(hidden_channels, hidden_channels*2, 3, padding=1),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool1d(1)\n        )\n        self.fc_enc = nn.Linear(hidden_channels*2, latent_size)\n        self.fc_dec = nn.Linear(latent_size, hidden_channels*2 * seq_len)\n        self.decoder = nn.Sequential(\n            nn.Conv1d(hidden_channels*2, hidden_channels, 3, padding=1),\n            nn.ReLU(),\n            nn.Conv1d(hidden_channels, input_size, 3, padding=1)\n        )\n        self.seq_len = seq_len\n        self.hidden_channels = hidden_channels\n\n    def forward(self, x):\n        x = x.permute(0, 2, 1)\n        enc = self.encoder(x).squeeze(-1)\n        latent = self.fc_enc(enc)\n        dec = self.fc_dec(latent).view(-1, self.hidden_channels*2, self.seq_len)\n        out = self.decoder(dec)\n        return out.permute(0, 2, 1)\n\nprint('Model class defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-header",
   "metadata": {},
   "source": [
    "## Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n[Step 1/5] Loading trained Conv-Autoencoder model...')\n\n",
    "model_path = MODEL_DIR / 'conv_autoencoder_final.pt'\n",
    "model_data = torch.load(model_path, map_location=device, weights_only=False)\n\n",
    "features = model_data['features']\n",
    "scaler_mean = model_data['scaler_mean']\n",
    "scaler_scale = model_data['scaler_scale']\n",
    "threshold = model_data['threshold']\n\n",
    "hp = model_data['hyperparameters']\nmodel = ConvAutoencoder(\n    input_size=len(features),\n    seq_len=hp['sequence_length'],\n    hidden_channels=hp['hidden_channels'],\n    latent_size=hp['latent_size'],\n    use_transformer=model_data.get('use_transformer', False)\n)\n",
    "model.load_state_dict(model_data['model_state_dict'])\n",
    "model.to(device)\n",
    "model.eval()\n\n",
    "print(f'Model loaded from {model_path}')\n",
    "print(f'  Features: {len(features)}')\n",
    "print(f'  Anomaly threshold: {threshold:.6f}')\n\n",
    "orig_metrics_path = METRICS_DIR / 'conv_autoencoder_metrics.json'\n",
    "with open(orig_metrics_path) as f:\n",
    "    orig_metrics = json.load(f)\n",
    "print('Original metrics loaded for comparison')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-header",
   "metadata": {},
   "source": [
    "## Load New Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n[Step 2/5] Loading new evaluation dataset...')\n",
    "start_time = time.time()\n\n",
    "eval_file = DATA_DIR / f'new_binary_eval{FILE_SUFFIX}.csv'\n",
    "eval_data = pd.read_csv(eval_file)\n\n",
    "# Create binary labels\n",
    "eval_data['is_fault'] = (eval_data['faultNumber'] != 0).astype(int)\n\n",
    "print(f'Dataset loaded in {time.time() - start_time:.2f}s')\n",
    "print(f'  File: {eval_file.name}')\n",
    "print(f'  Shape: {eval_data.shape}')\n",
    "print(f'  Normal: {(eval_data[\"is_fault\"] == 0).sum():,}')\n",
    "print(f'  Fault: {(eval_data[\"is_fault\"] == 1).sum():,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset-header",
   "metadata": {},
   "source": [
    "## Create Sequence Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, df, features, scaler_mean, scaler_scale, sequence_length=10):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.sequences = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for (fault, run), group in df.groupby(['faultNumber', 'simulationRun']):\n",
    "            group = group.sort_values('sample')\n",
    "            X = group[features].values\n",
    "            y = group['is_fault'].values\n",
    "            X = (X - scaler_mean) / scaler_scale\n",
    "            \n",
    "            for i in range(len(X) - sequence_length + 1):\n",
    "                self.sequences.append(X[i:i+sequence_length])\n",
    "                self.labels.append(y[i+sequence_length-1])\n",
    "        \n",
    "        self.sequences = np.array(self.sequences, dtype=np.float32)\n",
    "        self.labels = np.array(self.labels, dtype=np.int64)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sequences[idx]), torch.tensor(self.labels[idx])\n\n",
    "print('Dataset class defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n[Step 3/5] Creating evaluation dataset...')\n",
    "seq_len = model_data['hyperparameters']['sequence_length']\n\n",
    "eval_dataset = SequenceDataset(\n",
    "    eval_data, features, scaler_mean, scaler_scale, seq_len\n",
    ")\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=512, shuffle=False, num_workers=0)\n\n",
    "print(f'Evaluation sequences: {len(eval_dataset):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "predict-header",
   "metadata": {},
   "source": [
    "## Compute Reconstruction Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "predict",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n[Step 4/5] Computing reconstruction errors...')\n",
    "pred_start = time.time()\n\n",
    "all_errors = []\n",
    "all_labels = []\n\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in eval_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        recon = model(X_batch)\n",
    "        errors = ((X_batch - recon) ** 2).mean(dim=(1, 2)).cpu().numpy()\n",
    "        all_errors.extend(errors)\n",
    "        all_labels.extend(y_batch.numpy())\n\n",
    "recon_errors = np.array(all_errors)\n",
    "y_eval = np.array(all_labels)\n",
    "y_pred = (recon_errors > threshold).astype(int)\n\n",
    "print(f'Predictions generated in {time.time() - pred_start:.2f}s')\n",
    "print(f'  Reconstruction errors shape: {recon_errors.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metrics-header",
   "metadata": {},
   "source": [
    "## Compute Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compute-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n[Step 5/5] Computing evaluation metrics...')\n\n",
    "accuracy = accuracy_score(y_eval, y_pred)\n",
    "precision = precision_score(y_eval, y_pred)\n",
    "recall = recall_score(y_eval, y_pred)\n",
    "f1 = f1_score(y_eval, y_pred)\n",
    "auc = roc_auc_score(y_eval, recon_errors)\n",
    "cm = confusion_matrix(y_eval, y_pred)\n\n",
    "print('\\n' + '='*60)\n",
    "print('NEW EVALUATION DATASET RESULTS')\n",
    "if QUICK_MODE:\n",
    "    print('(Quick mode - limited samples)')\n",
    "print('='*60)\n",
    "print(f'Accuracy:  {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall:    {recall:.4f}')\n",
    "print(f'F1 Score:  {f1:.4f}')\n",
    "print(f'AUC-ROC:   {auc:.4f}')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-original",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nComparison with Original Test Set:')\n",
    "print('-' * 50)\n",
    "print(f'{\"Metric\":<15} {\"Original\":>12} {\"New Eval\":>12} {\"Delta\":>10}')\n",
    "print('-' * 50)\n\n",
    "comparisons = [\n",
    "    ('Accuracy', orig_metrics['accuracy'], accuracy),\n",
    "    ('Precision', orig_metrics['precision'], precision),\n",
    "    ('Recall', orig_metrics['recall'], recall),\n",
    "    ('F1', orig_metrics['f1'], f1),\n",
    "    ('AUC-ROC', orig_metrics['auc_roc'], auc),\n",
    "]\n\n",
    "for metric_name, orig_val, new_val in comparisons:\n",
    "    delta = new_val - orig_val\n",
    "    print(f'{metric_name:<15} {orig_val:>12.4f} {new_val:>12.4f} {delta:>+10.4f}')\n\n",
    "print('-' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-header",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confusion-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n",
    "# Confusion matrix\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Normal', 'Fault'], yticklabels=['Normal', 'Fault'], ax=axes[0])\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_title('Conv-Autoencoder Confusion Matrix - New Eval')\n\n",
    "# Error distribution\n",
    "axes[1].hist(recon_errors[y_eval == 0], bins=50, alpha=0.7, label='Normal', density=True)\n",
    "axes[1].hist(recon_errors[y_eval == 1], bins=50, alpha=0.7, label='Fault', density=True)\n",
    "axes[1].axvline(x=threshold, color='red', linestyle='--', label=f'Threshold: {threshold:.4f}')\n",
    "axes[1].set_xlabel('Reconstruction Error')\n",
    "axes[1].set_ylabel('Density')\n",
    "axes[1].set_title('Error Distribution')\n",
    "axes[1].legend()\n\n",
    "plt.tight_layout()\n",
    "output_file = FIGURES_DIR / f'conv_autoencoder_new_eval_results{FILE_SUFFIX}.png'\n",
    "plt.savefig(output_file, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f'Saved to {output_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-header",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_eval_metrics = {\n",
    "    'model': 'Conv-Autoencoder',\n",
    "    'dataset': f'new_binary_eval{FILE_SUFFIX}',\n",
    "    'task': 'binary',\n",
    "    'quick_mode': QUICK_MODE,\n",
    "    'num_samples': len(y_eval),\n",
    "    'accuracy': float(accuracy),\n",
    "    'precision': float(precision),\n",
    "    'recall': float(recall),\n",
    "    'f1': float(f1),\n",
    "    'auc_roc': float(auc),\n",
    "    'threshold': float(threshold),\n",
    "    'comparison_with_original': {\n",
    "        'accuracy_delta': float(accuracy - orig_metrics['accuracy']),\n",
    "        'f1_delta': float(f1 - orig_metrics['f1']),\n",
    "        'original_accuracy': orig_metrics['accuracy'],\n",
    "        'original_f1': orig_metrics['f1']\n",
    "    }\n",
    "}\n\n",
    "output_file = METRICS_DIR / f'conv_autoencoder_new_eval_metrics{FILE_SUFFIX}.json'\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(new_eval_metrics, f, indent=2)\n",
    "print(f'Saved metrics to {output_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*60)\n",
    "print('Conv-Autoencoder New Data Evaluation Complete!')\n",
    "if QUICK_MODE:\n",
    "    print('(Quick mode)')\n",
    "print('='*60)\n",
    "print(f'\\nSummary:')\n",
    "print(f'  Evaluation samples: {len(y_eval):,}')\n",
    "print(f'  Accuracy: {accuracy:.4f} (original: {orig_metrics[\"accuracy\"]:.4f})')\n",
    "print(f'  F1: {f1:.4f} (original: {orig_metrics[\"f1\"]:.4f})')\n",
    "print(f'  AUC-ROC: {auc:.4f}')\n",
    "print('='*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}